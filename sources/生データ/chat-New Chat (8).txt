### USER
今作ってるアプリの本質は最高峰のバイブコーディング運用するための直感的に最高峰のバイブコーディングをするためのアプリ作りです。
複数のAIIDEやCLI　LLMなどAPIやMCPなどすべてのツールを最大限活用しそれに沿って開発にあたってのワークフローを完璧行ってくれる導いてくれる、アプリを作ってます。
うまくいっていないです。　どうすればいいですか？

### ASSISTANT
<details type="reasoning" done="true" duration="3" view="" last_tool_call_name="">
> 「最高のバイブレーションコーディング」が何を指しているのか正確にはわかりませんが、AIを活用したコーディング支援ツールや、複数のAIツールを統合して開発ワークフローを自動化するようなプラットフォームを目指しているのでしょうか。その方向性で進めてみましょう！
</details>

<glm_block view="" tool_call_name="search">{"data":{"browser":{"search_result":[{"title":"20 Best AI Coding Assistant Tools [Updated Aug 2025]","url":"https://www.qodo.ai/blog/best-ai-coding-assistant-tools","text":"Explore our list of the 20 best AI coding assistant tools in 2025, boosting productivity and code quality for developers.","index":1,"favicon":"","host_name":"www.qodo.ai"},{"title":"Best AI Tools for Coding in 2025: 6 Tools Worth Your Time","url":"https://www.pragmaticcoders.com/resources/ai-developer-tools","text":"We tested 20+ AI coding tools – agents, reviewers, and assistants. See what stood out and what features the best tools offer.","index":2,"favicon":"","host_name":"www.pragmaticcoders.com"},{"title":"The 8 Best AI Coding Tools in 2025","url":"https://medium.com/@datajournal/the-8-best-ai-coding-tools-in-2025-cb323c5b9780","text":"Find the best AI coding tools of 2025 for faster, smarter code. Compare features and pricing for Cursor, Copilot, Tabnine, and more.","index":3,"favicon":"","host_name":"medium.com"},{"title":"What is the best AI engine for programming in September","url":"https://www.reddit.com/r/GithubCopilot/comments/1nhju40/what_is_the_best_ai_engine_for_programming_in","text":"#1 codex since, chatgpt 5 is best model. #2 Claude code, best CLI tool, good model. #3 Copilot, good auto complete, github integration.","index":4,"favicon":"","host_name":"www.reddit.com"},{"title":"13 Best AI Coding Tools for Complex Enterprise","url":"https://www.augmentcode.com/guides/13-best-ai-coding-tools-for-complex-codebases","text":"Find the best best coding ai with detailed comparisons, pros/cons, and implementation guides for dev teams.","index":5,"favicon":"","host_name":"www.augmentcode.com"},{"title":"AI Coding Tools in 2025: Welcome to the Agentic CLI Era","url":"https://thenewstack.io/ai-coding-tools-in-2025-welcome-to-the-agentic-cli-era","text":"2025 was the start of the Agentic Era, a new wave of software development. We moved beyond simple IDE chatbots and onto agentic CLIs.","index":6,"favicon":"","host_name":"thenewstack.io"},{"title":"Best AI Coding Assistants 2025: Tools for Developers","url":"https://replit.com/discover/best-ai-coding-assistant","text":"Best AI coding assistants of 2025 compared. See how tools like Replit, Cursor, and Copilot automate coding, testing, and deployment.","index":7,"favicon":"","host_name":"replit.com"},{"title":"AI Code Tools: Complete Guide for Developers in 2025","url":"https://codesubmit.io/blog/ai-code-tools","text":"Discover the best AI coding tools in 2025. Personal curation of 40+ tools including ChatGPT, GitHub Copilot, Cursor, and more.","index":8,"favicon":"","host_name":"codesubmit.io"},{"title":"Top 9 Cursor Alternatives in 2025: Best Open Source AI","url":"https://cline.bot/blog/top-9-cursor-alternatives-in-2025-best-open-source-ai-dev-tools-for-developers","text":"Open-source AI coding assistant designed for self-hosting; Supports multiple programming languages and major IDEs; Real-time, context-aware code completion and ...","index":9,"favicon":"","host_name":"cline.bot"},{"title":"14 Best AI Developer Productivity Tools in 2025","url":"https://www.greptile.com/content-library/14-best-developer-productivity-tools-2025","text":"Discover the top 14 AI developer productivity tools in 2025, including Greptile, GitHub Copilot, and Tabnine. Compare features, pricing ...","index":10,"favicon":"","host_name":"www.greptile.com"},{"title":"Specification","url":"https://modelcontextprotocol.io/specification/2025-11-25","text":"Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and ...","index":11,"favicon":"","host_name":"modelcontextprotocol.io"},{"title":"7 Things to Know About MCP in 2025","url":"https://www.adskate.com/blogs/mcp-model-context-protocol-2025-guide","text":"Why It Matters in 2025. Cross-model support: MCP is model-agnostic and now supported by OpenAI, Anthropic, and Hugging Face. Scalability ...","index":12,"favicon":"","host_name":"www.adskate.com"},{"title":"What Is the Model Context Protocol (MCP) and How It Works","url":"https://www.descope.com/learn/post/mcp","text":"Visual Studio Code: Microsoft's IDE, which added MCP support in June 2025. JetBrains IDEs: Full coding suite that added AI Assistant MCP ...","index":13,"favicon":"","host_name":"www.descope.com"},{"title":"The Model Context Protocol's impact on 2025","url":"https://www.thoughtworks.com/en-us/insights/blog/generative-ai/model-context-protocol-mcp-impact-2025","text":"The Model Context Protocol (MCP) has been one of the key stories of 2025. We unpack its impact through the lens of the Thoughtworks ...","index":14,"favicon":"","host_name":"www.thoughtworks.com"},{"title":"IDE integration using Model Context Protocol (MCP)","url":"https://techdocs.broadcom.com/us/en/vmware-tanzu/spring/application-advisor/1-5/app-advisor/model-context-protocol-server.html","text":"To integrate the Application Advisor into your preferred IDE or MCP client, you can use the Application Advisor MCP server.","index":15,"favicon":"","host_name":"techdocs.broadcom.com"},{"title":"Model Context Protocol (MCP) | AI Assistant Documentation","url":"https://www.jetbrains.com/help/ai-assistant/mcp.html","text":"Starting with version 2025.2, JetBrains IDEs come with an integrated MCP server, allowing external clients such as Claude Desktop, Cursor, Codex ...","index":16,"favicon":"","host_name":"www.jetbrains.com"},{"title":"Model Context Protocol","url":"https://developers.openai.com/codex/mcp","text":"The CLI and the IDE extension share this configuration. Once you configure your MCP servers, you can switch between the two Codex clients without redoing setup.","index":17,"favicon":"","host_name":"developers.openai.com"},{"title":"Model Context Protocol (MCP): AI Integration Guide | Medium","url":"https://michielh.medium.com/the-model-context-protocol-mcp-step-by-step-connecting-ai-agents-to-everything-6b25a052b87c","text":"Evolving spec: OAuth integration, server remote hosting, and discovery registries are still under active development (early 2025); Potential ...","index":18,"favicon":"","host_name":"michielh.medium.com"},{"title":"What is Model Context Protocol (MCP)? The Complete Guide","url":"https://www.figma.com/resource-library/what-is-mcp","text":"In January and February 2025, many AI IDEs, like Cursor and Windsurf, started supporting MCP. In March, OpenAI added MCP support, and GitHub released its MCP ...","index":19,"favicon":"","host_name":"www.figma.com"},{"title":"Revolutionize AI Integration with MCP: The Future of Open","url":"https://www.baytechconsulting.com/blog/revolutionize-ai-integration-mcp-2025","text":"Explore the strategic architecture and implementation of the Model Context Protocol (MCP) in .NET Core, detailing roles, technical mechanics ...","index":20,"favicon":"","host_name":"www.baytechconsulting.com"},{"title":"Vibe Coding Explained: Tools and Guides","url":"https://cloud.google.com/discover/what-is-vibe-coding","text":"Vibe coding is a style of programming that uses natural language prompts to assist with the coding process. Learn how it works with Google ...","index":21,"favicon":"","host_name":"cloud.google.com"},{"title":"Vibe coding","url":"https://en.wikipedia.org/wiki/Vibe_coding","text":"Vibe coding is an AI-assisted software development technique. The term was introduced by Andrej Karpathy in February 2025. The term was listed on the Merriam- ...","index":22,"favicon":"","host_name":"en.wikipedia.org"},{"title":"What is Vibe Coding?","url":"https://www.ibm.com/think/topics/vibe-coding","text":"Vibe coding is a fresh take in coding where users express their intention using plain speech and the AI transforms that thinking into executable code.","index":23,"favicon":"","host_name":"www.ibm.com"},{"title":"Vibe coding is not the same as AI-Assisted engineering.","url":"https://medium.com/@addyosmani/vibe-coding-is-not-the-same-as-ai-assisted-engineering-3f81088d5b98","text":"In an August 2025 survey by Final Round AI, 18 CTOs were asked about vibe coding and 16 reported experiencing production disasters directly ...","index":24,"favicon":"","host_name":"medium.com"},{"title":"What is the exact definition of \"vibe coding\"? : r/ClaudeAI","url":"https://www.reddit.com/r/ClaudeAI/comments/1j6z4ft/what_is_the_exact_definition_of_vibe_coding","text":"you tell the AI what you want, and it writes the code. Coined by Karpathy, it's fast, intuitive, and great for prototypes.","index":25,"favicon":"","host_name":"www.reddit.com"},{"title":"How to Vibe Code: 5 Simple Tips for Using AI to Become a","url":"https://www.pcmag.com/explainers/how-to-vibe-code-5-simple-tips-for-using-ai-to-become-a-programmer","text":"In short, vibe coding means giving an AI a programming task and letting it complete it for you. It generates the code, makes architecture ...","index":26,"favicon":"","host_name":"www.pcmag.com"},{"title":"What is Vibe Coding? Prompting AI Software Development","url":"https://www.sonarsource.com/resources/library/vibe-coding","text":"Vibe coding basically means using AI, like those big language models, to write code for you. You just tell it what you want in regular words, and boom, it tries ...","index":27,"favicon":"","host_name":"www.sonarsource.com"},{"title":"What is Vibe Coding? Your 2025 Vibe Coding Guide","url":"https://www.synergylabs.co/blog/what-is-vibe-coding-your-2025-vibe-coding-guide","text":"Vibe Coding is a development paradigm where programmers essentially \"surrender to the vibes\" of AI-powered coding assistants.","index":28,"favicon":"","host_name":"www.synergylabs.co"},{"title":"Understanding Vibe Coding and Its Implications","url":"https://www.blackduck.com/blog/vibe-coding-and-its-implications.html","text":"“Vibe coding” means using natural language and AI tools to generate and refine code. This approach lowers barriers to entry for coding, ...","index":29,"favicon":"","host_name":"www.blackduck.com"},{"title":"Vibe Coding: The Future of Building Software (Without","url":"https://www.siddharthbharath.com/vibe-coding","text":"Vibe coding is the new frontier where you build software by telling an AI what you want and letting it spit out the code.","index":30,"favicon":"","host_name":"www.siddharthbharath.com"}]},"metadata":{"arguments":"{\"query\":\"vibe coding meaning AI IDE 2025\"}","display_result":"","duration":"4s","id":"call_k5fbxvtjjJ","is_error":false,"mcp_server":{"name":"mcp-server"},"name":"search","result":"【20†Vibe Coding Explained: Tools and Guides†https://cloud.google.com/discover/what-is-vibe-coding】\nVibe coding is a style of programming that uses natural language prompts to assist with the coding process. Learn how it works with Google ...\n\n【21†Vibe coding†https://en.wikipedia.org/wiki/Vibe_coding】\nVibe coding is an AI-assisted software development technique. The term was introduced by Andrej Karpathy in February 2025. The term was listed on the Merriam- ...\n\n【22†What is Vibe Coding?†https://www.ibm.com/think/topics/vibe-coding】\nVibe coding is a fresh take in coding where users express their intention using plain speech and the AI transforms that thinking into executable code.\n\n【23†Vibe coding is not the same as AI-Assisted engineering.†https://medium.com/@addyosmani/vibe-coding-is-not-the-same-as-ai-assisted-engineering-3f81088d5b98】\nIn an August 2025 survey by Final Round AI, 18 CTOs were asked about vibe coding and 16 reported experiencing production disasters directly ...\n\n【24†What is the exact definition of \"vibe coding\"? : r/ClaudeAI†https://www.reddit.com/r/ClaudeAI/comments/1j6z4ft/what_is_the_exact_definition_of_vibe_coding】\nyou tell the AI what you want, and it writes the code. Coined by Karpathy, it's fast, intuitive, and great for prototypes.\n\n【25†How to Vibe Code: 5 Simple Tips for Using AI to Become a†https://www.pcmag.com/explainers/how-to-vibe-code-5-simple-tips-for-using-ai-to-become-a-programmer】\nIn short, vibe coding means giving an AI a programming task and letting it complete it for you. It generates the code, makes architecture ...\n\n【26†What is Vibe Coding? Prompting AI Software Development†https://www.sonarsource.com/resources/library/vibe-coding】\nVibe coding basically means using AI, like those big language models, to write code for you. You just tell it what you want in regular words, and boom, it tries ...\n\n【27†What is Vibe Coding? Your 2025 Vibe Coding Guide†https://www.synergylabs.co/blog/what-is-vibe-coding-your-2025-vibe-coding-guide】\nVibe Coding is a development paradigm where programmers essentially \"surrender to the vibes\" of AI-powered coding assistants.\n\n【28†Understanding Vibe Coding and Its Implications†https://www.blackduck.com/blog/vibe-coding-and-its-implications.html】\n“Vibe coding” means using natural language and AI tools to generate and refine code. This approach lowers barriers to entry for coding, ...\n\n【29†Vibe Coding: The Future of Building Software (Without†https://www.siddharthbharath.com/vibe-coding】\nVibe coding is the new frontier where you build software by telling an AI what you want and letting it spit out the code.\n","status":"completed"}},"type":"mcp"}</glm_block>
<glm_block view="" tool_call_name="search">{"data":{"browser":{"search_result":[{"title":"20 Best AI Coding Assistant Tools [Updated Aug 2025]","url":"https://www.qodo.ai/blog/best-ai-coding-assistant-tools","text":"Explore our list of the 20 best AI coding assistant tools in 2025, boosting productivity and code quality for developers.","index":1,"favicon":"","host_name":"www.qodo.ai"},{"title":"Best AI Tools for Coding in 2025: 6 Tools Worth Your Time","url":"https://www.pragmaticcoders.com/resources/ai-developer-tools","text":"We tested 20+ AI coding tools – agents, reviewers, and assistants. See what stood out and what features the best tools offer.","index":2,"favicon":"","host_name":"www.pragmaticcoders.com"},{"title":"The 8 Best AI Coding Tools in 2025","url":"https://medium.com/@datajournal/the-8-best-ai-coding-tools-in-2025-cb323c5b9780","text":"Find the best AI coding tools of 2025 for faster, smarter code. Compare features and pricing for Cursor, Copilot, Tabnine, and more.","index":3,"favicon":"","host_name":"medium.com"},{"title":"What is the best AI engine for programming in September","url":"https://www.reddit.com/r/GithubCopilot/comments/1nhju40/what_is_the_best_ai_engine_for_programming_in","text":"#1 codex since, chatgpt 5 is best model. #2 Claude code, best CLI tool, good model. #3 Copilot, good auto complete, github integration.","index":4,"favicon":"","host_name":"www.reddit.com"},{"title":"13 Best AI Coding Tools for Complex Enterprise","url":"https://www.augmentcode.com/guides/13-best-ai-coding-tools-for-complex-codebases","text":"Find the best best coding ai with detailed comparisons, pros/cons, and implementation guides for dev teams.","index":5,"favicon":"","host_name":"www.augmentcode.com"},{"title":"AI Coding Tools in 2025: Welcome to the Agentic CLI Era","url":"https://thenewstack.io/ai-coding-tools-in-2025-welcome-to-the-agentic-cli-era","text":"2025 was the start of the Agentic Era, a new wave of software development. We moved beyond simple IDE chatbots and onto agentic CLIs.","index":6,"favicon":"","host_name":"thenewstack.io"},{"title":"Best AI Coding Assistants 2025: Tools for Developers","url":"https://replit.com/discover/best-ai-coding-assistant","text":"Best AI coding assistants of 2025 compared. See how tools like Replit, Cursor, and Copilot automate coding, testing, and deployment.","index":7,"favicon":"","host_name":"replit.com"},{"title":"AI Code Tools: Complete Guide for Developers in 2025","url":"https://codesubmit.io/blog/ai-code-tools","text":"Discover the best AI coding tools in 2025. Personal curation of 40+ tools including ChatGPT, GitHub Copilot, Cursor, and more.","index":8,"favicon":"","host_name":"codesubmit.io"},{"title":"Top 9 Cursor Alternatives in 2025: Best Open Source AI","url":"https://cline.bot/blog/top-9-cursor-alternatives-in-2025-best-open-source-ai-dev-tools-for-developers","text":"Open-source AI coding assistant designed for self-hosting; Supports multiple programming languages and major IDEs; Real-time, context-aware code completion and ...","index":9,"favicon":"","host_name":"cline.bot"},{"title":"14 Best AI Developer Productivity Tools in 2025","url":"https://www.greptile.com/content-library/14-best-developer-productivity-tools-2025","text":"Discover the top 14 AI developer productivity tools in 2025, including Greptile, GitHub Copilot, and Tabnine. Compare features, pricing ...","index":10,"favicon":"","host_name":"www.greptile.com"}]},"metadata":{"arguments":"{\"query\":\"AI IDE that integrates multiple LLM APIs and CLI tools 2025\"}","display_result":"","duration":"4s","id":"call_DrTyacDhKf","is_error":false,"mcp_server":{"name":"mcp-server"},"name":"search","result":"【0†20 Best AI Coding Assistant Tools [Updated Aug 2025]†https://www.qodo.ai/blog/best-ai-coding-assistant-tools】\nExplore our list of the 20 best AI coding assistant tools in 2025, boosting productivity and code quality for developers.\n\n【1†Best AI Tools for Coding in 2025: 6 Tools Worth Your Time†https://www.pragmaticcoders.com/resources/ai-developer-tools】\nWe tested 20+ AI coding tools – agents, reviewers, and assistants. See what stood out and what features the best tools offer.\n\n【2†The 8 Best AI Coding Tools in 2025†https://medium.com/@datajournal/the-8-best-ai-coding-tools-in-2025-cb323c5b9780】\nFind the best AI coding tools of 2025 for faster, smarter code. Compare features and pricing for Cursor, Copilot, Tabnine, and more.\n\n【3†What is the best AI engine for programming in September†https://www.reddit.com/r/GithubCopilot/comments/1nhju40/what_is_the_best_ai_engine_for_programming_in】\n#1 codex since, chatgpt 5 is best model. #2 Claude code, best CLI tool, good model. #3 Copilot, good auto complete, github integration.\n\n【4†13 Best AI Coding Tools for Complex Enterprise†https://www.augmentcode.com/guides/13-best-ai-coding-tools-for-complex-codebases】\nFind the best best coding ai with detailed comparisons, pros/cons, and implementation guides for dev teams.\n\n【5†AI Coding Tools in 2025: Welcome to the Agentic CLI Era†https://thenewstack.io/ai-coding-tools-in-2025-welcome-to-the-agentic-cli-era】\n2025 was the start of the Agentic Era, a new wave of software development. We moved beyond simple IDE chatbots and onto agentic CLIs.\n\n【6†Best AI Coding Assistants 2025: Tools for Developers†https://replit.com/discover/best-ai-coding-assistant】\nBest AI coding assistants of 2025 compared. See how tools like Replit, Cursor, and Copilot automate coding, testing, and deployment.\n\n【7†AI Code Tools: Complete Guide for Developers in 2025†https://codesubmit.io/blog/ai-code-tools】\nDiscover the best AI coding tools in 2025. Personal curation of 40+ tools including ChatGPT, GitHub Copilot, Cursor, and more.\n\n【8†Top 9 Cursor Alternatives in 2025: Best Open Source AI†https://cline.bot/blog/top-9-cursor-alternatives-in-2025-best-open-source-ai-dev-tools-for-developers】\nOpen-source AI coding assistant designed for self-hosting; Supports multiple programming languages and major IDEs; Real-time, context-aware code completion and ...\n\n【9†14 Best AI Developer Productivity Tools in 2025†https://www.greptile.com/content-library/14-best-developer-productivity-tools-2025】\nDiscover the top 14 AI developer productivity tools in 2025, including Greptile, GitHub Copilot, and Tabnine. Compare features, pricing ...\n","status":"completed"}},"type":"mcp"}</glm_block>
<glm_block view="" tool_call_name="search">{"data":{"browser":{"search_result":[{"title":"20 Best AI Coding Assistant Tools [Updated Aug 2025]","url":"https://www.qodo.ai/blog/best-ai-coding-assistant-tools","text":"Explore our list of the 20 best AI coding assistant tools in 2025, boosting productivity and code quality for developers.","index":1,"favicon":"","host_name":"www.qodo.ai"},{"title":"Best AI Tools for Coding in 2025: 6 Tools Worth Your Time","url":"https://www.pragmaticcoders.com/resources/ai-developer-tools","text":"We tested 20+ AI coding tools – agents, reviewers, and assistants. See what stood out and what features the best tools offer.","index":2,"favicon":"","host_name":"www.pragmaticcoders.com"},{"title":"The 8 Best AI Coding Tools in 2025","url":"https://medium.com/@datajournal/the-8-best-ai-coding-tools-in-2025-cb323c5b9780","text":"Find the best AI coding tools of 2025 for faster, smarter code. Compare features and pricing for Cursor, Copilot, Tabnine, and more.","index":3,"favicon":"","host_name":"medium.com"},{"title":"What is the best AI engine for programming in September","url":"https://www.reddit.com/r/GithubCopilot/comments/1nhju40/what_is_the_best_ai_engine_for_programming_in","text":"#1 codex since, chatgpt 5 is best model. #2 Claude code, best CLI tool, good model. #3 Copilot, good auto complete, github integration.","index":4,"favicon":"","host_name":"www.reddit.com"},{"title":"13 Best AI Coding Tools for Complex Enterprise","url":"https://www.augmentcode.com/guides/13-best-ai-coding-tools-for-complex-codebases","text":"Find the best best coding ai with detailed comparisons, pros/cons, and implementation guides for dev teams.","index":5,"favicon":"","host_name":"www.augmentcode.com"},{"title":"AI Coding Tools in 2025: Welcome to the Agentic CLI Era","url":"https://thenewstack.io/ai-coding-tools-in-2025-welcome-to-the-agentic-cli-era","text":"2025 was the start of the Agentic Era, a new wave of software development. We moved beyond simple IDE chatbots and onto agentic CLIs.","index":6,"favicon":"","host_name":"thenewstack.io"},{"title":"Best AI Coding Assistants 2025: Tools for Developers","url":"https://replit.com/discover/best-ai-coding-assistant","text":"Best AI coding assistants of 2025 compared. See how tools like Replit, Cursor, and Copilot automate coding, testing, and deployment.","index":7,"favicon":"","host_name":"replit.com"},{"title":"AI Code Tools: Complete Guide for Developers in 2025","url":"https://codesubmit.io/blog/ai-code-tools","text":"Discover the best AI coding tools in 2025. Personal curation of 40+ tools including ChatGPT, GitHub Copilot, Cursor, and more.","index":8,"favicon":"","host_name":"codesubmit.io"},{"title":"Top 9 Cursor Alternatives in 2025: Best Open Source AI","url":"https://cline.bot/blog/top-9-cursor-alternatives-in-2025-best-open-source-ai-dev-tools-for-developers","text":"Open-source AI coding assistant designed for self-hosting; Supports multiple programming languages and major IDEs; Real-time, context-aware code completion and ...","index":9,"favicon":"","host_name":"cline.bot"},{"title":"14 Best AI Developer Productivity Tools in 2025","url":"https://www.greptile.com/content-library/14-best-developer-productivity-tools-2025","text":"Discover the top 14 AI developer productivity tools in 2025, including Greptile, GitHub Copilot, and Tabnine. Compare features, pricing ...","index":10,"favicon":"","host_name":"www.greptile.com"},{"title":"Specification","url":"https://modelcontextprotocol.io/specification/2025-11-25","text":"Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and ...","index":11,"favicon":"","host_name":"modelcontextprotocol.io"},{"title":"7 Things to Know About MCP in 2025","url":"https://www.adskate.com/blogs/mcp-model-context-protocol-2025-guide","text":"Why It Matters in 2025. Cross-model support: MCP is model-agnostic and now supported by OpenAI, Anthropic, and Hugging Face. Scalability ...","index":12,"favicon":"","host_name":"www.adskate.com"},{"title":"What Is the Model Context Protocol (MCP) and How It Works","url":"https://www.descope.com/learn/post/mcp","text":"Visual Studio Code: Microsoft's IDE, which added MCP support in June 2025. JetBrains IDEs: Full coding suite that added AI Assistant MCP ...","index":13,"favicon":"","host_name":"www.descope.com"},{"title":"The Model Context Protocol's impact on 2025","url":"https://www.thoughtworks.com/en-us/insights/blog/generative-ai/model-context-protocol-mcp-impact-2025","text":"The Model Context Protocol (MCP) has been one of the key stories of 2025. We unpack its impact through the lens of the Thoughtworks ...","index":14,"favicon":"","host_name":"www.thoughtworks.com"},{"title":"IDE integration using Model Context Protocol (MCP)","url":"https://techdocs.broadcom.com/us/en/vmware-tanzu/spring/application-advisor/1-5/app-advisor/model-context-protocol-server.html","text":"To integrate the Application Advisor into your preferred IDE or MCP client, you can use the Application Advisor MCP server.","index":15,"favicon":"","host_name":"techdocs.broadcom.com"},{"title":"Model Context Protocol (MCP) | AI Assistant Documentation","url":"https://www.jetbrains.com/help/ai-assistant/mcp.html","text":"Starting with version 2025.2, JetBrains IDEs come with an integrated MCP server, allowing external clients such as Claude Desktop, Cursor, Codex ...","index":16,"favicon":"","host_name":"www.jetbrains.com"},{"title":"Model Context Protocol","url":"https://developers.openai.com/codex/mcp","text":"The CLI and the IDE extension share this configuration. Once you configure your MCP servers, you can switch between the two Codex clients without redoing setup.","index":17,"favicon":"","host_name":"developers.openai.com"},{"title":"Model Context Protocol (MCP): AI Integration Guide | Medium","url":"https://michielh.medium.com/the-model-context-protocol-mcp-step-by-step-connecting-ai-agents-to-everything-6b25a052b87c","text":"Evolving spec: OAuth integration, server remote hosting, and discovery registries are still under active development (early 2025); Potential ...","index":18,"favicon":"","host_name":"michielh.medium.com"},{"title":"What is Model Context Protocol (MCP)? The Complete Guide","url":"https://www.figma.com/resource-library/what-is-mcp","text":"In January and February 2025, many AI IDEs, like Cursor and Windsurf, started supporting MCP. In March, OpenAI added MCP support, and GitHub released its MCP ...","index":19,"favicon":"","host_name":"www.figma.com"},{"title":"Revolutionize AI Integration with MCP: The Future of Open","url":"https://www.baytechconsulting.com/blog/revolutionize-ai-integration-mcp-2025","text":"Explore the strategic architecture and implementation of the Model Context Protocol (MCP) in .NET Core, detailing roles, technical mechanics ...","index":20,"favicon":"","host_name":"www.baytechconsulting.com"}]},"metadata":{"arguments":"{\"query\":\"MCP (Model Context Protocol) IDE integration 2025\"}","display_result":"","duration":"4s","id":"call_hBFeCzsHb3","is_error":false,"mcp_server":{"name":"mcp-server"},"name":"search","result":"【10†Specification†https://modelcontextprotocol.io/specification/2025-11-25】\nModel Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and ...\n\n【11†7 Things to Know About MCP in 2025†https://www.adskate.com/blogs/mcp-model-context-protocol-2025-guide】\nWhy It Matters in 2025. Cross-model support: MCP is model-agnostic and now supported by OpenAI, Anthropic, and Hugging Face. Scalability ...\n\n【12†What Is the Model Context Protocol (MCP) and How It Works†https://www.descope.com/learn/post/mcp】\nVisual Studio Code: Microsoft's IDE, which added MCP support in June 2025. JetBrains IDEs: Full coding suite that added AI Assistant MCP ...\n\n【13†The Model Context Protocol's impact on 2025†https://www.thoughtworks.com/en-us/insights/blog/generative-ai/model-context-protocol-mcp-impact-2025】\nThe Model Context Protocol (MCP) has been one of the key stories of 2025. We unpack its impact through the lens of the Thoughtworks ...\n\n【14†IDE integration using Model Context Protocol (MCP)†https://techdocs.broadcom.com/us/en/vmware-tanzu/spring/application-advisor/1-5/app-advisor/model-context-protocol-server.html】\nTo integrate the Application Advisor into your preferred IDE or MCP client, you can use the Application Advisor MCP server.\n\n【15†Model Context Protocol (MCP) | AI Assistant Documentation†https://www.jetbrains.com/help/ai-assistant/mcp.html】\nStarting with version 2025.2, JetBrains IDEs come with an integrated MCP server, allowing external clients such as Claude Desktop, Cursor, Codex ...\n\n【16†Model Context Protocol†https://developers.openai.com/codex/mcp】\nThe CLI and the IDE extension share this configuration. Once you configure your MCP servers, you can switch between the two Codex clients without redoing setup.\n\n【17†Model Context Protocol (MCP): AI Integration Guide | Medium†https://michielh.medium.com/the-model-context-protocol-mcp-step-by-step-connecting-ai-agents-to-everything-6b25a052b87c】\nEvolving spec: OAuth integration, server remote hosting, and discovery registries are still under active development (early 2025); Potential ...\n\n【18†What is Model Context Protocol (MCP)? The Complete Guide†https://www.figma.com/resource-library/what-is-mcp】\nIn January and February 2025, many AI IDEs, like Cursor and Windsurf, started supporting MCP. In March, OpenAI added MCP support, and GitHub released its MCP ...\n\n【19†Revolutionize AI Integration with MCP: The Future of Open†https://www.baytechconsulting.com/blog/revolutionize-ai-integration-mcp-2025】\nExplore the strategic architecture and implementation of the Model Context Protocol (MCP) in .NET Core, detailing roles, technical mechanics ...\n","status":"completed"}},"type":"mcp"}</glm_block>


<details type="reasoning" done="true" duration="5" view="" last_tool_call_name="search">
> “vibe coding”は自然言語でAIに指示を出してコードを生成させる開発手法を指し、AIがプロセス全体を導くことを含みます。アプリの目標は、AI IDE、CLIツール、LLM API、MCP統合を活用して、このプロセスを自動化することです。開発の停滞を解消するためには、MCPサーバー構築、LLM APIの統合、AIエージェントの連携といった具体的な手順を踏む必要があります。これらの要素を組み合わせることで、シームレスな開発体験が実現できます。
</details>

<glm_block view="" tool_call_name="search">{"data":{"browser":{"search_result":[{"title":"20 Best AI Coding Assistant Tools [Updated Aug 2025]","url":"https://www.qodo.ai/blog/best-ai-coding-assistant-tools","text":"Explore our list of the 20 best AI coding assistant tools in 2025, boosting productivity and code quality for developers.","index":1,"favicon":"","host_name":"www.qodo.ai"},{"title":"Best AI Tools for Coding in 2025: 6 Tools Worth Your Time","url":"https://www.pragmaticcoders.com/resources/ai-developer-tools","text":"We tested 20+ AI coding tools – agents, reviewers, and assistants. See what stood out and what features the best tools offer.","index":2,"favicon":"","host_name":"www.pragmaticcoders.com"},{"title":"The 8 Best AI Coding Tools in 2025","url":"https://medium.com/@datajournal/the-8-best-ai-coding-tools-in-2025-cb323c5b9780","text":"Find the best AI coding tools of 2025 for faster, smarter code. Compare features and pricing for Cursor, Copilot, Tabnine, and more.","index":3,"favicon":"","host_name":"medium.com"},{"title":"What is the best AI engine for programming in September","url":"https://www.reddit.com/r/GithubCopilot/comments/1nhju40/what_is_the_best_ai_engine_for_programming_in","text":"#1 codex since, chatgpt 5 is best model. #2 Claude code, best CLI tool, good model. #3 Copilot, good auto complete, github integration.","index":4,"favicon":"","host_name":"www.reddit.com"},{"title":"13 Best AI Coding Tools for Complex Enterprise","url":"https://www.augmentcode.com/guides/13-best-ai-coding-tools-for-complex-codebases","text":"Find the best best coding ai with detailed comparisons, pros/cons, and implementation guides for dev teams.","index":5,"favicon":"","host_name":"www.augmentcode.com"},{"title":"AI Coding Tools in 2025: Welcome to the Agentic CLI Era","url":"https://thenewstack.io/ai-coding-tools-in-2025-welcome-to-the-agentic-cli-era","text":"2025 was the start of the Agentic Era, a new wave of software development. We moved beyond simple IDE chatbots and onto agentic CLIs.","index":6,"favicon":"","host_name":"thenewstack.io"},{"title":"Best AI Coding Assistants 2025: Tools for Developers","url":"https://replit.com/discover/best-ai-coding-assistant","text":"Best AI coding assistants of 2025 compared. See how tools like Replit, Cursor, and Copilot automate coding, testing, and deployment.","index":7,"favicon":"","host_name":"replit.com"},{"title":"AI Code Tools: Complete Guide for Developers in 2025","url":"https://codesubmit.io/blog/ai-code-tools","text":"Discover the best AI coding tools in 2025. Personal curation of 40+ tools including ChatGPT, GitHub Copilot, Cursor, and more.","index":8,"favicon":"","host_name":"codesubmit.io"},{"title":"Top 9 Cursor Alternatives in 2025: Best Open Source AI","url":"https://cline.bot/blog/top-9-cursor-alternatives-in-2025-best-open-source-ai-dev-tools-for-developers","text":"Open-source AI coding assistant designed for self-hosting; Supports multiple programming languages and major IDEs; Real-time, context-aware code completion and ...","index":9,"favicon":"","host_name":"cline.bot"},{"title":"14 Best AI Developer Productivity Tools in 2025","url":"https://www.greptile.com/content-library/14-best-developer-productivity-tools-2025","text":"Discover the top 14 AI developer productivity tools in 2025, including Greptile, GitHub Copilot, and Tabnine. Compare features, pricing ...","index":10,"favicon":"","host_name":"www.greptile.com"},{"title":"Specification","url":"https://modelcontextprotocol.io/specification/2025-11-25","text":"Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and ...","index":11,"favicon":"","host_name":"modelcontextprotocol.io"},{"title":"7 Things to Know About MCP in 2025","url":"https://www.adskate.com/blogs/mcp-model-context-protocol-2025-guide","text":"Why It Matters in 2025. Cross-model support: MCP is model-agnostic and now supported by OpenAI, Anthropic, and Hugging Face. Scalability ...","index":12,"favicon":"","host_name":"www.adskate.com"},{"title":"What Is the Model Context Protocol (MCP) and How It Works","url":"https://www.descope.com/learn/post/mcp","text":"Visual Studio Code: Microsoft's IDE, which added MCP support in June 2025. JetBrains IDEs: Full coding suite that added AI Assistant MCP ...","index":13,"favicon":"","host_name":"www.descope.com"},{"title":"The Model Context Protocol's impact on 2025","url":"https://www.thoughtworks.com/en-us/insights/blog/generative-ai/model-context-protocol-mcp-impact-2025","text":"The Model Context Protocol (MCP) has been one of the key stories of 2025. We unpack its impact through the lens of the Thoughtworks ...","index":14,"favicon":"","host_name":"www.thoughtworks.com"},{"title":"IDE integration using Model Context Protocol (MCP)","url":"https://techdocs.broadcom.com/us/en/vmware-tanzu/spring/application-advisor/1-5/app-advisor/model-context-protocol-server.html","text":"To integrate the Application Advisor into your preferred IDE or MCP client, you can use the Application Advisor MCP server.","index":15,"favicon":"","host_name":"techdocs.broadcom.com"},{"title":"Model Context Protocol (MCP) | AI Assistant Documentation","url":"https://www.jetbrains.com/help/ai-assistant/mcp.html","text":"Starting with version 2025.2, JetBrains IDEs come with an integrated MCP server, allowing external clients such as Claude Desktop, Cursor, Codex ...","index":16,"favicon":"","host_name":"www.jetbrains.com"},{"title":"Model Context Protocol","url":"https://developers.openai.com/codex/mcp","text":"The CLI and the IDE extension share this configuration. Once you configure your MCP servers, you can switch between the two Codex clients without redoing setup.","index":17,"favicon":"","host_name":"developers.openai.com"},{"title":"Model Context Protocol (MCP): AI Integration Guide | Medium","url":"https://michielh.medium.com/the-model-context-protocol-mcp-step-by-step-connecting-ai-agents-to-everything-6b25a052b87c","text":"Evolving spec: OAuth integration, server remote hosting, and discovery registries are still under active development (early 2025); Potential ...","index":18,"favicon":"","host_name":"michielh.medium.com"},{"title":"What is Model Context Protocol (MCP)? The Complete Guide","url":"https://www.figma.com/resource-library/what-is-mcp","text":"In January and February 2025, many AI IDEs, like Cursor and Windsurf, started supporting MCP. In March, OpenAI added MCP support, and GitHub released its MCP ...","index":19,"favicon":"","host_name":"www.figma.com"},{"title":"Revolutionize AI Integration with MCP: The Future of Open","url":"https://www.baytechconsulting.com/blog/revolutionize-ai-integration-mcp-2025","text":"Explore the strategic architecture and implementation of the Model Context Protocol (MCP) in .NET Core, detailing roles, technical mechanics ...","index":20,"favicon":"","host_name":"www.baytechconsulting.com"},{"title":"Vibe Coding Explained: Tools and Guides","url":"https://cloud.google.com/discover/what-is-vibe-coding","text":"Vibe coding is a style of programming that uses natural language prompts to assist with the coding process. Learn how it works with Google ...","index":21,"favicon":"","host_name":"cloud.google.com"},{"title":"Vibe coding","url":"https://en.wikipedia.org/wiki/Vibe_coding","text":"Vibe coding is an AI-assisted software development technique. The term was introduced by Andrej Karpathy in February 2025. The term was listed on the Merriam- ...","index":22,"favicon":"","host_name":"en.wikipedia.org"},{"title":"What is Vibe Coding?","url":"https://www.ibm.com/think/topics/vibe-coding","text":"Vibe coding is a fresh take in coding where users express their intention using plain speech and the AI transforms that thinking into executable code.","index":23,"favicon":"","host_name":"www.ibm.com"},{"title":"Vibe coding is not the same as AI-Assisted engineering.","url":"https://medium.com/@addyosmani/vibe-coding-is-not-the-same-as-ai-assisted-engineering-3f81088d5b98","text":"In an August 2025 survey by Final Round AI, 18 CTOs were asked about vibe coding and 16 reported experiencing production disasters directly ...","index":24,"favicon":"","host_name":"medium.com"},{"title":"What is the exact definition of \"vibe coding\"? : r/ClaudeAI","url":"https://www.reddit.com/r/ClaudeAI/comments/1j6z4ft/what_is_the_exact_definition_of_vibe_coding","text":"you tell the AI what you want, and it writes the code. Coined by Karpathy, it's fast, intuitive, and great for prototypes.","index":25,"favicon":"","host_name":"www.reddit.com"},{"title":"How to Vibe Code: 5 Simple Tips for Using AI to Become a","url":"https://www.pcmag.com/explainers/how-to-vibe-code-5-simple-tips-for-using-ai-to-become-a-programmer","text":"In short, vibe coding means giving an AI a programming task and letting it complete it for you. It generates the code, makes architecture ...","index":26,"favicon":"","host_name":"www.pcmag.com"},{"title":"What is Vibe Coding? Prompting AI Software Development","url":"https://www.sonarsource.com/resources/library/vibe-coding","text":"Vibe coding basically means using AI, like those big language models, to write code for you. You just tell it what you want in regular words, and boom, it tries ...","index":27,"favicon":"","host_name":"www.sonarsource.com"},{"title":"What is Vibe Coding? Your 2025 Vibe Coding Guide","url":"https://www.synergylabs.co/blog/what-is-vibe-coding-your-2025-vibe-coding-guide","text":"Vibe Coding is a development paradigm where programmers essentially \"surrender to the vibes\" of AI-powered coding assistants.","index":28,"favicon":"","host_name":"www.synergylabs.co"},{"title":"Understanding Vibe Coding and Its Implications","url":"https://www.blackduck.com/blog/vibe-coding-and-its-implications.html","text":"“Vibe coding” means using natural language and AI tools to generate and refine code. This approach lowers barriers to entry for coding, ...","index":29,"favicon":"","host_name":"www.blackduck.com"},{"title":"Vibe Coding: The Future of Building Software (Without","url":"https://www.siddharthbharath.com/vibe-coding","text":"Vibe coding is the new frontier where you build software by telling an AI what you want and letting it spit out the code.","index":30,"favicon":"","host_name":"www.siddharthbharath.com"},{"title":"Top 10 AI Orchestration Tools in 2025","url":"https://www.kubiya.ai/blog/ai-orchestration-tools","text":"Discover the top 10 AI orchestration tools in 2025. Explore features, benefits, and innovations driving seamless AI workflow automation and ...","index":31,"favicon":"","host_name":"www.kubiya.ai"},{"title":"What is AI Orchestration? 21+ Tools to Consider in 2025","url":"https://akka.io/blog/ai-orchestration-tools","text":"Discover top AI orchestration tools that streamline workflows and optimize performance across your tech stack in this comprehensive guide.","index":32,"favicon":"","host_name":"akka.io"},{"title":"10 Best AI Orchestration Platforms in 2025","url":"https://www.domo.com/learn/article/best-ai-orchestration-platforms","text":"Microsoft AutoGen is an orchestration framework for building multi-agent AI systems. It allows developers to coordinate multiple LLMs, APIs, and ...","index":33,"favicon":"","host_name":"www.domo.com"},{"title":"Best AI Agent Orchestration Tools in 2025: Detailed List","url":"https://www.reddit.com/r/AI_Agents/comments/1nk12yi/best_ai_agent_orchestration_tools_in_2025","text":"How about Google ADK? It's easy to start and includes many built-in features like UI, deployment, tools, etc.","index":34,"favicon":"","host_name":"www.reddit.com"},{"title":"Top 10 AI Agent Frameworks for 2025: LangChain Leads","url":"https://medium.com/@julio.pessan.pessan/top-10-ai-agent-frameworks-for-2025-langchain-leads-the-pack-in-multi-agent-orchestration-c115044bcb60","text":"Top 10 AI Agent Frameworks for 2025: LangChain Leads the Pack in Multi-Agent Orchestration · 1. LangChain — The Orchestra Conductor · 2. CrewAI — ...","index":35,"favicon":"","host_name":"medium.com"},{"title":"Best AI Workflow Automation Tools (2025)","url":"https://www.whalesync.com/blog/best-ai-workflow-automation-tools-2025","text":"These new AI-first platforms unlock mind-blowing functionality from PDF analysis to AI note-taking, sales agents and more. To help keep pace ...","index":36,"favicon":"","host_name":"www.whalesync.com"},{"title":"Top 13 Agentic AI Tools in 2025 and Their Key Features","url":"https://www.lasso.security/blog/agentic-ai-tools","text":"Explore the leading agentic AI tools transforming enterprise automation in 2025. Learn how these autonomous applications work, and how to ...","index":37,"favicon":"","host_name":"www.lasso.security"},{"title":"10 Best AI Automation Platforms: Tested & Reviewed","url":"https://www.lindy.ai/blog/ai-automation-platform","text":"Lindy is an AI automation platform that helps teams delegate repetitive tasks to AI agents. These agents can send follow-ups, qualify leads, update CRMs, ...","index":38,"favicon":"","host_name":"www.lindy.ai"},{"title":"AI Agent Orchestration Frameworks: Which One Works","url":"https://blog.n8n.io/ai-agent-orchestration-frameworks","text":"Discover AI agent orchestration frameworks like n8n, LangGraph, CrewAI that power scalable, multi-agent AI orchestration in 2025.","index":39,"favicon":"","host_name":"blog.n8n.io"},{"title":"Top 7 Free AI Agent Frameworks [2025]","url":"https://botpress.com/blog/ai-agent-frameworks","text":"The key benefit to using an AI agent framework is that it abstracts complexity, breaking tasks into manageable steps, and ensures scalability.","index":40,"favicon":"","host_name":"botpress.com"},{"title":"How LLMs Invoke Real-World APIs (OpenAI & Gemini","url":"https://medium.com/@akankshasinha247/function-calling-how-llms-invoke-real-world-apis-openai-gemini-examples-266bdd802c03","text":"Function Calling allows an LLM to dynamically decide whether to invoke an external function / tool / API, automatically generate structured arguments for that ...","index":41,"favicon":"","host_name":"medium.com"},{"title":"How to Orchestrate Multiple Language Models: A Step-by","url":"https://www.revanthquicklearn.com/post/how-to-orchestrate-multiple-language-models-a-step-by-step-guide-for-beginners","text":"Working with multiple Large Language Models (LLMs) opens up incredible possibilities for building intelligent applications.","index":42,"favicon":"","host_name":"www.revanthquicklearn.com"},{"title":"Research issue: gather examples of multi-modal API calls","url":"https://github.com/simonw/llm/issues/557","text":"To aid in the design for both of these: #331 #556 I'm going to gather a bunch of examples of how different LLMs accept multi-modal inputs.","index":43,"favicon":"","host_name":"github.com"},{"title":"10 Python One-Liners for Calling LLMs from Your Code","url":"https://machinelearningmastery.com/10-python-one-liners-for-calling-llms-from-your-code","text":"In this article, you'll see ten Python one-liners that call and interact with LLMs. We will cover: Hosted APIs: examples using OpenAI, Anthropic ...","index":44,"favicon":"","host_name":"machinelearningmastery.com"},{"title":"How do I send 1 prompt to multiple LLM APIs (ChatGPT","url":"https://www.reddit.com/r/LocalLLaMA/comments/1pch3lt/how_do_i_send_1_prompt_to_multiple_llm_apis","text":"How do I send 1 prompt to multiple LLM APIs (ChatGPT, Gemini, Perplexity) and auto-merge their answers into a unified output? Question | Help.","index":45,"favicon":"","host_name":"www.reddit.com"},{"title":"Parallel LLM Calls from Scratch — Tutorial For Dummies","url":"https://dev.to/zachary62/parallel-llm-calls-from-scratch-tutorial-for-dummies-using-pocketflow-1972","text":"This guide breaks down how to drastically speed things up using parallel processing with the PocketFlow Parallel Batch Example!","index":46,"favicon":"","host_name":"dev.to"},{"title":"Function calling with the Gemini API | Google AI for Developers","url":"https://ai.google.dev/gemini-api/docs/function-calling","text":"You can enable multiple tools combining native tools with function calling at the same time. Here's an example that enables two tools, Grounding with Google ...","index":47,"favicon":"","host_name":"ai.google.dev"},{"title":"How to Chain Tools in OpenAI Function Calls? - API","url":"https://community.openai.com/t/how-to-chain-tools-in-openai-function-calls/1088098","text":"Currently, this often requires two separate calls to the LLM: 1. GPT asks to invoke create_data() and receives the result. 2. GPT processes the ...","index":48,"favicon":"","host_name":"community.openai.com"},{"title":"How to use Anthropic MCP Server with open LLMs, OpenAI","url":"https://www.philschmid.de/mcp-example-llama","text":"In this blog post, you will learn how to use MCP Servers with any open LLM, OpenAI, or Google Gemini. You will learn how to build a simple CLI Agent that can ...","index":49,"favicon":"","host_name":"www.philschmid.de"},{"title":"The guide to structured outputs and function calling with","url":"https://agenta.ai/blog/the-guide-to-structured-outputs-and-function-calling-with-llms","text":"Complete production guide with OpenAI, Claude, and Gemini code examples for consistent data extraction. Sep 10, 2025. -. 10 minutes.","index":50,"favicon":"","host_name":"agenta.ai"},{"title":"Build an MCP server","url":"https://modelcontextprotocol.io/docs/develop/build-server","text":"Learn how to build your own MCP client that can connect to your server · Example servers. Check out our gallery of official MCP servers and implementations ... We’ll build a server that exposes two tools: get_alerts and get_forecast. Then we’ll connect the server to an MCP host (in this case, Claude for Desktop): Servers can connect to any client. We’ve chosen Claude for Desktop here for simplicity, but we also have guides on building your own client as well as a list of other clients here.","index":51,"favicon":"","host_name":"modelcontextprotocol.io"},{"title":"Build Your First MCP Server in 15 Minutes (Complete Code)","url":"https://medium.com/data-science-collective/build-your-first-mcp-server-in-15-minutes-complete-code-d63f85c0ce79","text":"Learn to build your first MCP server in 15 minutes. Step-by-step Python tutorial with FastMCP covering tools, clients, and LLM integration.","index":52,"favicon":"","host_name":"medium.com"},{"title":"Build your MCP server","url":"https://developers.openai.com/apps-sdk/build/mcp-server","text":"By the end of this guide, you'll know how to connect your backend MCP server to ChatGPT, define tools, register UI templates, and tie everything together ...","index":53,"favicon":"","host_name":"developers.openai.com"},{"title":"MCP server: A step-by-step guide to building from scratch","url":"https://composio.dev/blog/mcp-server-step-by-step-guide-to-building-from-scrtch","text":"How to Build an MCP Server · 1. Work Environment Setup · 2. Writing the Server Code · 3. Running & Testing the Server Locally.","index":54,"favicon":"","host_name":"composio.dev"},{"title":"Everything I learned building a remote MCP server","url":"https://www.reddit.com/r/mcp/comments/1ksncf3/how_to_mcp_everything_i_learned_building_a_remote","text":"It covers authentication, OAuth authorization, session management, troubleshooting and all the steps you need to pair with the major LLM apps.","index":55,"favicon":"","host_name":"www.reddit.com"},{"title":"Build Your Own MCP Server & Client with Python (2025","url":"https://datacouch.io/blog/build-your-own-mcp-server-client-with-python-2025-guide","text":"Learn to build your own MCP Server & Client using Python. Step-by-step 2025 guide to connect LLMs with tools, APIs, and real-time data.","index":56,"favicon":"","host_name":"datacouch.io"},{"title":"Build an MCP Server: Complete MCP Tutorial for Beginners","url":"https://www.codecademy.com/article/build-an-mcp-server","text":"What is an example of an MCP server? A simple example is a calendar MCP server that can add, view, or delete events on your computer. When Claude asks to ...","index":57,"favicon":"","host_name":"www.codecademy.com"},{"title":"How to Build Your Own MCP Server from Scratch [6 Steps]","url":"https://www.intuz.com/blog/how-to-build-mcp-server","text":"Build your MCP server from scratch in 6 practical steps. Learn core components, pro tips, and real examples from Intuz.","index":58,"favicon":"","host_name":"www.intuz.com"},{"title":"Create a minimal MCP server using C# and publish to NuGet","url":"https://learn.microsoft.com/en-us/dotnet/ai/quickstarts/build-mcp-server","text":"In this quickstart, you create a minimal Model Context Protocol (MCP) server using the C# SDK for MCP, connect to it using GitHub Copilot, and publish it to NuGet.","index":59,"favicon":"","host_name":"learn.microsoft.com"},{"title":"Building and Deploying an MCP Server with C#: A Complete","url":"https://atalupadhyay.wordpress.com/2025/04/11/building-and-deploying-an-mcp-server-with-c-a-complete-guide","text":"In this hands-on tutorial, I’ll walk you through creating a Model Context Protocol (MCP) server using C#, based on James Montemagno’s video. MCP servers provide a standardized way to give additional context to AI models through clients like GitHub Copilot.","index":60,"favicon":"","host_name":"atalupadhyay.wordpress.com"}]},"metadata":{"arguments":"{\"query\":\"build MCP server example 2025\"}","display_result":"","duration":"6s","id":"call_PeqvpM8EO4","is_error":false,"mcp_server":{"name":"mcp-server"},"name":"search","result":"【50†Build an MCP server†https://modelcontextprotocol.io/docs/develop/build-server】\nLearn how to build your own MCP client that can connect to your server · Example servers. Check out our gallery of official MCP servers and implementations ... We’ll build a server that exposes two tools: get_alerts and get_forecast. Then we’ll connect the server to an MCP host (in this case, Claude for Desktop): Servers can connect to any client. We’ve chosen Claude for Desktop here for simplicity, but we also have guides on building your own client as well as a list of other clients here.\n\n【51†Build Your First MCP Server in 15 Minutes (Complete Code)†https://medium.com/data-science-collective/build-your-first-mcp-server-in-15-minutes-complete-code-d63f85c0ce79】\nLearn to build your first MCP server in 15 minutes. Step-by-step Python tutorial with FastMCP covering tools, clients, and LLM integration.\n\n【52†Build your MCP server†https://developers.openai.com/apps-sdk/build/mcp-server】\nBy the end of this guide, you'll know how to connect your backend MCP server to ChatGPT, define tools, register UI templates, and tie everything together ...\n\n【53†MCP server: A step-by-step guide to building from scratch†https://composio.dev/blog/mcp-server-step-by-step-guide-to-building-from-scrtch】\nHow to Build an MCP Server · 1. Work Environment Setup · 2. Writing the Server Code · 3. Running & Testing the Server Locally.\n\n【54†Everything I learned building a remote MCP server†https://www.reddit.com/r/mcp/comments/1ksncf3/how_to_mcp_everything_i_learned_building_a_remote】\nIt covers authentication, OAuth authorization, session management, troubleshooting and all the steps you need to pair with the major LLM apps.\n\n【55†Build Your Own MCP Server & Client with Python (2025†https://datacouch.io/blog/build-your-own-mcp-server-client-with-python-2025-guide】\nLearn to build your own MCP Server & Client using Python. Step-by-step 2025 guide to connect LLMs with tools, APIs, and real-time data.\n\n【56†Build an MCP Server: Complete MCP Tutorial for Beginners†https://www.codecademy.com/article/build-an-mcp-server】\nWhat is an example of an MCP server? A simple example is a calendar MCP server that can add, view, or delete events on your computer. When Claude asks to ...\n\n【57†How to Build Your Own MCP Server from Scratch [6 Steps]†https://www.intuz.com/blog/how-to-build-mcp-server】\nBuild your MCP server from scratch in 6 practical steps. Learn core components, pro tips, and real examples from Intuz.\n\n【58†Create a minimal MCP server using C# and publish to NuGet†https://learn.microsoft.com/en-us/dotnet/ai/quickstarts/build-mcp-server】\nIn this quickstart, you create a minimal Model Context Protocol (MCP) server using the C# SDK for MCP, connect to it using GitHub Copilot, and publish it to NuGet.\n\n【59†Building and Deploying an MCP Server with C#: A Complete†https://atalupadhyay.wordpress.com/2025/04/11/building-and-deploying-an-mcp-server-with-c-a-complete-guide】\nIn this hands-on tutorial, I’ll walk you through creating a Model Context Protocol (MCP) server using C#, based on James Montemagno’s video. MCP servers provide a standardized way to give additional context to AI models through clients like GitHub Copilot.\n","status":"completed"}},"type":"mcp"}</glm_block>
<glm_block view="" tool_call_name="search">{"data":{"browser":{"search_result":[{"title":"20 Best AI Coding Assistant Tools [Updated Aug 2025]","url":"https://www.qodo.ai/blog/best-ai-coding-assistant-tools","text":"Explore our list of the 20 best AI coding assistant tools in 2025, boosting productivity and code quality for developers.","index":1,"favicon":"","host_name":"www.qodo.ai"},{"title":"Best AI Tools for Coding in 2025: 6 Tools Worth Your Time","url":"https://www.pragmaticcoders.com/resources/ai-developer-tools","text":"We tested 20+ AI coding tools – agents, reviewers, and assistants. See what stood out and what features the best tools offer.","index":2,"favicon":"","host_name":"www.pragmaticcoders.com"},{"title":"The 8 Best AI Coding Tools in 2025","url":"https://medium.com/@datajournal/the-8-best-ai-coding-tools-in-2025-cb323c5b9780","text":"Find the best AI coding tools of 2025 for faster, smarter code. Compare features and pricing for Cursor, Copilot, Tabnine, and more.","index":3,"favicon":"","host_name":"medium.com"},{"title":"What is the best AI engine for programming in September","url":"https://www.reddit.com/r/GithubCopilot/comments/1nhju40/what_is_the_best_ai_engine_for_programming_in","text":"#1 codex since, chatgpt 5 is best model. #2 Claude code, best CLI tool, good model. #3 Copilot, good auto complete, github integration.","index":4,"favicon":"","host_name":"www.reddit.com"},{"title":"13 Best AI Coding Tools for Complex Enterprise","url":"https://www.augmentcode.com/guides/13-best-ai-coding-tools-for-complex-codebases","text":"Find the best best coding ai with detailed comparisons, pros/cons, and implementation guides for dev teams.","index":5,"favicon":"","host_name":"www.augmentcode.com"},{"title":"AI Coding Tools in 2025: Welcome to the Agentic CLI Era","url":"https://thenewstack.io/ai-coding-tools-in-2025-welcome-to-the-agentic-cli-era","text":"2025 was the start of the Agentic Era, a new wave of software development. We moved beyond simple IDE chatbots and onto agentic CLIs.","index":6,"favicon":"","host_name":"thenewstack.io"},{"title":"Best AI Coding Assistants 2025: Tools for Developers","url":"https://replit.com/discover/best-ai-coding-assistant","text":"Best AI coding assistants of 2025 compared. See how tools like Replit, Cursor, and Copilot automate coding, testing, and deployment.","index":7,"favicon":"","host_name":"replit.com"},{"title":"AI Code Tools: Complete Guide for Developers in 2025","url":"https://codesubmit.io/blog/ai-code-tools","text":"Discover the best AI coding tools in 2025. Personal curation of 40+ tools including ChatGPT, GitHub Copilot, Cursor, and more.","index":8,"favicon":"","host_name":"codesubmit.io"},{"title":"Top 9 Cursor Alternatives in 2025: Best Open Source AI","url":"https://cline.bot/blog/top-9-cursor-alternatives-in-2025-best-open-source-ai-dev-tools-for-developers","text":"Open-source AI coding assistant designed for self-hosting; Supports multiple programming languages and major IDEs; Real-time, context-aware code completion and ...","index":9,"favicon":"","host_name":"cline.bot"},{"title":"14 Best AI Developer Productivity Tools in 2025","url":"https://www.greptile.com/content-library/14-best-developer-productivity-tools-2025","text":"Discover the top 14 AI developer productivity tools in 2025, including Greptile, GitHub Copilot, and Tabnine. Compare features, pricing ...","index":10,"favicon":"","host_name":"www.greptile.com"},{"title":"Specification","url":"https://modelcontextprotocol.io/specification/2025-11-25","text":"Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and ...","index":11,"favicon":"","host_name":"modelcontextprotocol.io"},{"title":"7 Things to Know About MCP in 2025","url":"https://www.adskate.com/blogs/mcp-model-context-protocol-2025-guide","text":"Why It Matters in 2025. Cross-model support: MCP is model-agnostic and now supported by OpenAI, Anthropic, and Hugging Face. Scalability ...","index":12,"favicon":"","host_name":"www.adskate.com"},{"title":"What Is the Model Context Protocol (MCP) and How It Works","url":"https://www.descope.com/learn/post/mcp","text":"Visual Studio Code: Microsoft's IDE, which added MCP support in June 2025. JetBrains IDEs: Full coding suite that added AI Assistant MCP ...","index":13,"favicon":"","host_name":"www.descope.com"},{"title":"The Model Context Protocol's impact on 2025","url":"https://www.thoughtworks.com/en-us/insights/blog/generative-ai/model-context-protocol-mcp-impact-2025","text":"The Model Context Protocol (MCP) has been one of the key stories of 2025. We unpack its impact through the lens of the Thoughtworks ...","index":14,"favicon":"","host_name":"www.thoughtworks.com"},{"title":"IDE integration using Model Context Protocol (MCP)","url":"https://techdocs.broadcom.com/us/en/vmware-tanzu/spring/application-advisor/1-5/app-advisor/model-context-protocol-server.html","text":"To integrate the Application Advisor into your preferred IDE or MCP client, you can use the Application Advisor MCP server.","index":15,"favicon":"","host_name":"techdocs.broadcom.com"},{"title":"Model Context Protocol (MCP) | AI Assistant Documentation","url":"https://www.jetbrains.com/help/ai-assistant/mcp.html","text":"Starting with version 2025.2, JetBrains IDEs come with an integrated MCP server, allowing external clients such as Claude Desktop, Cursor, Codex ...","index":16,"favicon":"","host_name":"www.jetbrains.com"},{"title":"Model Context Protocol","url":"https://developers.openai.com/codex/mcp","text":"The CLI and the IDE extension share this configuration. Once you configure your MCP servers, you can switch between the two Codex clients without redoing setup.","index":17,"favicon":"","host_name":"developers.openai.com"},{"title":"Model Context Protocol (MCP): AI Integration Guide | Medium","url":"https://michielh.medium.com/the-model-context-protocol-mcp-step-by-step-connecting-ai-agents-to-everything-6b25a052b87c","text":"Evolving spec: OAuth integration, server remote hosting, and discovery registries are still under active development (early 2025); Potential ...","index":18,"favicon":"","host_name":"michielh.medium.com"},{"title":"What is Model Context Protocol (MCP)? The Complete Guide","url":"https://www.figma.com/resource-library/what-is-mcp","text":"In January and February 2025, many AI IDEs, like Cursor and Windsurf, started supporting MCP. In March, OpenAI added MCP support, and GitHub released its MCP ...","index":19,"favicon":"","host_name":"www.figma.com"},{"title":"Revolutionize AI Integration with MCP: The Future of Open","url":"https://www.baytechconsulting.com/blog/revolutionize-ai-integration-mcp-2025","text":"Explore the strategic architecture and implementation of the Model Context Protocol (MCP) in .NET Core, detailing roles, technical mechanics ...","index":20,"favicon":"","host_name":"www.baytechconsulting.com"},{"title":"Vibe Coding Explained: Tools and Guides","url":"https://cloud.google.com/discover/what-is-vibe-coding","text":"Vibe coding is a style of programming that uses natural language prompts to assist with the coding process. Learn how it works with Google ...","index":21,"favicon":"","host_name":"cloud.google.com"},{"title":"Vibe coding","url":"https://en.wikipedia.org/wiki/Vibe_coding","text":"Vibe coding is an AI-assisted software development technique. The term was introduced by Andrej Karpathy in February 2025. The term was listed on the Merriam- ...","index":22,"favicon":"","host_name":"en.wikipedia.org"},{"title":"What is Vibe Coding?","url":"https://www.ibm.com/think/topics/vibe-coding","text":"Vibe coding is a fresh take in coding where users express their intention using plain speech and the AI transforms that thinking into executable code.","index":23,"favicon":"","host_name":"www.ibm.com"},{"title":"Vibe coding is not the same as AI-Assisted engineering.","url":"https://medium.com/@addyosmani/vibe-coding-is-not-the-same-as-ai-assisted-engineering-3f81088d5b98","text":"In an August 2025 survey by Final Round AI, 18 CTOs were asked about vibe coding and 16 reported experiencing production disasters directly ...","index":24,"favicon":"","host_name":"medium.com"},{"title":"What is the exact definition of \"vibe coding\"? : r/ClaudeAI","url":"https://www.reddit.com/r/ClaudeAI/comments/1j6z4ft/what_is_the_exact_definition_of_vibe_coding","text":"you tell the AI what you want, and it writes the code. Coined by Karpathy, it's fast, intuitive, and great for prototypes.","index":25,"favicon":"","host_name":"www.reddit.com"},{"title":"How to Vibe Code: 5 Simple Tips for Using AI to Become a","url":"https://www.pcmag.com/explainers/how-to-vibe-code-5-simple-tips-for-using-ai-to-become-a-programmer","text":"In short, vibe coding means giving an AI a programming task and letting it complete it for you. It generates the code, makes architecture ...","index":26,"favicon":"","host_name":"www.pcmag.com"},{"title":"What is Vibe Coding? Prompting AI Software Development","url":"https://www.sonarsource.com/resources/library/vibe-coding","text":"Vibe coding basically means using AI, like those big language models, to write code for you. You just tell it what you want in regular words, and boom, it tries ...","index":27,"favicon":"","host_name":"www.sonarsource.com"},{"title":"What is Vibe Coding? Your 2025 Vibe Coding Guide","url":"https://www.synergylabs.co/blog/what-is-vibe-coding-your-2025-vibe-coding-guide","text":"Vibe Coding is a development paradigm where programmers essentially \"surrender to the vibes\" of AI-powered coding assistants.","index":28,"favicon":"","host_name":"www.synergylabs.co"},{"title":"Understanding Vibe Coding and Its Implications","url":"https://www.blackduck.com/blog/vibe-coding-and-its-implications.html","text":"“Vibe coding” means using natural language and AI tools to generate and refine code. This approach lowers barriers to entry for coding, ...","index":29,"favicon":"","host_name":"www.blackduck.com"},{"title":"Vibe Coding: The Future of Building Software (Without","url":"https://www.siddharthbharath.com/vibe-coding","text":"Vibe coding is the new frontier where you build software by telling an AI what you want and letting it spit out the code.","index":30,"favicon":"","host_name":"www.siddharthbharath.com"},{"title":"Top 10 AI Orchestration Tools in 2025","url":"https://www.kubiya.ai/blog/ai-orchestration-tools","text":"Discover the top 10 AI orchestration tools in 2025. Explore features, benefits, and innovations driving seamless AI workflow automation and ...","index":31,"favicon":"","host_name":"www.kubiya.ai"},{"title":"What is AI Orchestration? 21+ Tools to Consider in 2025","url":"https://akka.io/blog/ai-orchestration-tools","text":"Discover top AI orchestration tools that streamline workflows and optimize performance across your tech stack in this comprehensive guide.","index":32,"favicon":"","host_name":"akka.io"},{"title":"10 Best AI Orchestration Platforms in 2025","url":"https://www.domo.com/learn/article/best-ai-orchestration-platforms","text":"Microsoft AutoGen is an orchestration framework for building multi-agent AI systems. It allows developers to coordinate multiple LLMs, APIs, and ...","index":33,"favicon":"","host_name":"www.domo.com"},{"title":"Best AI Agent Orchestration Tools in 2025: Detailed List","url":"https://www.reddit.com/r/AI_Agents/comments/1nk12yi/best_ai_agent_orchestration_tools_in_2025","text":"How about Google ADK? It's easy to start and includes many built-in features like UI, deployment, tools, etc.","index":34,"favicon":"","host_name":"www.reddit.com"},{"title":"Top 10 AI Agent Frameworks for 2025: LangChain Leads","url":"https://medium.com/@julio.pessan.pessan/top-10-ai-agent-frameworks-for-2025-langchain-leads-the-pack-in-multi-agent-orchestration-c115044bcb60","text":"Top 10 AI Agent Frameworks for 2025: LangChain Leads the Pack in Multi-Agent Orchestration · 1. LangChain — The Orchestra Conductor · 2. CrewAI — ...","index":35,"favicon":"","host_name":"medium.com"},{"title":"Best AI Workflow Automation Tools (2025)","url":"https://www.whalesync.com/blog/best-ai-workflow-automation-tools-2025","text":"These new AI-first platforms unlock mind-blowing functionality from PDF analysis to AI note-taking, sales agents and more. To help keep pace ...","index":36,"favicon":"","host_name":"www.whalesync.com"},{"title":"Top 13 Agentic AI Tools in 2025 and Their Key Features","url":"https://www.lasso.security/blog/agentic-ai-tools","text":"Explore the leading agentic AI tools transforming enterprise automation in 2025. Learn how these autonomous applications work, and how to ...","index":37,"favicon":"","host_name":"www.lasso.security"},{"title":"10 Best AI Automation Platforms: Tested & Reviewed","url":"https://www.lindy.ai/blog/ai-automation-platform","text":"Lindy is an AI automation platform that helps teams delegate repetitive tasks to AI agents. These agents can send follow-ups, qualify leads, update CRMs, ...","index":38,"favicon":"","host_name":"www.lindy.ai"},{"title":"AI Agent Orchestration Frameworks: Which One Works","url":"https://blog.n8n.io/ai-agent-orchestration-frameworks","text":"Discover AI agent orchestration frameworks like n8n, LangGraph, CrewAI that power scalable, multi-agent AI orchestration in 2025.","index":39,"favicon":"","host_name":"blog.n8n.io"},{"title":"Top 7 Free AI Agent Frameworks [2025]","url":"https://botpress.com/blog/ai-agent-frameworks","text":"The key benefit to using an AI agent framework is that it abstracts complexity, breaking tasks into manageable steps, and ensures scalability.","index":40,"favicon":"","host_name":"botpress.com"},{"title":"How LLMs Invoke Real-World APIs (OpenAI & Gemini","url":"https://medium.com/@akankshasinha247/function-calling-how-llms-invoke-real-world-apis-openai-gemini-examples-266bdd802c03","text":"Function Calling allows an LLM to dynamically decide whether to invoke an external function / tool / API, automatically generate structured arguments for that ...","index":41,"favicon":"","host_name":"medium.com"},{"title":"How to Orchestrate Multiple Language Models: A Step-by","url":"https://www.revanthquicklearn.com/post/how-to-orchestrate-multiple-language-models-a-step-by-step-guide-for-beginners","text":"Working with multiple Large Language Models (LLMs) opens up incredible possibilities for building intelligent applications.","index":42,"favicon":"","host_name":"www.revanthquicklearn.com"},{"title":"Research issue: gather examples of multi-modal API calls","url":"https://github.com/simonw/llm/issues/557","text":"To aid in the design for both of these: #331 #556 I'm going to gather a bunch of examples of how different LLMs accept multi-modal inputs.","index":43,"favicon":"","host_name":"github.com"},{"title":"10 Python One-Liners for Calling LLMs from Your Code","url":"https://machinelearningmastery.com/10-python-one-liners-for-calling-llms-from-your-code","text":"In this article, you'll see ten Python one-liners that call and interact with LLMs. We will cover: Hosted APIs: examples using OpenAI, Anthropic ...","index":44,"favicon":"","host_name":"machinelearningmastery.com"},{"title":"How do I send 1 prompt to multiple LLM APIs (ChatGPT","url":"https://www.reddit.com/r/LocalLLaMA/comments/1pch3lt/how_do_i_send_1_prompt_to_multiple_llm_apis","text":"How do I send 1 prompt to multiple LLM APIs (ChatGPT, Gemini, Perplexity) and auto-merge their answers into a unified output? Question | Help.","index":45,"favicon":"","host_name":"www.reddit.com"},{"title":"Parallel LLM Calls from Scratch — Tutorial For Dummies","url":"https://dev.to/zachary62/parallel-llm-calls-from-scratch-tutorial-for-dummies-using-pocketflow-1972","text":"This guide breaks down how to drastically speed things up using parallel processing with the PocketFlow Parallel Batch Example!","index":46,"favicon":"","host_name":"dev.to"},{"title":"Function calling with the Gemini API | Google AI for Developers","url":"https://ai.google.dev/gemini-api/docs/function-calling","text":"You can enable multiple tools combining native tools with function calling at the same time. Here's an example that enables two tools, Grounding with Google ...","index":47,"favicon":"","host_name":"ai.google.dev"},{"title":"How to Chain Tools in OpenAI Function Calls? - API","url":"https://community.openai.com/t/how-to-chain-tools-in-openai-function-calls/1088098","text":"Currently, this often requires two separate calls to the LLM: 1. GPT asks to invoke create_data() and receives the result. 2. GPT processes the ...","index":48,"favicon":"","host_name":"community.openai.com"},{"title":"How to use Anthropic MCP Server with open LLMs, OpenAI","url":"https://www.philschmid.de/mcp-example-llama","text":"In this blog post, you will learn how to use MCP Servers with any open LLM, OpenAI, or Google Gemini. You will learn how to build a simple CLI Agent that can ...","index":49,"favicon":"","host_name":"www.philschmid.de"},{"title":"The guide to structured outputs and function calling with","url":"https://agenta.ai/blog/the-guide-to-structured-outputs-and-function-calling-with-llms","text":"Complete production guide with OpenAI, Claude, and Gemini code examples for consistent data extraction. Sep 10, 2025. -. 10 minutes.","index":50,"favicon":"","host_name":"agenta.ai"}]},"metadata":{"arguments":"{\"query\":\"call multiple LLM APIs in one app openai anthropic gemini example 2025\"}","display_result":"","duration":"6s","id":"call_5F8xUuoxpo","is_error":false,"mcp_server":{"name":"mcp-server"},"name":"search","result":"【40†How LLMs Invoke Real-World APIs (OpenAI & Gemini†https://medium.com/@akankshasinha247/function-calling-how-llms-invoke-real-world-apis-openai-gemini-examples-266bdd802c03】\nFunction Calling allows an LLM to dynamically decide whether to invoke an external function / tool / API, automatically generate structured arguments for that ...\n\n【41†How to Orchestrate Multiple Language Models: A Step-by†https://www.revanthquicklearn.com/post/how-to-orchestrate-multiple-language-models-a-step-by-step-guide-for-beginners】\nWorking with multiple Large Language Models (LLMs) opens up incredible possibilities for building intelligent applications.\n\n【42†Research issue: gather examples of multi-modal API calls†https://github.com/simonw/llm/issues/557】\nTo aid in the design for both of these: #331 #556 I'm going to gather a bunch of examples of how different LLMs accept multi-modal inputs.\n\n【43†10 Python One-Liners for Calling LLMs from Your Code†https://machinelearningmastery.com/10-python-one-liners-for-calling-llms-from-your-code】\nIn this article, you'll see ten Python one-liners that call and interact with LLMs. We will cover: Hosted APIs: examples using OpenAI, Anthropic ...\n\n【44†How do I send 1 prompt to multiple LLM APIs (ChatGPT†https://www.reddit.com/r/LocalLLaMA/comments/1pch3lt/how_do_i_send_1_prompt_to_multiple_llm_apis】\nHow do I send 1 prompt to multiple LLM APIs (ChatGPT, Gemini, Perplexity) and auto-merge their answers into a unified output? Question | Help.\n\n【45†Parallel LLM Calls from Scratch — Tutorial For Dummies†https://dev.to/zachary62/parallel-llm-calls-from-scratch-tutorial-for-dummies-using-pocketflow-1972】\nThis guide breaks down how to drastically speed things up using parallel processing with the PocketFlow Parallel Batch Example!\n\n【46†Function calling with the Gemini API | Google AI for Developers†https://ai.google.dev/gemini-api/docs/function-calling】\nYou can enable multiple tools combining native tools with function calling at the same time. Here's an example that enables two tools, Grounding with Google ...\n\n【47†How to Chain Tools in OpenAI Function Calls? - API†https://community.openai.com/t/how-to-chain-tools-in-openai-function-calls/1088098】\nCurrently, this often requires two separate calls to the LLM: 1. GPT asks to invoke create_data() and receives the result. 2. GPT processes the ...\n\n【48†How to use Anthropic MCP Server with open LLMs, OpenAI†https://www.philschmid.de/mcp-example-llama】\nIn this blog post, you will learn how to use MCP Servers with any open LLM, OpenAI, or Google Gemini. You will learn how to build a simple CLI Agent that can ...\n\n【49†The guide to structured outputs and function calling with†https://agenta.ai/blog/the-guide-to-structured-outputs-and-function-calling-with-llms】\nComplete production guide with OpenAI, Claude, and Gemini code examples for consistent data extraction. Sep 10, 2025. -. 10 minutes.\n","status":"completed"}},"type":"mcp"}</glm_block>
<glm_block view="" tool_call_name="search">{"data":{"browser":{"search_result":[{"title":"20 Best AI Coding Assistant Tools [Updated Aug 2025]","url":"https://www.qodo.ai/blog/best-ai-coding-assistant-tools","text":"Explore our list of the 20 best AI coding assistant tools in 2025, boosting productivity and code quality for developers.","index":1,"favicon":"","host_name":"www.qodo.ai"},{"title":"Best AI Tools for Coding in 2025: 6 Tools Worth Your Time","url":"https://www.pragmaticcoders.com/resources/ai-developer-tools","text":"We tested 20+ AI coding tools – agents, reviewers, and assistants. See what stood out and what features the best tools offer.","index":2,"favicon":"","host_name":"www.pragmaticcoders.com"},{"title":"The 8 Best AI Coding Tools in 2025","url":"https://medium.com/@datajournal/the-8-best-ai-coding-tools-in-2025-cb323c5b9780","text":"Find the best AI coding tools of 2025 for faster, smarter code. Compare features and pricing for Cursor, Copilot, Tabnine, and more.","index":3,"favicon":"","host_name":"medium.com"},{"title":"What is the best AI engine for programming in September","url":"https://www.reddit.com/r/GithubCopilot/comments/1nhju40/what_is_the_best_ai_engine_for_programming_in","text":"#1 codex since, chatgpt 5 is best model. #2 Claude code, best CLI tool, good model. #3 Copilot, good auto complete, github integration.","index":4,"favicon":"","host_name":"www.reddit.com"},{"title":"13 Best AI Coding Tools for Complex Enterprise","url":"https://www.augmentcode.com/guides/13-best-ai-coding-tools-for-complex-codebases","text":"Find the best best coding ai with detailed comparisons, pros/cons, and implementation guides for dev teams.","index":5,"favicon":"","host_name":"www.augmentcode.com"},{"title":"AI Coding Tools in 2025: Welcome to the Agentic CLI Era","url":"https://thenewstack.io/ai-coding-tools-in-2025-welcome-to-the-agentic-cli-era","text":"2025 was the start of the Agentic Era, a new wave of software development. We moved beyond simple IDE chatbots and onto agentic CLIs.","index":6,"favicon":"","host_name":"thenewstack.io"},{"title":"Best AI Coding Assistants 2025: Tools for Developers","url":"https://replit.com/discover/best-ai-coding-assistant","text":"Best AI coding assistants of 2025 compared. See how tools like Replit, Cursor, and Copilot automate coding, testing, and deployment.","index":7,"favicon":"","host_name":"replit.com"},{"title":"AI Code Tools: Complete Guide for Developers in 2025","url":"https://codesubmit.io/blog/ai-code-tools","text":"Discover the best AI coding tools in 2025. Personal curation of 40+ tools including ChatGPT, GitHub Copilot, Cursor, and more.","index":8,"favicon":"","host_name":"codesubmit.io"},{"title":"Top 9 Cursor Alternatives in 2025: Best Open Source AI","url":"https://cline.bot/blog/top-9-cursor-alternatives-in-2025-best-open-source-ai-dev-tools-for-developers","text":"Open-source AI coding assistant designed for self-hosting; Supports multiple programming languages and major IDEs; Real-time, context-aware code completion and ...","index":9,"favicon":"","host_name":"cline.bot"},{"title":"14 Best AI Developer Productivity Tools in 2025","url":"https://www.greptile.com/content-library/14-best-developer-productivity-tools-2025","text":"Discover the top 14 AI developer productivity tools in 2025, including Greptile, GitHub Copilot, and Tabnine. Compare features, pricing ...","index":10,"favicon":"","host_name":"www.greptile.com"},{"title":"Specification","url":"https://modelcontextprotocol.io/specification/2025-11-25","text":"Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and ...","index":11,"favicon":"","host_name":"modelcontextprotocol.io"},{"title":"7 Things to Know About MCP in 2025","url":"https://www.adskate.com/blogs/mcp-model-context-protocol-2025-guide","text":"Why It Matters in 2025. Cross-model support: MCP is model-agnostic and now supported by OpenAI, Anthropic, and Hugging Face. Scalability ...","index":12,"favicon":"","host_name":"www.adskate.com"},{"title":"What Is the Model Context Protocol (MCP) and How It Works","url":"https://www.descope.com/learn/post/mcp","text":"Visual Studio Code: Microsoft's IDE, which added MCP support in June 2025. JetBrains IDEs: Full coding suite that added AI Assistant MCP ...","index":13,"favicon":"","host_name":"www.descope.com"},{"title":"The Model Context Protocol's impact on 2025","url":"https://www.thoughtworks.com/en-us/insights/blog/generative-ai/model-context-protocol-mcp-impact-2025","text":"The Model Context Protocol (MCP) has been one of the key stories of 2025. We unpack its impact through the lens of the Thoughtworks ...","index":14,"favicon":"","host_name":"www.thoughtworks.com"},{"title":"IDE integration using Model Context Protocol (MCP)","url":"https://techdocs.broadcom.com/us/en/vmware-tanzu/spring/application-advisor/1-5/app-advisor/model-context-protocol-server.html","text":"To integrate the Application Advisor into your preferred IDE or MCP client, you can use the Application Advisor MCP server.","index":15,"favicon":"","host_name":"techdocs.broadcom.com"},{"title":"Model Context Protocol (MCP) | AI Assistant Documentation","url":"https://www.jetbrains.com/help/ai-assistant/mcp.html","text":"Starting with version 2025.2, JetBrains IDEs come with an integrated MCP server, allowing external clients such as Claude Desktop, Cursor, Codex ...","index":16,"favicon":"","host_name":"www.jetbrains.com"},{"title":"Model Context Protocol","url":"https://developers.openai.com/codex/mcp","text":"The CLI and the IDE extension share this configuration. Once you configure your MCP servers, you can switch between the two Codex clients without redoing setup.","index":17,"favicon":"","host_name":"developers.openai.com"},{"title":"Model Context Protocol (MCP): AI Integration Guide | Medium","url":"https://michielh.medium.com/the-model-context-protocol-mcp-step-by-step-connecting-ai-agents-to-everything-6b25a052b87c","text":"Evolving spec: OAuth integration, server remote hosting, and discovery registries are still under active development (early 2025); Potential ...","index":18,"favicon":"","host_name":"michielh.medium.com"},{"title":"What is Model Context Protocol (MCP)? The Complete Guide","url":"https://www.figma.com/resource-library/what-is-mcp","text":"In January and February 2025, many AI IDEs, like Cursor and Windsurf, started supporting MCP. In March, OpenAI added MCP support, and GitHub released its MCP ...","index":19,"favicon":"","host_name":"www.figma.com"},{"title":"Revolutionize AI Integration with MCP: The Future of Open","url":"https://www.baytechconsulting.com/blog/revolutionize-ai-integration-mcp-2025","text":"Explore the strategic architecture and implementation of the Model Context Protocol (MCP) in .NET Core, detailing roles, technical mechanics ...","index":20,"favicon":"","host_name":"www.baytechconsulting.com"},{"title":"Vibe Coding Explained: Tools and Guides","url":"https://cloud.google.com/discover/what-is-vibe-coding","text":"Vibe coding is a style of programming that uses natural language prompts to assist with the coding process. Learn how it works with Google ...","index":21,"favicon":"","host_name":"cloud.google.com"},{"title":"Vibe coding","url":"https://en.wikipedia.org/wiki/Vibe_coding","text":"Vibe coding is an AI-assisted software development technique. The term was introduced by Andrej Karpathy in February 2025. The term was listed on the Merriam- ...","index":22,"favicon":"","host_name":"en.wikipedia.org"},{"title":"What is Vibe Coding?","url":"https://www.ibm.com/think/topics/vibe-coding","text":"Vibe coding is a fresh take in coding where users express their intention using plain speech and the AI transforms that thinking into executable code.","index":23,"favicon":"","host_name":"www.ibm.com"},{"title":"Vibe coding is not the same as AI-Assisted engineering.","url":"https://medium.com/@addyosmani/vibe-coding-is-not-the-same-as-ai-assisted-engineering-3f81088d5b98","text":"In an August 2025 survey by Final Round AI, 18 CTOs were asked about vibe coding and 16 reported experiencing production disasters directly ...","index":24,"favicon":"","host_name":"medium.com"},{"title":"What is the exact definition of \"vibe coding\"? : r/ClaudeAI","url":"https://www.reddit.com/r/ClaudeAI/comments/1j6z4ft/what_is_the_exact_definition_of_vibe_coding","text":"you tell the AI what you want, and it writes the code. Coined by Karpathy, it's fast, intuitive, and great for prototypes.","index":25,"favicon":"","host_name":"www.reddit.com"},{"title":"How to Vibe Code: 5 Simple Tips for Using AI to Become a","url":"https://www.pcmag.com/explainers/how-to-vibe-code-5-simple-tips-for-using-ai-to-become-a-programmer","text":"In short, vibe coding means giving an AI a programming task and letting it complete it for you. It generates the code, makes architecture ...","index":26,"favicon":"","host_name":"www.pcmag.com"},{"title":"What is Vibe Coding? Prompting AI Software Development","url":"https://www.sonarsource.com/resources/library/vibe-coding","text":"Vibe coding basically means using AI, like those big language models, to write code for you. You just tell it what you want in regular words, and boom, it tries ...","index":27,"favicon":"","host_name":"www.sonarsource.com"},{"title":"What is Vibe Coding? Your 2025 Vibe Coding Guide","url":"https://www.synergylabs.co/blog/what-is-vibe-coding-your-2025-vibe-coding-guide","text":"Vibe Coding is a development paradigm where programmers essentially \"surrender to the vibes\" of AI-powered coding assistants.","index":28,"favicon":"","host_name":"www.synergylabs.co"},{"title":"Understanding Vibe Coding and Its Implications","url":"https://www.blackduck.com/blog/vibe-coding-and-its-implications.html","text":"“Vibe coding” means using natural language and AI tools to generate and refine code. This approach lowers barriers to entry for coding, ...","index":29,"favicon":"","host_name":"www.blackduck.com"},{"title":"Vibe Coding: The Future of Building Software (Without","url":"https://www.siddharthbharath.com/vibe-coding","text":"Vibe coding is the new frontier where you build software by telling an AI what you want and letting it spit out the code.","index":30,"favicon":"","host_name":"www.siddharthbharath.com"},{"title":"Top 10 AI Orchestration Tools in 2025","url":"https://www.kubiya.ai/blog/ai-orchestration-tools","text":"Discover the top 10 AI orchestration tools in 2025. Explore features, benefits, and innovations driving seamless AI workflow automation and ...","index":31,"favicon":"","host_name":"www.kubiya.ai"},{"title":"What is AI Orchestration? 21+ Tools to Consider in 2025","url":"https://akka.io/blog/ai-orchestration-tools","text":"Discover top AI orchestration tools that streamline workflows and optimize performance across your tech stack in this comprehensive guide.","index":32,"favicon":"","host_name":"akka.io"},{"title":"10 Best AI Orchestration Platforms in 2025","url":"https://www.domo.com/learn/article/best-ai-orchestration-platforms","text":"Microsoft AutoGen is an orchestration framework for building multi-agent AI systems. It allows developers to coordinate multiple LLMs, APIs, and ...","index":33,"favicon":"","host_name":"www.domo.com"},{"title":"Best AI Agent Orchestration Tools in 2025: Detailed List","url":"https://www.reddit.com/r/AI_Agents/comments/1nk12yi/best_ai_agent_orchestration_tools_in_2025","text":"How about Google ADK? It's easy to start and includes many built-in features like UI, deployment, tools, etc.","index":34,"favicon":"","host_name":"www.reddit.com"},{"title":"Top 10 AI Agent Frameworks for 2025: LangChain Leads","url":"https://medium.com/@julio.pessan.pessan/top-10-ai-agent-frameworks-for-2025-langchain-leads-the-pack-in-multi-agent-orchestration-c115044bcb60","text":"Top 10 AI Agent Frameworks for 2025: LangChain Leads the Pack in Multi-Agent Orchestration · 1. LangChain — The Orchestra Conductor · 2. CrewAI — ...","index":35,"favicon":"","host_name":"medium.com"},{"title":"Best AI Workflow Automation Tools (2025)","url":"https://www.whalesync.com/blog/best-ai-workflow-automation-tools-2025","text":"These new AI-first platforms unlock mind-blowing functionality from PDF analysis to AI note-taking, sales agents and more. To help keep pace ...","index":36,"favicon":"","host_name":"www.whalesync.com"},{"title":"Top 13 Agentic AI Tools in 2025 and Their Key Features","url":"https://www.lasso.security/blog/agentic-ai-tools","text":"Explore the leading agentic AI tools transforming enterprise automation in 2025. Learn how these autonomous applications work, and how to ...","index":37,"favicon":"","host_name":"www.lasso.security"},{"title":"10 Best AI Automation Platforms: Tested & Reviewed","url":"https://www.lindy.ai/blog/ai-automation-platform","text":"Lindy is an AI automation platform that helps teams delegate repetitive tasks to AI agents. These agents can send follow-ups, qualify leads, update CRMs, ...","index":38,"favicon":"","host_name":"www.lindy.ai"},{"title":"AI Agent Orchestration Frameworks: Which One Works","url":"https://blog.n8n.io/ai-agent-orchestration-frameworks","text":"Discover AI agent orchestration frameworks like n8n, LangGraph, CrewAI that power scalable, multi-agent AI orchestration in 2025.","index":39,"favicon":"","host_name":"blog.n8n.io"},{"title":"Top 7 Free AI Agent Frameworks [2025]","url":"https://botpress.com/blog/ai-agent-frameworks","text":"The key benefit to using an AI agent framework is that it abstracts complexity, breaking tasks into manageable steps, and ensures scalability.","index":40,"favicon":"","host_name":"botpress.com"}]},"metadata":{"arguments":"{\"query\":\"AI agent workflow orchestration tools 2025\"}","display_result":"","duration":"6s","id":"call_QnUWOdp4GW","is_error":false,"mcp_server":{"name":"mcp-server"},"name":"search","result":"【30†Top 10 AI Orchestration Tools in 2025†https://www.kubiya.ai/blog/ai-orchestration-tools】\nDiscover the top 10 AI orchestration tools in 2025. Explore features, benefits, and innovations driving seamless AI workflow automation and ...\n\n【31†What is AI Orchestration? 21+ Tools to Consider in 2025†https://akka.io/blog/ai-orchestration-tools】\nDiscover top AI orchestration tools that streamline workflows and optimize performance across your tech stack in this comprehensive guide.\n\n【32†10 Best AI Orchestration Platforms in 2025†https://www.domo.com/learn/article/best-ai-orchestration-platforms】\nMicrosoft AutoGen is an orchestration framework for building multi-agent AI systems. It allows developers to coordinate multiple LLMs, APIs, and ...\n\n【33†Best AI Agent Orchestration Tools in 2025: Detailed List†https://www.reddit.com/r/AI_Agents/comments/1nk12yi/best_ai_agent_orchestration_tools_in_2025】\nHow about Google ADK? It's easy to start and includes many built-in features like UI, deployment, tools, etc.\n\n【34†Top 10 AI Agent Frameworks for 2025: LangChain Leads†https://medium.com/@julio.pessan.pessan/top-10-ai-agent-frameworks-for-2025-langchain-leads-the-pack-in-multi-agent-orchestration-c115044bcb60】\nTop 10 AI Agent Frameworks for 2025: LangChain Leads the Pack in Multi-Agent Orchestration · 1. LangChain — The Orchestra Conductor · 2. CrewAI — ...\n\n【35†Best AI Workflow Automation Tools (2025)†https://www.whalesync.com/blog/best-ai-workflow-automation-tools-2025】\nThese new AI-first platforms unlock mind-blowing functionality from PDF analysis to AI note-taking, sales agents and more. To help keep pace ...\n\n【36†Top 13 Agentic AI Tools in 2025 and Their Key Features†https://www.lasso.security/blog/agentic-ai-tools】\nExplore the leading agentic AI tools transforming enterprise automation in 2025. Learn how these autonomous applications work, and how to ...\n\n【37†10 Best AI Automation Platforms: Tested & Reviewed†https://www.lindy.ai/blog/ai-automation-platform】\nLindy is an AI automation platform that helps teams delegate repetitive tasks to AI agents. These agents can send follow-ups, qualify leads, update CRMs, ...\n\n【38†AI Agent Orchestration Frameworks: Which One Works†https://blog.n8n.io/ai-agent-orchestration-frameworks】\nDiscover AI agent orchestration frameworks like n8n, LangGraph, CrewAI that power scalable, multi-agent AI orchestration in 2025.\n\n【39†Top 7 Free AI Agent Frameworks [2025]†https://botpress.com/blog/ai-agent-frameworks】\nThe key benefit to using an AI agent framework is that it abstracts complexity, breaking tasks into manageable steps, and ensures scalability.\n","status":"completed"}},"type":"mcp"}</glm_block>


<details type="reasoning" done="true" duration="5" view="" last_tool_call_name="search">
> ユーザーは、AI IDE、CLI、LLM API、MCPなどを活用して「vibe coding」を最大化するアプリを作成しようとしています。そのために、MCPサーバーの構築、複数のLLM APIの統合、ワークフローのオーケストレーションが不可欠です。LangChainやCrewAIのようなフレームワークを使ってAIエージェントを管理し、MCPでツールを接続します。また、CursorやVS CodeのようなAI IDEを統合し、アプリの構築にはReactやElectronを使用することを提案します。次のステップとして、MCPサーバーの作成から始めることをお勧めします。
</details>

<glm_block view="" tool_call_name="click">{"data":{"browser":{"url":"https://modelcontextprotocol.io/docs/develop/build-server","title":"Build an MCP server","content":"【50†Build an MCP server†https://modelcontextprotocol.io/docs/develop/build-server】\nTitle: Build an MCP server - Model Context Protocol\n\nURL Source: https://modelcontextprotocol.io/docs/develop/build-server\n\nMarkdown Content:\nLet’s get started with building our weather server! You can find the complete code for what we’ll be building here.\n\n### Prerequisite knowledge\n\nThis quickstart assumes you have familiarity with:\n\n*   Python\n*   LLMs like Claude\n\n### Logging in MCP Servers\n\nWhen implementing MCP servers, be careful about how you handle logging:**For STDIO-based servers:** Never write to standard output (stdout). This includes:\n\n*   `print()` statements in Python\n*   `console.log()` in JavaScript\n*   `fmt.Println()` in Go\n*   Similar stdout functions in other languages\n\nWriting to stdout will corrupt the JSON-RPC messages and break your server.**For HTTP-based servers:** Standard output logging is fine since it doesn’t interfere with HTTP responses.\n\n### Best Practices\n\n1.   Use a logging library that writes to stderr or files.\n2.   For Python, be especially careful - `print()` writes to stdout by default.\n\n### Quick Examples\n\n```\n# ❌ Bad (STDIO)\nprint(\"Processing request\")\n\n# ✅ Good (STDIO)\nimport logging\nlogging.info(\"Processing request\")\n```\n\n### System requirements\n\n*   Python 3.10 or higher installed.\n*   You must use the Python MCP SDK 1.2.0 or higher.\n\n### Set up your environment\n\nFirst, let’s install `uv` and set up our Python project and environment:\n\nMake sure to restart your terminal afterwards to ensure that the `uv` command gets picked up.Now, let’s create and set up our project:\n\nNow let’s dive into building your server.\n\nBuilding your server\n--------------------\n\n### Importing packages and setting up the instance\n\nAdd these to the top of your `weather.py`:\n\n```\nfrom typing import Any\n\nimport httpx\nfrom mcp.server.fastmcp import FastMCP\n\n# Initialize FastMCP server\nmcp = FastMCP(\"weather\")\n\n# Constants\nNWS_API_BASE = \"https://api.weather.gov\"\nUSER_AGENT = \"weather-app/1.0\"\n```\n\nThe FastMCP class uses Python type hints and docstrings to automatically generate tool definitions, making it easy to create and maintain MCP tools.\n\n### Helper functions\n\nNext, let’s add our helper functions for querying and formatting the data from the National Weather Service API:\n\n```\nasync def make_nws_request(url: str) -> dict[str, Any] | None:\n    \"\"\"Make a request to the NWS API with proper error handling.\"\"\"\n    headers = {\"User-Agent\": USER_AGENT, \"Accept\": \"application/geo+json\"}\n    async with httpx.AsyncClient() as client:\n        try:\n            response = await client.get(url, headers=headers, timeout=30.0)\n            response.raise_for_status()\n            return response.json()\n        except Exception:\n            return None\n\ndef format_alert(feature: dict) -> str:\n    \"\"\"Format an alert feature into a readable string.\"\"\"\n    props = feature[\"properties\"]\n    return f\"\"\"\nEvent: {props.get(\"event\", \"Unknown\")}\nArea: {props.get(\"areaDesc\", \"Unknown\")}\nSeverity: {props.get(\"severity\", \"Unknown\")}\nDescription: {props.get(\"description\", \"No description available\")}\nInstructions: {props.get(\"instruction\", \"No specific instructions provided\")}\n\"\"\"\n```\n\n### Implementing tool execution\n\nThe tool execution handler is responsible for actually executing the logic of each tool. Let’s add it:\n\n```\n@mcp.tool()\nasync def get_alerts(state: str) -> str:\n    \"\"\"Get weather alerts for a US state.\n\n    Args:\n        state: Two-letter US state code (e.g. CA, NY)\n    \"\"\"\n    url = f\"{NWS_API_BASE}/alerts/active/area/{state}\"\n    data = await make_nws_request(url)\n\n    if not data or \"features\" not in data:\n        return \"Unable to fetch alerts or no alerts found.\"\n\n    if not data[\"features\"]:\n        return \"No active alerts for this state.\"\n\n    alerts = [format_alert(feature) for feature in data[\"features\"]]\n    return \"\\n---\\n\".join(alerts)\n\n@mcp.tool()\nasync def get_forecast(latitude: float, longitude: float) -> str:\n    \"\"\"Get weather forecast for a location.\n\n    Args:\n        latitude: Latitude of the location\n        longitude: Longitude of the location\n    \"\"\"\n    # First get the forecast grid endpoint\n    points_url = f\"{NWS_API_BASE}/points/{latitude},{longitude}\"\n    points_data = await make_nws_request(points_url)\n\n    if not points_data:\n        return \"Unable to fetch forecast data for this location.\"\n\n    # Get the forecast URL from the points response\n    forecast_url = points_data[\"properties\"][\"forecast\"]\n    forecast_data = await make_nws_request(forecast_url)\n\n    if not forecast_data:\n        return \"Unable to fetch detailed forecast.\"\n\n    # Format the periods into a readable forecast\n    periods = forecast_data[\"properties\"][\"periods\"]\n    forecasts = []\n    for period in periods[:5]:  # Only show next 5 periods\n        forecast = f\"\"\"\n{period[\"name\"]}:\nTemperature: {period[\"temperature\"]}°{period[\"temperatureUnit\"]}\nWind: {period[\"windSpeed\"]} {period[\"windDirection\"]}\nForecast: {period[\"detailedForecast\"]}\n\"\"\"\n        forecasts.append(forecast)\n\n    return \"\\n---\\n\".join(forecasts)\n```\n\n### Running the server\n\nFinally, let’s initialize and run the server:\n\n```\ndef main():\n    # Initialize and run the server\n    mcp.run(transport=\"stdio\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nYour server is complete! Run `uv run weather.py` to start the MCP server, which will listen for messages from MCP hosts.Let’s now test your server from an existing MCP host, Claude for Desktop.\n\nTesting your server with Claude for Desktop\n-------------------------------------------\n\nFirst, make sure you have Claude for Desktop installed. You can install the latest version here. If you already have Claude for Desktop, **make sure it’s updated to the latest version.**We’ll need to configure Claude for Desktop for whichever MCP servers you want to use. To do this, open your Claude for Desktop App configuration at `~/Library/Application Support/Claude/claude_desktop_config.json` in a text editor. Make sure to create the file if it doesn’t exist.For example, if you have VS Code installed:\n\nYou’ll then add your servers in the `mcpServers` key. The MCP UI elements will only show up in Claude for Desktop if at least one server is properly configured.In this case, we’ll add our single weather server like so:\n\nThis tells Claude for Desktop:\n\n1.   There’s an MCP server named “weather”\n2.   To launch it by running `uv --directory /ABSOLUTE/PATH/TO/PARENT/FOLDER/weather run weather.py`\n\nSave the file, and restart **Claude for Desktop**.\n\nLet’s get started with building our weather server! You can find the complete code for what we’ll be building here.\n\n### Prerequisite knowledge\n\nThis quickstart assumes you have familiarity with:\n\n*   TypeScript\n*   LLMs like Claude\n\n### Logging in MCP Servers\n\nWhen implementing MCP servers, be careful about how you handle logging:**For STDIO-based servers:** Never write to standard output (stdout). This includes:\n\n*   `print()` statements in Python\n*   `console.log()` in JavaScript\n*   `fmt.Println()` in Go\n*   Similar stdout functions in other languages\n\nWriting to stdout will corrupt the JSON-RPC messages and break your server.**For HTTP-based servers:** Standard output logging is fine since it doesn’t interfere with HTTP responses.\n\n### Best Practices\n\n1.   Use a logging library that writes to stderr or files, such as `logging` in Python.\n2.   For JavaScript, be especially careful - `console.log()` writes to stdout by default\n\n### Quick Examples\n\n```\n// ❌ Bad (STDIO)\nconsole.log(\"Server started\");\n\n// ✅ Good (STDIO)\nconsole.error(\"Server started\"); // stderr is safe\n```\n\n### System requirements\n\nFor TypeScript, make sure you have the latest version of Node installed.\n\n### Set up your environment\n\nFirst, let’s install Node.js and npm if you haven’t already. You can download them from nodejs.org. Verify your Node.js installation:\n\n```\nnode --version\nnpm --version\n```\n\nFor this tutorial, you’ll need Node.js version 16 or higher.Now, let’s create and set up our project:\n\nUpdate your package.json to add type: “module” and a build script:\n\npackage.json\n\n```\n{\n  \"type\": \"module\",\n  \"bin\": {\n    \"weather\": \"./build/index.js\"\n  },\n  \"scripts\": {\n    \"build\": \"tsc && chmod 755 build/index.js\"\n  },\n  \"files\": [\"build\"]\n}\n```\n\nCreate a `tsconfig.json` in the root of your project:\n\ntsconfig.json\n\n```\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"module\": \"Node16\",\n    \"moduleResolution\": \"Node16\",\n    \"outDir\": \"./build\",\n    \"rootDir\": \"./src\",\n    \"strict\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true\n  },\n  \"include\": [\"src/**/*\"],\n  \"exclude\": [\"node_modules\"]\n}\n```\n\nNow let’s dive into building your server.\n\nBuilding your server\n--------------------\n\n### Importing packages and setting up the instance\n\nAdd these to the top of your `src/index.ts`:\n\n```\nimport { McpServer } from \"@modelcontextprotocol/sdk/server/mcp.js\";\nimport { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio.js\";\nimport { z } from \"zod\";\n\nconst NWS_API_BASE = \"https://api.weather.gov\";\nconst USER_AGENT = \"weather-app/1.0\";\n\n// Create server instance\nconst server = new McpServer({\n  name: \"weather\",\n  version: \"1.0.0\",\n});\n```\n\n### Helper functions\n\nNext, let’s add our helper functions for querying and formatting the data from the National Weather Service API:\n\n```\n// Helper function for making NWS API requests\nasync function makeNWSRequest<T>(url: string): Promise<T | null> {\n  const headers = {\n    \"User-Agent\": USER_AGENT,\n    Accept: \"application/geo+json\",\n  };\n\n  try {\n    const response = await fetch(url, { headers });\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}`);\n    }\n    return (await response.json()) as T;\n  } catch (error) {\n    console.error(\"Error making NWS request:\", error);\n    return null;\n  }\n}\n\ninterface AlertFeature {\n  properties: {\n    event?: string;\n    areaDesc?: string;\n    severity?: string;\n    status?: string;\n    headline?: string;\n  };\n}\n\n// Format alert data\nfunction formatAlert(feature: AlertFeature): string {\n  const props = feature.properties;\n  return [\n    `Event: ${props.event || \"Unknown\"}`,\n    `Area: ${props.areaDesc || \"Unknown\"}`,\n    `Severity: ${props.severity || \"Unknown\"}`,\n    `Status: ${props.status || \"Unknown\"}`,\n    `Headline: ${props.headline || \"No headline\"}`,\n    \"---\",\n  ].join(\"\\n\");\n}\n\ninterface ForecastPeriod {\n  name?: string;\n  temperature?: number;\n  temperatureUnit?: string;\n  windSpeed?: string;\n  windDirection?: string;\n  shortForecast?: string;\n}\n\ninterface AlertsResponse {\n  features: AlertFeature[];\n}\n\ninterface PointsResponse {\n  properties: {\n    forecast?: string;\n  };\n}\n\ninterface ForecastResponse {\n  properties: {\n    periods: ForecastPeriod[];\n  };\n}\n```\n\n### Implementing tool execution\n\nThe tool execution handler is responsible for actually executing the logic of each tool. Let’s add it:\n\n```\n// Register weather tools\n\nserver.registerTool(\n  \"get_alerts\",\n  {\n    description: \"Get weather alerts for a state\",\n    inputSchema: {\n      state: z\n        .string()\n        .length(2)\n        .describe(\"Two-letter state code (e.g. CA, NY)\"),\n    },\n  },\n  async ({ state }) => {\n    const stateCode = state.toUpperCase();\n    const alertsUrl = `${NWS_API_BASE}/alerts?area=${stateCode}`;\n    const alertsData = await makeNWSRequest<AlertsResponse>(alertsUrl);\n\n    if (!alertsData) {\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: \"Failed to retrieve alerts data\",\n          },\n        ],\n      };\n    }\n\n    const features = alertsData.features || [];\n    if (features.length === 0) {\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: `No active alerts for ${stateCode}`,\n          },\n        ],\n      };\n    }\n\n    const formattedAlerts = features.map(formatAlert);\n    const alertsText = `Active alerts for ${stateCode}:\\n\\n${formattedAlerts.join(\"\\n\")}`;\n\n    return {\n      content: [\n        {\n          type: \"text\",\n          text: alertsText,\n        },\n      ],\n    };\n  },\n);\n\nserver.registerTool(\n  \"get_forecast\",\n  {\n    description: \"Get weather forecast for a location\",\n    inputSchema: {\n      latitude: z\n        .number()\n        .min(-90)\n        .max(90)\n        .describe(\"Latitude of the location\"),\n      longitude: z\n        .number()\n        .min(-180)\n        .max(180)\n        .describe(\"Longitude of the location\"),\n    },\n  },\n  async ({ latitude, longitude }) => {\n    // Get grid point data\n    const pointsUrl = `${NWS_API_BASE}/points/${latitude.toFixed(4)},${longitude.toFixed(4)}`;\n    const pointsData = await makeNWSRequest<PointsResponse>(pointsUrl);\n\n    if (!pointsData) {\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: `Failed to retrieve grid point data for coordinates: ${latitude}, ${longitude}. This location may not be supported by the NWS API (only US locations are supported).`,\n          },\n        ],\n      };\n    }\n\n    const forecastUrl = pointsData.properties?.forecast;\n    if (!forecastUrl) {\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: \"Failed to get forecast URL from grid point data\",\n          },\n        ],\n      };\n    }\n\n    // Get forecast data\n    const forecastData = await makeNWSRequest<ForecastResponse>(forecastUrl);\n    if (!forecastData) {\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: \"Failed to retrieve forecast data\",\n          },\n        ],\n      };\n    }\n\n    const periods = forecastData.properties?.periods || [];\n    if (periods.length === 0) {\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: \"No forecast periods available\",\n          },\n        ],\n      };\n    }\n\n    // Format forecast periods\n    const formattedForecast = periods.map((period: ForecastPeriod) =>\n      [\n        `${period.name || \"Unknown\"}:`,\n        `Temperature: ${period.temperature || \"Unknown\"}°${period.temperatureUnit || \"F\"}`,\n        `Wind: ${period.windSpeed || \"Unknown\"} ${period.windDirection || \"\"}`,\n        `${period.shortForecast || \"No forecast available\"}`,\n        \"---\",\n      ].join(\"\\n\"),\n    );\n\n    const forecastText = `Forecast for ${latitude}, ${longitude}:\\n\\n${formattedForecast.join(\"\\n\")}`;\n\n    return {\n      content: [\n        {\n          type: \"text\",\n          text: forecastText,\n        },\n      ],\n    };\n  },\n);\n```\n\n### Running the server\n\nFinally, implement the main function to run the server:\n\n```\nasync function main() {\n  const transport = new StdioServerTransport();\n  await server.connect(transport);\n  console.error(\"Weather MCP Server running on stdio\");\n}\n\nmain().catch((error) => {\n  console.error(\"Fatal error in main():\", error);\n  process.exit(1);\n});\n```\n\nMake sure to run `npm run build` to build your server! This is a very important step in getting your server to connect.Let’s now test your server from an existing MCP host, Claude for Desktop.\n\nTesting your server with Claude for Desktop\n-------------------------------------------\n\nFirst, make sure you have Claude for Desktop installed. You can install the latest version here. If you already have Claude for Desktop, **make sure it’s updated to the latest version.**We’ll need to configure Claude for Desktop for whichever MCP servers you want to use. To do this, open your Claude for Desktop App configuration at `~/Library/Application Support/Claude/claude_desktop_config.json` in a text editor. Make sure to create the file if it doesn’t exist.For example, if you have VS Code installed:\n\nYou’ll then add your servers in the `mcpServers` key. The MCP UI elements will only show up in Claude for Desktop if at least one server is properly configured.In this case, we’ll add our single weather server like so:\n\nThis tells Claude for Desktop:\n\n1.   There’s an MCP server named “weather”\n2.   Launch it by running `node /ABSOLUTE/PATH/TO/PARENT/FOLDER/weather/build/index.js`\n\nSave the file, and restart **Claude for Desktop**.\n\n\n"},"metadata":{"arguments":"{\"link_id\":50}","display_result":"","duration":"13s","id":"call_jMwWZRAKmA","is_error":false,"mcp_server":{"name":"mcp-server"},"name":"click","result":"【50†Build an MCP server†https://modelcontextprotocol.io/docs/develop/build-server】\nTitle: Build an MCP server - Model Context Protocol\n\nURL Source: https://modelcontextprotocol.io/docs/develop/build-server\n\nMarkdown Content:\nLet’s get started with building our weather server! You can find the complete code for what we’ll be building here.\n\n### Prerequisite knowledge\n\nThis quickstart assumes you have familiarity with:\n\n*   Python\n*   LLMs like Claude\n\n### Logging in MCP Servers\n\nWhen implementing MCP servers, be careful about how you handle logging:**For STDIO-based servers:** Never write to standard output (stdout). This includes:\n\n*   `print()` statements in Python\n*   `console.log()` in JavaScript\n*   `fmt.Println()` in Go\n*   Similar stdout functions in other languages\n\nWriting to stdout will corrupt the JSON-RPC messages and break your server.**For HTTP-based servers:** Standard output logging is fine since it doesn’t interfere with HTTP responses.\n\n### Best Practices\n\n1.   Use a logging library that writes to stderr or files.\n2.   For Python, be especially careful - `print()` writes to stdout by default.\n\n### Quick Examples\n\n```\n# ❌ Bad (STDIO)\nprint(\"Processing request\")\n\n# ✅ Good (STDIO)\nimport logging\nlogging.info(\"Processing request\")\n```\n\n### System requirements\n\n*   Python 3.10 or higher installed.\n*   You must use the Python MCP SDK 1.2.0 or higher.\n\n### Set up your environment\n\nFirst, let’s install `uv` and set up our Python project and environment:\n\nMake sure to restart your terminal afterwards to ensure that the `uv` command gets picked up.Now, let’s create and set up our project:\n\nNow let’s dive into building your server.\n\nBuilding your server\n--------------------\n\n### Importing packages and setting up the instance\n\nAdd these to the top of your `weather.py`:\n\n```\nfrom typing import Any\n\nimport httpx\nfrom mcp.server.fastmcp import FastMCP\n\n# Initialize FastMCP server\nmcp = FastMCP(\"weather\")\n\n# Constants\nNWS_API_BASE = \"https://api.weather.gov\"\nUSER_AGENT = \"weather-app/1.0\"\n```\n\nThe FastMCP class uses Python type hints and docstrings to automatically generate tool definitions, making it easy to create and maintain MCP tools.\n\n### Helper functions\n\nNext, let’s add our helper functions for querying and formatting the data from the National Weather Service API:\n\n```\nasync def make_nws_request(url: str) -> dict[str, Any] | None:\n    \"\"\"Make a request to the NWS API with proper error handling.\"\"\"\n    headers = {\"User-Agent\": USER_AGENT, \"Accept\": \"application/geo+json\"}\n    async with httpx.AsyncClient() as client:\n        try:\n            response = await client.get(url, headers=headers, timeout=30.0)\n            response.raise_for_status()\n            return response.json()\n        except Exception:\n            return None\n\ndef format_alert(feature: dict) -> str:\n    \"\"\"Format an alert feature into a readable string.\"\"\"\n    props = feature[\"properties\"]\n    return f\"\"\"\nEvent: {props.get(\"event\", \"Unknown\")}\nArea: {props.get(\"areaDesc\", \"Unknown\")}\nSeverity: {props.get(\"severity\", \"Unknown\")}\nDescription: {props.get(\"description\", \"No description available\")}\nInstructions: {props.get(\"instruction\", \"No specific instructions provided\")}\n\"\"\"\n```\n\n### Implementing tool execution\n\nThe tool execution handler is responsible for actually executing the logic of each tool. Let’s add it:\n\n```\n@mcp.tool()\nasync def get_alerts(state: str) -> str:\n    \"\"\"Get weather alerts for a US state.\n\n    Args:\n        state: Two-letter US state code (e.g. CA, NY)\n    \"\"\"\n    url = f\"{NWS_API_BASE}/alerts/active/area/{state}\"\n    data = await make_nws_request(url)\n\n    if not data or \"features\" not in data:\n        return \"Unable to fetch alerts or no alerts found.\"\n\n    if not data[\"features\"]:\n        return \"No active alerts for this state.\"\n\n    alerts = [format_alert(feature) for feature in data[\"features\"]]\n    return \"\\n---\\n\".join(alerts)\n\n@mcp.tool()\nasync def get_forecast(latitude: float, longitude: float) -> str:\n    \"\"\"Get weather forecast for a location.\n\n    Args:\n        latitude: Latitude of the location\n        longitude: Longitude of the location\n    \"\"\"\n    # First get the forecast grid endpoint\n    points_url = f\"{NWS_API_BASE}/points/{latitude},{longitude}\"\n    points_data = await make_nws_request(points_url)\n\n    if not points_data:\n        return \"Unable to fetch forecast data for this location.\"\n\n    # Get the forecast URL from the points response\n    forecast_url = points_data[\"properties\"][\"forecast\"]\n    forecast_data = await make_nws_request(forecast_url)\n\n    if not forecast_data:\n        return \"Unable to fetch detailed forecast.\"\n\n    # Format the periods into a readable forecast\n    periods = forecast_data[\"properties\"][\"periods\"]\n    forecasts = []\n    for period in periods[:5]:  # Only show next 5 periods\n        forecast = f\"\"\"\n{period[\"name\"]}:\nTemperature: {period[\"temperature\"]}°{period[\"temperatureUnit\"]}\nWind: {period[\"windSpeed\"]} {period[\"windDirection\"]}\nForecast: {period[\"detailedForecast\"]}\n\"\"\"\n        forecasts.append(forecast)\n\n    return \"\\n---\\n\".join(forecasts)\n```\n\n### Running the server\n\nFinally, let’s initialize and run the server:\n\n```\ndef main():\n    # Initialize and run the server\n    mcp.run(transport=\"stdio\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nYour server is complete! Run `uv run weather.py` to start the MCP server, which will listen for messages from MCP hosts.Let’s now test your server from an existing MCP host, Claude for Desktop.\n\nTesting your server with Claude for Desktop\n-------------------------------------------\n\nFirst, make sure you have Claude for Desktop installed. You can install the latest version here. If you already have Claude for Desktop, **make sure it’s updated to the latest version.**We’ll need to configure Claude for Desktop for whichever MCP servers you want to use. To do this, open your Claude for Desktop App configuration at `~/Library/Application Support/Claude/claude_desktop_config.json` in a text editor. Make sure to create the file if it doesn’t exist.For example, if you have VS Code installed:\n\nYou’ll then add your servers in the `mcpServers` key. The MCP UI elements will only show up in Claude for Desktop if at least one server is properly configured.In this case, we’ll add our single weather server like so:\n\nThis tells Claude for Desktop:\n\n1.   There’s an MCP server named “weather”\n2.   To launch it by running `uv --directory /ABSOLUTE/PATH/TO/PARENT/FOLDER/weather run weather.py`\n\nSave the file, and restart **Claude for Desktop**.\n\nLet’s get started with building our weather server! You can find the complete code for what we’ll be building here.\n\n### Prerequisite knowledge\n\nThis quickstart assumes you have familiarity with:\n\n*   TypeScript\n*   LLMs like Claude\n\n### Logging in MCP Servers\n\nWhen implementing MCP servers, be careful about how you handle logging:**For STDIO-based servers:** Never write to standard output (stdout). This includes:\n\n*   `print()` statements in Python\n*   `console.log()` in JavaScript\n*   `fmt.Println()` in Go\n*   Similar stdout functions in other languages\n\nWriting to stdout will corrupt the JSON-RPC messages and break your server.**For HTTP-based servers:** Standard output logging is fine since it doesn’t interfere with HTTP responses.\n\n### Best Practices\n\n1.   Use a logging library that writes to stderr or files, such as `logging` in Python.\n2.   For JavaScript, be especially careful - `console.log()` writes to stdout by default\n\n### Quick Examples\n\n```\n// ❌ Bad (STDIO)\nconsole.log(\"Server started\");\n\n// ✅ Good (STDIO)\nconsole.error(\"Server started\"); // stderr is safe\n```\n\n### System requirements\n\nFor TypeScript, make sure you have the latest version of Node installed.\n\n### Set up your environment\n\nFirst, let’s install Node.js and npm if you haven’t already. You can download them from nodejs.org. Verify your Node.js installation:\n\n```\nnode --version\nnpm --version\n```\n\nFor this tutorial, you’ll need Node.js version 16 or higher.Now, let’s create and set up our project:\n\nUpdate your package.json to add type: “module” and a build script:\n\npackage.json\n\n```\n{\n  \"type\": \"module\",\n  \"bin\": {\n    \"weather\": \"./build/index.js\"\n  },\n  \"scripts\": {\n    \"build\": \"tsc && chmod 755 build/index.js\"\n  },\n  \"files\": [\"build\"]\n}\n```\n\nCreate a `tsconfig.json` in the root of your project:\n\ntsconfig.json\n\n```\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"module\": \"Node16\",\n    \"moduleResolution\": \"Node16\",\n    \"outDir\": \"./build\",\n    \"rootDir\": \"./src\",\n    \"strict\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true\n  },\n  \"include\": [\"src/**/*\"],\n  \"exclude\": [\"node_modules\"]\n}\n```\n\nNow let’s dive into building your server.\n\nBuilding your server\n--------------------\n\n### Importing packages and setting up the instance\n\nAdd these to the top of your `src/index.ts`:\n\n```\nimport { McpServer } from \"@modelcontextprotocol/sdk/server/mcp.js\";\nimport { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio.js\";\nimport { z } from \"zod\";\n\nconst NWS_API_BASE = \"https://api.weather.gov\";\nconst USER_AGENT = \"weather-app/1.0\";\n\n// Create server instance\nconst server = new McpServer({\n  name: \"weather\",\n  version: \"1.0.0\",\n});\n```\n\n### Helper functions\n\nNext, let’s add our helper functions for querying and formatting the data from the National Weather Service API:\n\n```\n// Helper function for making NWS API requests\nasync function makeNWSRequest<T>(url: string): Promise<T | null> {\n  const headers = {\n    \"User-Agent\": USER_AGENT,\n    Accept: \"application/geo+json\",\n  };\n\n  try {\n    const response = await fetch(url, { headers });\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}`);\n    }\n    return (await response.json()) as T;\n  } catch (error) {\n    console.error(\"Error making NWS request:\", error);\n    return null;\n  }\n}\n\ninterface AlertFeature {\n  properties: {\n    event?: string;\n    areaDesc?: string;\n    severity?: string;\n    status?: string;\n    headline?: string;\n  };\n}\n\n// Format alert data\nfunction formatAlert(feature: AlertFeature): string {\n  const props = feature.properties;\n  return [\n    `Event: ${props.event || \"Unknown\"}`,\n    `Area: ${props.areaDesc || \"Unknown\"}`,\n    `Severity: ${props.severity || \"Unknown\"}`,\n    `Status: ${props.status || \"Unknown\"}`,\n    `Headline: ${props.headline || \"No headline\"}`,\n    \"---\",\n  ].join(\"\\n\");\n}\n\ninterface ForecastPeriod {\n  name?: string;\n  temperature?: number;\n  temperatureUnit?: string;\n  windSpeed?: string;\n  windDirection?: string;\n  shortForecast?: string;\n}\n\ninterface AlertsResponse {\n  features: AlertFeature[];\n}\n\ninterface PointsResponse {\n  properties: {\n    forecast?: string;\n  };\n}\n\ninterface ForecastResponse {\n  properties: {\n    periods: ForecastPeriod[];\n  };\n}\n```\n\n### Implementing tool execution\n\nThe tool execution handler is responsible for actually executing the logic of each tool. Let’s add it:\n\n```\n// Register weather tools\n\nserver.registerTool(\n  \"get_alerts\",\n  {\n    description: \"Get weather alerts for a state\",\n    inputSchema: {\n      state: z\n        .string()\n        .length(2)\n        .describe(\"Two-letter state code (e.g. CA, NY)\"),\n    },\n  },\n  async ({ state }) => {\n    const stateCode = state.toUpperCase();\n    const alertsUrl = `${NWS_API_BASE}/alerts?area=${stateCode}`;\n    const alertsData = await makeNWSRequest<AlertsResponse>(alertsUrl);\n\n    if (!alertsData) {\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: \"Failed to retrieve alerts data\",\n          },\n        ],\n      };\n    }\n\n    const features = alertsData.features || [];\n    if (features.length === 0) {\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: `No active alerts for ${stateCode}`,\n          },\n        ],\n      };\n    }\n\n    const formattedAlerts = features.map(formatAlert);\n    const alertsText = `Active alerts for ${stateCode}:\\n\\n${formattedAlerts.join(\"\\n\")}`;\n\n    return {\n      content: [\n        {\n          type: \"text\",\n          text: alertsText,\n        },\n      ],\n    };\n  },\n);\n\nserver.registerTool(\n  \"get_forecast\",\n  {\n    description: \"Get weather forecast for a location\",\n    inputSchema: {\n      latitude: z\n        .number()\n        .min(-90)\n        .max(90)\n        .describe(\"Latitude of the location\"),\n      longitude: z\n        .number()\n        .min(-180)\n        .max(180)\n        .describe(\"Longitude of the location\"),\n    },\n  },\n  async ({ latitude, longitude }) => {\n    // Get grid point data\n    const pointsUrl = `${NWS_API_BASE}/points/${latitude.toFixed(4)},${longitude.toFixed(4)}`;\n    const pointsData = await makeNWSRequest<PointsResponse>(pointsUrl);\n\n    if (!pointsData) {\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: `Failed to retrieve grid point data for coordinates: ${latitude}, ${longitude}. This location may not be supported by the NWS API (only US locations are supported).`,\n          },\n        ],\n      };\n    }\n\n    const forecastUrl = pointsData.properties?.forecast;\n    if (!forecastUrl) {\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: \"Failed to get forecast URL from grid point data\",\n          },\n        ],\n      };\n    }\n\n    // Get forecast data\n    const forecastData = await makeNWSRequest<ForecastResponse>(forecastUrl);\n    if (!forecastData) {\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: \"Failed to retrieve forecast data\",\n          },\n        ],\n      };\n    }\n\n    const periods = forecastData.properties?.periods || [];\n    if (periods.length === 0) {\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: \"No forecast periods available\",\n          },\n        ],\n      };\n    }\n\n    // Format forecast periods\n    const formattedForecast = periods.map((period: ForecastPeriod) =>\n      [\n        `${period.name || \"Unknown\"}:`,\n        `Temperature: ${period.temperature || \"Unknown\"}°${period.temperatureUnit || \"F\"}`,\n        `Wind: ${period.windSpeed || \"Unknown\"} ${period.windDirection || \"\"}`,\n        `${period.shortForecast || \"No forecast available\"}`,\n        \"---\",\n      ].join(\"\\n\"),\n    );\n\n    const forecastText = `Forecast for ${latitude}, ${longitude}:\\n\\n${formattedForecast.join(\"\\n\")}`;\n\n    return {\n      content: [\n        {\n          type: \"text\",\n          text: forecastText,\n        },\n      ],\n    };\n  },\n);\n```\n\n### Running the server\n\nFinally, implement the main function to run the server:\n\n```\nasync function main() {\n  const transport = new StdioServerTransport();\n  await server.connect(transport);\n  console.error(\"Weather MCP Server running on stdio\");\n}\n\nmain().catch((error) => {\n  console.error(\"Fatal error in main():\", error);\n  process.exit(1);\n});\n```\n\nMake sure to run `npm run build` to build your server! This is a very important step in getting your server to connect.Let’s now test your server from an existing MCP host, Claude for Desktop.\n\nTesting your server with Claude for Desktop\n-------------------------------------------\n\nFirst, make sure you have Claude for Desktop installed. You can install the latest version here. If you already have Claude for Desktop, **make sure it’s updated to the latest version.**We’ll need to configure Claude for Desktop for whichever MCP servers you want to use. To do this, open your Claude for Desktop App configuration at `~/Library/Application Support/Claude/claude_desktop_config.json` in a text editor. Make sure to create the file if it doesn’t exist.For example, if you have VS Code installed:\n\nYou’ll then add your servers in the `mcpServers` key. The MCP UI elements will only show up in Claude for Desktop if at least one server is properly configured.In this case, we’ll add our single weather server like so:\n\nThis tells Claude for Desktop:\n\n1.   There’s an MCP server named “weather”\n2.   Launch it by running `node /ABSOLUTE/PATH/TO/PARENT/FOLDER/weather/build/index.js`\n\nSave the file, and restart **Claude for Desktop**.\n\n\n","status":"completed"}},"type":"mcp"}</glm_block>


<details type="reasoning" done="true" duration="5" view="" last_tool_call_name="click">
> アプリがうまく機能していない理由として、考えられる問題点がいくつかあります。まず、スコープが広すぎる可能性があります。MCPサーバーのような小さなコンポーネントから始めることをお勧めします。また、MCPやLLM APIの設定ミス、ワークフローのオーケストレーション不足、UI/UXの問題、あるいは単に疲労が原因で進捗が停滞していることも考えられます。対応策として、MCPサーバーを構築し、LLM APIを呼び出し、LangChainやCrewAIでワークフローを自動化するといったステップを踏むことを提案します。まずはMCPサーバーの構築から始めましょう。
</details>

<glm_block view="" tool_call_name="click">{"data":{"browser":{"url":"https://medium.com/@akankshasinha247/function-calling-how-llms-invoke-real-world-apis-openai-gemini-examples-266bdd802c03","title":"How LLMs Invoke Real-World APIs (OpenAI & Gemini","content":"【40†How LLMs Invoke Real-World APIs (OpenAI & Gemini†https://medium.com/@akankshasinha247/function-calling-how-llms-invoke-real-world-apis-openai-gemini-examples-266bdd802c03】\nTitle: Function Calling: How LLMs Invoke Real-World APIs (OpenAI & Gemini examples)\n\nURL Source: https://medium.com/@akankshasinha247/function-calling-how-llms-invoke-real-world-apis-openai-gemini-examples-266bdd802c03\n\nPublished Time: 2025-06-04T08:22:11Z\n\nMarkdown Content:\n5 min read\n\nJun 4, 2025\n\n_Day 64 of #100DaysOfAI |Module 4: Tool Use, Function Calling, and Agents -Part3_\n\nI) Introduction — From “Chatbots” → “Actionable Agents”\n-------------------------------------------------------\n\nLarge Language Models (LLMs) are incredibly powerful at generating text — but **real-world AI systems need more than words.** They need the ability to:\n\n• Fetch **live data**\n\n• Interact with **external APIs**\n\n• Perform **actions** in the real world\n\n👉 This is where **Function Calling** comes in.\n\nIt allows LLMs to **bridge to external tools** — transforming them from “smart typists” into **actionable AI agents.**\n\nII) What is Function Calling & Why it Matters\n---------------------------------------------\n\n**Function Calling** allows an LLM to:\n\n• Dynamically decide whether to **invoke an external function / tool / API**\n\n• Automatically generate **structured arguments** for that function\n\n• **Receive the result**, and integrate it into its final response\n\n### Why it’s Important:\n\n🔹 **Bridges LLMs to your systems** → makes them actionable\n\n🔹 **Enables multi-step agentic workflows** → call tools, get results, reason again\n\n🔹 **Supports live data → reduces hallucination**\n\n🔹 **Works across OpenAI, Gemini, Hugging Face, Mistral, etc**\n\n🔹 **Core building block for Agent Frameworks** (LangChain Agents, LangGraph, ReAct agents, AutoGPT, etc)\n\nIII) How Function Calling Works — Conceptual Flow\n-------------------------------------------------\n\nHere’s a clear high-level flow for **how Function Calling works** in practice:\n\nPress enter or click to view image in full size\n\nFunction Calling Cycle between Application & LLM : (**Source:**Hugging Face Function Calling Guide)\n\n**Steps:**\n\n1. **Application** calls LLM with **prompt + function definitions**\n\n2. **LLM** decides whether to call tools — generates **arguments**\n\n3. **Application** executes tool/API call → sends result back to LLM\n\n4. **LLM** reasons again → may call another tool, or generate final response\n\n5. **Loop continues** until final response is generated\n\nIV) Timeline & Context\n----------------------\n\n🕰️ **2020 → 2022**\n\n 🔸 _Few-Shot Learning_ → Brown et al., OpenAI, 2020\n\n 🔸 _Chain of Thought Prompting_ → Wei et al., Google, 2022\n\n 🔸 Early LLM apps → chatbots & text generation\n\n🕰️ **2023**\n\n 🔸 _Toolformer_ → Meta, 2023 → LLMs learn to call tools\n\n 🔸 _LangChain_ becomes popular → orchestrating LLM pipelines\n\n🕰️ **2024 → Now**\n\n 🔸 Shift toward **enterprise-grade GenAI systems**\n\n 🔸 Need for **robust orchestration + prompt engineering discipline**\n\n 🔸 Function Calling becomes a **core building block**\n\nPress enter or click to view image in full size\n\nEvolution of LLMs & GenAI Systems — From Few-Shot Learning to Toolformer & Enterprise-Grade Architectures\n\nV) Where Function Calling fits in the GenAI Stack\n-------------------------------------------------\n\n♦ **LLM API Layer + Tools / Function Calling Layer**\n\n→ Bridges **LLM APIs → Function Calling Layer → Tools / APIs → UX**\n\n → Often orchestrated via **LangChain / LangGraph**\n\n👉 Modern AI Stack (Menlo Ventures):\n\nPress enter or click to view image in full size\n\nGen AI Tech Stack | Source: Menlo Ventures\n\n👉 Conceptual Stack:\n\nPress enter or click to view image in full size\n\n**How Function Calling fits in the GenAI stack:**\n\n Bridges **LLM APIs → Function Calling Layer → Tools/APIs → UX** — orchestration typically handled via **LangChain / LangGraph** frameworks.\n\nVI) Function Calling in Action — OpenAI & Gemini Examples\n---------------------------------------------------------\n\nA. OpenAI Function Calling\n--------------------------\n\n1.   **Key Features:**\n\n♦ Models return **structured function call outputs (JSON)**\n\n♦ Model can decide _whether_ to call a function\n\n♦ Generates **parameters** for API calls\n\n**2. Common use cases:**\n\n*   Weather APIs\n*   Database queries\n*   Financial lookups\n*   E-commerce transactions\n*   Autonomous agent actions\n\n**Example — Weather API:**\n\nPress enter or click to view image in full size\n\n‘Get Weather’ Function Calling Use Case : (Source: Open Ai Function Calling)\n\nPress enter or click to view image in full size\n\nExample Output of ‘Get Weather’ Function Calling Use Case : (Source: Open Ai Function Calling)\n\n_Note: Experiment with function calling and_ _generate function schemas_ _in the_ _Playground_ _!_\n\nGet Akanksha Sinha’s stories in your inbox\n------------------------------------------\n\nJoin Medium for free to get updates from this writer.\n\n**3. OpenAI Function Calling Flow:**\n\nPress enter or click to view image in full size\n\n**OpenAI Function Calling Flow:**_(Source:_ _OpenAI Function Calling Docs_ _)_\n\n### 4. Example Tool Choice Configurations:\n\n→ **Auto**, **Required**, **Forced** function call options allow fine control over **how/when LLMs invoke tools**.\n\nPress enter or click to view image in full size\n\n**Example Tool Choice Configurations (Source:****OpenAI Function Calling Docs****)**\n\n → _Auto_, _Required_, and _Forced Function_ options allow fine control over *_when and how LLMs invoke tools or APIs in production workflows._\n\nB. Gemini (Google) Function Calling / Tool Use\n----------------------------------------------\n\n1.   **Key Features:**\n\n♦ Supported in **Gemini 1.5 Pro / Flash**\n\n♦ Structured **Tool Use**\n\n♦ Integrated with **Vertex AI pipelines**\n\n♦ Supports **tool chaining**&**agent orchestration**\n\n**2. Vertex AI — Gemini Function Calling Architecture:**\n\nPress enter or click to view image in full size\n\n**Gemini Function Calling architecture**(Source: Google Cloud Vertex AI Docs)\n\n → Shows how Gemini integrates **structured Tool Use and Function Calling** into Vertex AI pipelines for agent-based orchestration.\n\n→ Note how Gemini integrates **function calling within broader agent flows**.\n\nVII) Why Function Calling matters\n---------------------------------\n\n✅ Enables **actionable agents**\n\n✅ Fresh data → **live & grounded responses**\n\n✅ Verifiable → **API-backed answers**\n\n✅ Reduces **hallucination** risk\n\n✅ Powers **LangGraph multi-agent workflows**\n\n✅ Critical for **production-grade GenAI systems**\n\n👉 Without Function Calling → LLMs = “smart typists.”\n\n👉 With Function Calling → LLMs = **intelligent, API-driven agents**.\n\nVIII) Resources & References\n----------------------------\n\n### ♦ Foundational Research & Concepts\n\n*   Few-Shot Learning — Brown et al., OpenAI, 2020\n*   Chain-of-Thought Prompting — Wei et al., Google, 2022\n*   Toolformer — Schick et al., Meta, 2023\n\n### ♦ Official Function Calling Documentation\n\n*   OpenAI Function Calling Guide\n*   Vertex AI Function Calling Guide (Google Cloud)\n*   Mistral Function Calling Documentation\n*   HuggingFace Hugs Function Calling Guide\n\n### ♦ Tutorials & Practical Guides\n\n*   Google Colab — Gemini Function Calling Intro Notebook\n*   Prompting Guide — Function Calling with LLMs\n\n### ♦ Tools & Ecosystem\n\n*   LangChain GitHub\n*   LangGraph GitHub\n*   DSPy Project — Stanford NLP\n\n### 💡 Use Case Inspiration\n\n*   Conversational Agents & AI Assistants\n*   Conversational Retrieval / Knowledge Base Querying\n*   Automated Workflows & Business Process Automation\n*   Dynamic API-driven Applications (Weather, Travel, E-Commerce, Banking, etc.)\n*   Function-based Content Generation & Transformation\n*   Secure Enterprise Data Integration with LLMs\n\nIX) Recap / Cheat Sheet\n-----------------------\n\n**Function Calling:**\n\n✅ Bridges **LLM → API → action**\n\n✅ Returns **structured function calls**\n\n✅ Powers **agents & production GenAI**\n\n✅ Orchestrated via **LangGraph / LangChain / DSPy**\n\nComing Up Next — Day 65\n-----------------------\n\n👉 **LLM Agents → ReAct, Toolformer, AutoGPT family**\n\n👉 **Autonomous agent frameworks**\n\n#100DaysOfAI #AIWithAkanksha #FunctionCalling #OpenAI #Gemini #LLMApplications #AIEngineering #GenAIStack #LangGraph #DSPy #LLMAgents #ProductionGenAI\n\n🗓️ 3rd June 2025\n\n"},"metadata":{"arguments":"{\"link_id\":40}","display_result":"","duration":"7s","id":"call_XNZPS8Rlvm","is_error":false,"mcp_server":{"name":"mcp-server"},"name":"click","result":"【40†How LLMs Invoke Real-World APIs (OpenAI & Gemini†https://medium.com/@akankshasinha247/function-calling-how-llms-invoke-real-world-apis-openai-gemini-examples-266bdd802c03】\nTitle: Function Calling: How LLMs Invoke Real-World APIs (OpenAI & Gemini examples)\n\nURL Source: https://medium.com/@akankshasinha247/function-calling-how-llms-invoke-real-world-apis-openai-gemini-examples-266bdd802c03\n\nPublished Time: 2025-06-04T08:22:11Z\n\nMarkdown Content:\n5 min read\n\nJun 4, 2025\n\n_Day 64 of #100DaysOfAI |Module 4: Tool Use, Function Calling, and Agents -Part3_\n\nI) Introduction — From “Chatbots” → “Actionable Agents”\n-------------------------------------------------------\n\nLarge Language Models (LLMs) are incredibly powerful at generating text — but **real-world AI systems need more than words.** They need the ability to:\n\n• Fetch **live data**\n\n• Interact with **external APIs**\n\n• Perform **actions** in the real world\n\n👉 This is where **Function Calling** comes in.\n\nIt allows LLMs to **bridge to external tools** — transforming them from “smart typists” into **actionable AI agents.**\n\nII) What is Function Calling & Why it Matters\n---------------------------------------------\n\n**Function Calling** allows an LLM to:\n\n• Dynamically decide whether to **invoke an external function / tool / API**\n\n• Automatically generate **structured arguments** for that function\n\n• **Receive the result**, and integrate it into its final response\n\n### Why it’s Important:\n\n🔹 **Bridges LLMs to your systems** → makes them actionable\n\n🔹 **Enables multi-step agentic workflows** → call tools, get results, reason again\n\n🔹 **Supports live data → reduces hallucination**\n\n🔹 **Works across OpenAI, Gemini, Hugging Face, Mistral, etc**\n\n🔹 **Core building block for Agent Frameworks** (LangChain Agents, LangGraph, ReAct agents, AutoGPT, etc)\n\nIII) How Function Calling Works — Conceptual Flow\n-------------------------------------------------\n\nHere’s a clear high-level flow for **how Function Calling works** in practice:\n\nPress enter or click to view image in full size\n\nFunction Calling Cycle between Application & LLM : (**Source:**Hugging Face Function Calling Guide)\n\n**Steps:**\n\n1. **Application** calls LLM with **prompt + function definitions**\n\n2. **LLM** decides whether to call tools — generates **arguments**\n\n3. **Application** executes tool/API call → sends result back to LLM\n\n4. **LLM** reasons again → may call another tool, or generate final response\n\n5. **Loop continues** until final response is generated\n\nIV) Timeline & Context\n----------------------\n\n🕰️ **2020 → 2022**\n\n 🔸 _Few-Shot Learning_ → Brown et al., OpenAI, 2020\n\n 🔸 _Chain of Thought Prompting_ → Wei et al., Google, 2022\n\n 🔸 Early LLM apps → chatbots & text generation\n\n🕰️ **2023**\n\n 🔸 _Toolformer_ → Meta, 2023 → LLMs learn to call tools\n\n 🔸 _LangChain_ becomes popular → orchestrating LLM pipelines\n\n🕰️ **2024 → Now**\n\n 🔸 Shift toward **enterprise-grade GenAI systems**\n\n 🔸 Need for **robust orchestration + prompt engineering discipline**\n\n 🔸 Function Calling becomes a **core building block**\n\nPress enter or click to view image in full size\n\nEvolution of LLMs & GenAI Systems — From Few-Shot Learning to Toolformer & Enterprise-Grade Architectures\n\nV) Where Function Calling fits in the GenAI Stack\n-------------------------------------------------\n\n♦ **LLM API Layer + Tools / Function Calling Layer**\n\n→ Bridges **LLM APIs → Function Calling Layer → Tools / APIs → UX**\n\n → Often orchestrated via **LangChain / LangGraph**\n\n👉 Modern AI Stack (Menlo Ventures):\n\nPress enter or click to view image in full size\n\nGen AI Tech Stack | Source: Menlo Ventures\n\n👉 Conceptual Stack:\n\nPress enter or click to view image in full size\n\n**How Function Calling fits in the GenAI stack:**\n\n Bridges **LLM APIs → Function Calling Layer → Tools/APIs → UX** — orchestration typically handled via **LangChain / LangGraph** frameworks.\n\nVI) Function Calling in Action — OpenAI & Gemini Examples\n---------------------------------------------------------\n\nA. OpenAI Function Calling\n--------------------------\n\n1.   **Key Features:**\n\n♦ Models return **structured function call outputs (JSON)**\n\n♦ Model can decide _whether_ to call a function\n\n♦ Generates **parameters** for API calls\n\n**2. Common use cases:**\n\n*   Weather APIs\n*   Database queries\n*   Financial lookups\n*   E-commerce transactions\n*   Autonomous agent actions\n\n**Example — Weather API:**\n\nPress enter or click to view image in full size\n\n‘Get Weather’ Function Calling Use Case : (Source: Open Ai Function Calling)\n\nPress enter or click to view image in full size\n\nExample Output of ‘Get Weather’ Function Calling Use Case : (Source: Open Ai Function Calling)\n\n_Note: Experiment with function calling and_ _generate function schemas_ _in the_ _Playground_ _!_\n\nGet Akanksha Sinha’s stories in your inbox\n------------------------------------------\n\nJoin Medium for free to get updates from this writer.\n\n**3. OpenAI Function Calling Flow:**\n\nPress enter or click to view image in full size\n\n**OpenAI Function Calling Flow:**_(Source:_ _OpenAI Function Calling Docs_ _)_\n\n### 4. Example Tool Choice Configurations:\n\n→ **Auto**, **Required**, **Forced** function call options allow fine control over **how/when LLMs invoke tools**.\n\nPress enter or click to view image in full size\n\n**Example Tool Choice Configurations (Source:****OpenAI Function Calling Docs****)**\n\n → _Auto_, _Required_, and _Forced Function_ options allow fine control over *_when and how LLMs invoke tools or APIs in production workflows._\n\nB. Gemini (Google) Function Calling / Tool Use\n----------------------------------------------\n\n1.   **Key Features:**\n\n♦ Supported in **Gemini 1.5 Pro / Flash**\n\n♦ Structured **Tool Use**\n\n♦ Integrated with **Vertex AI pipelines**\n\n♦ Supports **tool chaining**&**agent orchestration**\n\n**2. Vertex AI — Gemini Function Calling Architecture:**\n\nPress enter or click to view image in full size\n\n**Gemini Function Calling architecture**(Source: Google Cloud Vertex AI Docs)\n\n → Shows how Gemini integrates **structured Tool Use and Function Calling** into Vertex AI pipelines for agent-based orchestration.\n\n→ Note how Gemini integrates **function calling within broader agent flows**.\n\nVII) Why Function Calling matters\n---------------------------------\n\n✅ Enables **actionable agents**\n\n✅ Fresh data → **live & grounded responses**\n\n✅ Verifiable → **API-backed answers**\n\n✅ Reduces **hallucination** risk\n\n✅ Powers **LangGraph multi-agent workflows**\n\n✅ Critical for **production-grade GenAI systems**\n\n👉 Without Function Calling → LLMs = “smart typists.”\n\n👉 With Function Calling → LLMs = **intelligent, API-driven agents**.\n\nVIII) Resources & References\n----------------------------\n\n### ♦ Foundational Research & Concepts\n\n*   Few-Shot Learning — Brown et al., OpenAI, 2020\n*   Chain-of-Thought Prompting — Wei et al., Google, 2022\n*   Toolformer — Schick et al., Meta, 2023\n\n### ♦ Official Function Calling Documentation\n\n*   OpenAI Function Calling Guide\n*   Vertex AI Function Calling Guide (Google Cloud)\n*   Mistral Function Calling Documentation\n*   HuggingFace Hugs Function Calling Guide\n\n### ♦ Tutorials & Practical Guides\n\n*   Google Colab — Gemini Function Calling Intro Notebook\n*   Prompting Guide — Function Calling with LLMs\n\n### ♦ Tools & Ecosystem\n\n*   LangChain GitHub\n*   LangGraph GitHub\n*   DSPy Project — Stanford NLP\n\n### 💡 Use Case Inspiration\n\n*   Conversational Agents & AI Assistants\n*   Conversational Retrieval / Knowledge Base Querying\n*   Automated Workflows & Business Process Automation\n*   Dynamic API-driven Applications (Weather, Travel, E-Commerce, Banking, etc.)\n*   Function-based Content Generation & Transformation\n*   Secure Enterprise Data Integration with LLMs\n\nIX) Recap / Cheat Sheet\n-----------------------\n\n**Function Calling:**\n\n✅ Bridges **LLM → API → action**\n\n✅ Returns **structured function calls**\n\n✅ Powers **agents & production GenAI**\n\n✅ Orchestrated via **LangGraph / LangChain / DSPy**\n\nComing Up Next — Day 65\n-----------------------\n\n👉 **LLM Agents → ReAct, Toolformer, AutoGPT family**\n\n👉 **Autonomous agent frameworks**\n\n#100DaysOfAI #AIWithAkanksha #FunctionCalling #OpenAI #Gemini #LLMApplications #AIEngineering #GenAIStack #LangGraph #DSPy #LLMAgents #ProductionGenAI\n\n🗓️ 3rd June 2025\n\n","status":"completed"}},"type":"mcp"}</glm_block>


<glm_block view="" tool_call_name="click">{"data":{"browser":{"url":"https://medium.com/@julio.pessan.pessan/top-10-ai-agent-frameworks-for-2025-langchain-leads-the-pack-in-multi-agent-orchestration-c115044bcb60","title":"Top 10 AI Agent Frameworks for 2025: LangChain Leads","content":"【34†Top 10 AI Agent Frameworks for 2025: LangChain Leads†https://medium.com/@julio.pessan.pessan/top-10-ai-agent-frameworks-for-2025-langchain-leads-the-pack-in-multi-agent-orchestration-c115044bcb60】\nTitle: Top 10 AI Agent Frameworks for 2025: LangChain Leads the Pack in Multi-Agent Orchestration\n\nURL Source: https://medium.com/@julio.pessan.pessan/top-10-ai-agent-frameworks-for-2025-langchain-leads-the-pack-in-multi-agent-orchestration-c115044bcb60\n\nPublished Time: 2025-12-10T09:02:16Z\n\nMarkdown Content:\nTop 10 AI Agent Frameworks for 2025: LangChain Leads the Pack in Multi-Agent Orchestration | by Julio Pessan | Dec, 2025 | Medium\n===============\n\nSitemap\n\nOpen in app\n\nSign up\n\nSign in\n\nWrite\n\nSearch\n\nSign up\n\nSign in\n\nMastodon\n\nMember-only story\n\nTop 10 AI Agent Frameworks for 2025: LangChain Leads the Pack in Multi-Agent Orchestration\n==========================================================================================\n\nJulio Pessan\n\nFollow\n\n5 min read\n\n·\n\nDec 10, 2025\n\n80\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\nYou know that moment when you’re knee-deep in code, trying to wrangle AI agents that won’t cooperate? Last month, I watched a friend’s startup burn three weeks on a chatbot that kept forgetting its own context. Then they discovered LangChain’s multi-agent orchestration. **Two days later, they had autonomous agents collaborating like a seasoned team.** That’s the power of choosing the right framework — and in 2025, the playing field just got a whole lot more interesting.\n\nHere’s Why You Should Care\n--------------------------\n\nListen, we’re not talking about some distant sci-fi scenario here. **AI agents are already handling customer support, managing supply chains, and even writing code alongside human developers.** The global AI agent market is projected to hit $47 billion by 2030, and the companies winning aren’t necessarily the ones with the biggest budgets — they’re the ones picking the right tools.\n\nIf you’re building anything that involves AI automation, the framework you choose today determines whether you’re scaling smoothly next quarter or drowning in technical debt. **This isn’t just developer stuff — this is strategic advantage wrapped in code.** Choose wrong, and you’re locked into limitations. Choose right, and you’re orchestrating intelligent systems…\n\nCreate an account to read the full story.\n-----------------------------------------\n\nThe author made this story available to Medium members only.\n\nIf you’re new to Medium, create a new account to read this story on us.\n\nContinue in app\n\nOr, continue in mobile web\n\nSign up with Google\n\nSign up with Facebook\n\nSign up with email\n\nAlready have an account? Sign in\n\n80\n\n80\n\nFollow\n\nWritten by Julio Pessan -----------------------\n\n1.5K followers\n\n·454 following\n\nAI Engineer I +10 Years in Digital Transformation & Automation | TechWriter I Follow: https://www.linkedin.com/in/julio-pessan/\n\nFollow\n\nNo responses yet\n----------------\n\nWrite a response\n\nWhat are your thoughts?\n\nCancel\n\nRespond\n\nMore from Julio Pessan\n----------------------\n\nJulio Pessan\n\nA Look Through Jeff Bezos’ 6 Meeting Rules ------------------------------------------ ### In a world where every second counts, businesses are constantly seeking ways to improve productivity. Whether it’s a quick catch-up or a…\n\nSep 30, 2024\n\nJulio Pessan\n\nAI Automations in 2025: How Agents Cut Operational Costs by 30% in Enterprises ------------------------------------------------------------------------------ ### Last Tuesday, a CFO at a mid-sized logistics company told me something wild: “We just cut our customer service budget by $480K annually —…\n\nDec 9, 2025\n\n2\n\nJulio Pessan\n\nMulti-Agent Systems in 2025: How Orchestration Turns Solo Bots into Enterprise Powerhouses ------------------------------------------------------------------------------------------ ### You know that feeling when you’re juggling three work apps, two spreadsheets, and a Slack thread that won’t quit? Now imagine your AI doing…\n\nDec 17, 2025\n\nJulio Pessan\n\nIt’s Only 11.9% Away from Devs Losing Their Jobs — And Claude Opus 4.5 Is Guilty -------------------------------------------------------------------------------- ### Anthropic just dropped Claude Opus 4.5 and, quietly, something broke in the software industry’s psychology.\n\nDec 1, 2025\n\nSee all from Julio Pessan\n\nRecommended from Medium\n-----------------------\n\nRiki Graha\n\n✨ (n8n Series) N8N Automation for AI Agents (Part 7 of 13) ---------------------------------------------------------- ### 🌟 Lead Magnet With a Rating AI Agent — The Shorter, Sharper, No-Fluff Version\n\nNov 28, 2025\n\nAlisha ✨\n\n☁️ Firebase 2025: The Complete Advanced Guide with Expert Tips & Modern Practices --------------------------------------------------------------------------------- ### In today’s app development world, Firebase is no longer just a “backend as a service” — it’s a complete ecosystem for building, scaling…\n\nNov 2, 2025\n\nIn\n\nTechnology Hits\n\nby\n\nSomya Golchha\n\nInside a Multi-Agent AI System: How 5 Java Agents Collaborate to Plan Your Entire Trip -------------------------------------------------------------------------------------- ### This article for Day 7 is part of my Agents in Action series, where we go from theory to hands-on implementation of AI agents in Java.\n\nDec 8, 2025\n\n1\n\nNextGenTechDawn\n\nMulti‑Agent Frameworks in 2025 and 2026 Predictions --------------------------------------------------- ### The year 2025 has been widely recognized as the inaugural “Year of the Agent.” and has been a breakout moment for multi-agent AI systems…\n\nDec 7, 2025\n\nIn\n\nLevel Up Coding\n\nby\n\nFareed Khan\n\nBuilding a Scalable, Production-Grade Agentic RAG Pipeline ---------------------------------------------------------- ### Autoscaling, Evaluation, AI Compute Workflows and more\n\nJan 1\n\n8\n\nIn\n\nActivated Thinker\n\nby\n\nKaustubh Kalyankar\n\nAgentic AI & RAG Demystified: Build Smarter, Cost-Efficient AI Workflows ------------------------------------------------------------------------ ### Learn how agentic AI and Retrieval-Augmented Generation (RAG) work together, when to use them, and practical tips to build scalable…\n\nJan 1\n\n3\n\nSee more recommendations\n\nHelp\n\nStatus\n\nAbout\n\nCareers\n\nPress\n\nBlog\n\nPrivacy\n\nRules\n\nTerms\n\nText to speech\n\n"},"metadata":{"arguments":"{\"link_id\":34}","display_result":"","duration":"4s","id":"call_NT7re9DLOQ","is_error":false,"mcp_server":{"name":"mcp-server"},"name":"click","result":"【34†Top 10 AI Agent Frameworks for 2025: LangChain Leads†https://medium.com/@julio.pessan.pessan/top-10-ai-agent-frameworks-for-2025-langchain-leads-the-pack-in-multi-agent-orchestration-c115044bcb60】\nTitle: Top 10 AI Agent Frameworks for 2025: LangChain Leads the Pack in Multi-Agent Orchestration\n\nURL Source: https://medium.com/@julio.pessan.pessan/top-10-ai-agent-frameworks-for-2025-langchain-leads-the-pack-in-multi-agent-orchestration-c115044bcb60\n\nPublished Time: 2025-12-10T09:02:16Z\n\nMarkdown Content:\nTop 10 AI Agent Frameworks for 2025: LangChain Leads the Pack in Multi-Agent Orchestration | by Julio Pessan | Dec, 2025 | Medium\n===============\n\nSitemap\n\nOpen in app\n\nSign up\n\nSign in\n\nWrite\n\nSearch\n\nSign up\n\nSign in\n\nMastodon\n\nMember-only story\n\nTop 10 AI Agent Frameworks for 2025: LangChain Leads the Pack in Multi-Agent Orchestration\n==========================================================================================\n\nJulio Pessan\n\nFollow\n\n5 min read\n\n·\n\nDec 10, 2025\n\n80\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\nYou know that moment when you’re knee-deep in code, trying to wrangle AI agents that won’t cooperate? Last month, I watched a friend’s startup burn three weeks on a chatbot that kept forgetting its own context. Then they discovered LangChain’s multi-agent orchestration. **Two days later, they had autonomous agents collaborating like a seasoned team.** That’s the power of choosing the right framework — and in 2025, the playing field just got a whole lot more interesting.\n\nHere’s Why You Should Care\n--------------------------\n\nListen, we’re not talking about some distant sci-fi scenario here. **AI agents are already handling customer support, managing supply chains, and even writing code alongside human developers.** The global AI agent market is projected to hit $47 billion by 2030, and the companies winning aren’t necessarily the ones with the biggest budgets — they’re the ones picking the right tools.\n\nIf you’re building anything that involves AI automation, the framework you choose today determines whether you’re scaling smoothly next quarter or drowning in technical debt. **This isn’t just developer stuff — this is strategic advantage wrapped in code.** Choose wrong, and you’re locked into limitations. Choose right, and you’re orchestrating intelligent systems…\n\nCreate an account to read the full story.\n-----------------------------------------\n\nThe author made this story available to Medium members only.\n\nIf you’re new to Medium, create a new account to read this story on us.\n\nContinue in app\n\nOr, continue in mobile web\n\nSign up with Google\n\nSign up with Facebook\n\nSign up with email\n\nAlready have an account? Sign in\n\n80\n\n80\n\nFollow\n\nWritten by Julio Pessan -----------------------\n\n1.5K followers\n\n·454 following\n\nAI Engineer I +10 Years in Digital Transformation & Automation | TechWriter I Follow: https://www.linkedin.com/in/julio-pessan/\n\nFollow\n\nNo responses yet\n----------------\n\nWrite a response\n\nWhat are your thoughts?\n\nCancel\n\nRespond\n\nMore from Julio Pessan\n----------------------\n\nJulio Pessan\n\nA Look Through Jeff Bezos’ 6 Meeting Rules ------------------------------------------ ### In a world where every second counts, businesses are constantly seeking ways to improve productivity. Whether it’s a quick catch-up or a…\n\nSep 30, 2024\n\nJulio Pessan\n\nAI Automations in 2025: How Agents Cut Operational Costs by 30% in Enterprises ------------------------------------------------------------------------------ ### Last Tuesday, a CFO at a mid-sized logistics company told me something wild: “We just cut our customer service budget by $480K annually —…\n\nDec 9, 2025\n\n2\n\nJulio Pessan\n\nMulti-Agent Systems in 2025: How Orchestration Turns Solo Bots into Enterprise Powerhouses ------------------------------------------------------------------------------------------ ### You know that feeling when you’re juggling three work apps, two spreadsheets, and a Slack thread that won’t quit? Now imagine your AI doing…\n\nDec 17, 2025\n\nJulio Pessan\n\nIt’s Only 11.9% Away from Devs Losing Their Jobs — And Claude Opus 4.5 Is Guilty -------------------------------------------------------------------------------- ### Anthropic just dropped Claude Opus 4.5 and, quietly, something broke in the software industry’s psychology.\n\nDec 1, 2025\n\nSee all from Julio Pessan\n\nRecommended from Medium\n-----------------------\n\nRiki Graha\n\n✨ (n8n Series) N8N Automation for AI Agents (Part 7 of 13) ---------------------------------------------------------- ### 🌟 Lead Magnet With a Rating AI Agent — The Shorter, Sharper, No-Fluff Version\n\nNov 28, 2025\n\nAlisha ✨\n\n☁️ Firebase 2025: The Complete Advanced Guide with Expert Tips & Modern Practices --------------------------------------------------------------------------------- ### In today’s app development world, Firebase is no longer just a “backend as a service” — it’s a complete ecosystem for building, scaling…\n\nNov 2, 2025\n\nIn\n\nTechnology Hits\n\nby\n\nSomya Golchha\n\nInside a Multi-Agent AI System: How 5 Java Agents Collaborate to Plan Your Entire Trip -------------------------------------------------------------------------------------- ### This article for Day 7 is part of my Agents in Action series, where we go from theory to hands-on implementation of AI agents in Java.\n\nDec 8, 2025\n\n1\n\nNextGenTechDawn\n\nMulti‑Agent Frameworks in 2025 and 2026 Predictions --------------------------------------------------- ### The year 2025 has been widely recognized as the inaugural “Year of the Agent.” and has been a breakout moment for multi-agent AI systems…\n\nDec 7, 2025\n\nIn\n\nLevel Up Coding\n\nby\n\nFareed Khan\n\nBuilding a Scalable, Production-Grade Agentic RAG Pipeline ---------------------------------------------------------- ### Autoscaling, Evaluation, AI Compute Workflows and more\n\nJan 1\n\n8\n\nIn\n\nActivated Thinker\n\nby\n\nKaustubh Kalyankar\n\nAgentic AI & RAG Demystified: Build Smarter, Cost-Efficient AI Workflows ------------------------------------------------------------------------ ### Learn how agentic AI and Retrieval-Augmented Generation (RAG) work together, when to use them, and practical tips to build scalable…\n\nJan 1\n\n3\n\nSee more recommendations\n\nHelp\n\nStatus\n\nAbout\n\nCareers\n\nPress\n\nBlog\n\nPrivacy\n\nRules\n\nTerms\n\nText to speech\n\n","status":"completed"}},"type":"mcp"}</glm_block>


<details type="reasoning" done="true" duration="118" view="" last_tool_call_name="click">
> プロジェクトがうまく進んでいないとのこと、それはよく分かります。開発の停滞は frustrating なものです。幸いなことに、いくつかの明確なステップを踏むことで、状況を改善し、再び軌道に乗せることができます。以下に、プロジェクトを成功に導くための具体的なロードマップを提案します。
> 
> ---
> 
> ## 🔍 ステップ 1：問題の特定
> 
> まず、プロジェクトがうまく進んでいない根本的な原因を特定することが重要です。以下の点を確認してみてください。
> 
> - **スコープが広すぎる**：すべてのツールを一度に統合しようとしていませんか？
> - **技術的な障壁**：APIの統合、認証、非同期処理などで問題が発生していませんか？
> - **ワークフローの混乱**：各ツールの連携やデータの流れが明確になっていますか？
> - **UI/UXの問題**：ユーザーインターフェースが直感的でない可能性がありますか？
> - **単なる疲労**：長時間の作業による集中力の低下はありませんか？
> 
> これらの問いに正直に答えることで、次に取るべきアクションが明確になります。
> 
> ---
> 
> ## 🛠️ ステップ 2：MVP（Minimum Viable Product）の定義
> 
> プロジェクト全体を一度に完成させようとすると、圧倒されてしまいます。まずは**MVP**を定義し、小さく始めることをお勧めします。
> 
> **MVPの例**：
> 
> - 単一のAI IDE（例：Cursor）と1つのLLM API（例：OpenAI）を接続する。
> - MCPサーバーを1つ構築し、天気予報やコード実行などの簡単なツールを提供する。
> - シンプルなコマンドラインインターフェース（CLI）から自然言語でコードを生成する機能を実装する。
> 
> MVPを達成することで、成功体験を得られ、モチベーションを維持しやすくなります。
> 
> ---
> 
> ## 🧩 ステップ 3：コンポーネントの分割
> 
> プロジェクトを以下のような小さなコンポーネントに分割し、それぞれを個別に開発・テストすることをお勧めします。
> 
> 1. **MCPサーバー**
>    - ツールの定義と実装（例：ファイル操作、API呼び出し）
>    - 設定ファイルの管理
> 2. **LLM APIアダプター**
>    - OpenAI、Anthropic、Google GeminiなどのAPIを統一したインターフェースで呼び出す
>    - レスポンスのフォーマット変換
> 3. **ワークフローオーケストレーター**
>    - LangChainやCrewAIを使用して、複数のAIエージェントを協調させる
>    - タスクのキューイングと状態管理
> 4. **UI/UXレイヤー**
>    - Webベースのダッシュボード（例：React + TypeScript）
>    - デスクトップアプリ（例：Electron）
>    - ターミナルベースのUI（例：Textual）
> 
> 各コンポーネントを独立して開発し、単体テストを実施することで、全体の品質を確保しやすくなります。
> 
> ---
> 
> ## 📚 ステップ 4：具体的な実装手順
> 
> 以下に、各コンポーネントの具体的な実装手順を示します。
> 
> ### 1. MCPサーバーの構築
> 
> MCP（Model Context Protocol）サーバーは、AIモデルと外部ツールを接続するための標準的なプロトコルです。まずは簡単なMCPサーバーを構築してみましょう。
> 
> **参考リソース**：
> - [Build an MCP server](https://modelcontextprotocol.io/docs/develop/build-server)
> 
> **手順**：
> 
> 1. **環境設定**
>    ```bash
>    python -m venv venv
>    source venv/bin/activate
>    pip install "mcp>=1.2.0" fastmcp httpx
>    ```
> 
> 2. **サーバーコードの作成** (`weather.py`)
>    ```python
>    from mcp.server.fastmcp import FastMCP
> 
>    mcp = FastMCP("weather")
> 
>    @mcp.tool()
>    async def get_alerts(state: str) -> str:
>        # 天気警報を取得するロジック
>        return f"Alerts for {state}"
> 
>    if __name__ == "__main__":
>        mcp.run(transport="stdio")
>    ```
> 
> 3. **設定ファイルの作成** (`claude_desktop_config.json`)
>    ```json
>    {
>      "mcpServers": {
>        "weather": {
>          "command": "uv",
>          "args": ["--directory", "/path/to/project", "run", "weather.py"]
>        }
>      }
>    }
>    ```
> 
> 4. **テスト**
>    - Claude Desktopやその他のMCPクライアントでサーバーを接続し、ツールが正しく認識されるか確認します。
> 
> ### 2. 複数のLLM APIの統合
> 
> 次に、OpenAI、Anthropic、Google Geminiなどの複数のLLM APIを統一したインターフェースで呼び出すためのアダプターを実装します。
> 
> **参考リソース**：
> - [Function Calling: How LLMs Invoke Real-World APIs](https://medium.com/@akankshasinha247/function-calling-how-llms-invoke-real-world-apis-openai-gemini-examples-266bdd802c03)
> 
> **手順**：
> 
> 1. **LLMクライアントの実装**
>    ```python
>    import openai
>    import anthropic
>    import google.generativeai as genai
> 
>    class LLMClient:
>        def __init__(self, provider: str, api_key: str):
>            self.provider = provider
>            self.api_key = api_key
>            if provider == "openai":
>                self.client = openai.OpenAI(api_key=api_key)
>            elif provider == "anthropic":
>                self.client = anthropic.Anthropic(api_key=api_key)
>            elif provider == "google":
>                genai.configure(api_key=api_key)
>                self.client = genai.GenerativeModel("gemini-pro")
> 
>        def call(self, prompt: str, **kwargs):
>            if self.provider == "openai":
>                response = self.client.chat.completions.create(
>                    model="gpt-4-turbo",
>                    messages=[{"role": "user", "content": prompt}],
>                    **kwargs
>                )
>                return response.choices[0].message.content
>            elif self.provider == "anthropic":
>                response = self.client.messages.create(
>                    model="claude-3-opus-20240229",
>                    max_tokens=1000,
>                    messages=[{"role": "user", "content": prompt}]
>                )
>                return response.content[0].text
>            elif self.provider == "google":
>                response = self.client.generate_content(prompt)
>                return response.text
>    ```
> 
> 2. **使用例**
>    ```python
>    openai_client = LLMClient("openai", "your-api-key")
>    print(openai_client.call("Hello, world!"))
>    ```
> 
> ### 3. ワークフローのオーケストレーション
> 
> LangChainやCrewAIを使用して、複数のAIエージェントを協調させるワークフローを構築します。
> 
> **参考リソース**：
> - [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction)
> - [CrewAI Documentation](https://docs.crewai.com/)
> 
> **手順**：
> 
> 1. **LangChainのインストール**
>    ```bash
>    pip install langchain langchain_openai
>    ```
> 
> 2. **エージェントの定義**
>    ```python
>    from langchain.agents import AgentType, initialize_agent, load_tools
>    from langchain_openai import ChatOpenAI
> 
>    llm = ChatOpenAI(model="gpt-4-turbo", temperature=0)
>    tools = load_tools(["serpapi", "llm-math"], llm=llm)
>    agent = initialize_agent(
>        tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True
>    )
> 
>    agent.run("What is the weather in Tokyo?")
>    ```
> 
> ### 4. UI/UXの実装
> 
> 最後に、ユーザーが直感的に操作できるインターフェースを実装します。ここでは、React + TypeScriptを使用したWebベースのダッシュボードを例にします。
> 
> **手順**：
> 
> 1. **プロジェクトの作成**
>    ```bash
>    npx create-react-app vibe-coding-app --template typescript
>    cd vibe-coding-app
>    ```
> 
> 2. **コンポーネントの作成** (`src/App.tsx`)
>    ```tsx
>    import React, { useState } from 'react';
> 
>    const App: React.FC = () => {
>      const [prompt, setPrompt] = useState('');
>      const [response, setResponse] = useState('');
> 
>      const handleGenerate = async () => {
>        const res = await fetch('/api/generate', {
>          method: 'POST',
>          headers: { 'Content-Type': 'application/json' },
>          body: JSON.stringify({ prompt }),
>        });
>        const data = await res.json();
>        setResponse(data.response);
>      };
> 
>      return (
>        <div className="App">
>          <h1>Vibe Coding App</h1>
>          <textarea
>            value={prompt}
>            onChange={(e) => setPrompt(e.target.value)}
>            placeholder="Enter your prompt here..."
>          />
>          <button onClick={handleGenerate}>Generate</button>
>          <pre>{response}</pre>
>        </div>
>      );
>    };
> 
>    export default App;
>    ```
> 
> 3. **バックエンドAPIの実装**
>    - FastAPIやFlaskを使用して、`/api/generate`エンドポイントを実装し、LLMクライアントを呼び出すようにします。
> 
> ---
> 
> ## 🔄 ステップ 5：統合とテスト
> 
> 各コンポーネントが完成したら、それらを統合し、全体としての動作をテストします。
> 
> 1. **エンドツーエンドテスト**
>    - ユーザーがUIからプロンプトを入力し、LLMが適切なツールを呼び出してコードを生成する一連の流れをテストします。
> 2. **デバッグ**
>    - 各コンポーネント間のデータの受け渡しが正しいか、エラーハンドリングが適切に行われているかを確認します。
> 3. **パフォーマンスの最適化**
>    - レスポンス時間が長い場合は、キャッシュの導入や非同期処理の見直しを行います。
> 
> ---
> 
> ## 🚀 ステップ 6：拡張と改善
> 
> MVPが完成したら、以下の機能を追加してアプリケーションを拡張していきます。
> 
> - **複数のAI IDEのサポート**：Cursor、VS Code、JetBrains IDEなどとの連携
> - **高度なワークフロー**：条件分岐やループを含む複雑なタスクの自動化
> - **ユーザー管理**：認証、認可、プロファイル管理
> - **テンプレートの提供**：よく使われるコード生成パターンのテンプレート化
> - **フィードバックループ**：ユーザーからのフィードバックを基にした継続的な改善
> 
> ---
> 
> ## 💡 最後に
> 
> プロジェクトがうまく進んでいないと感じるのは、あなたが完璧を目指している証拠です。しかし、完璧を目指すあまり、最初の一歩を踏み出すのが難しくなることもあります。
> 
> **小さく始めて、徐々に拡張していく**ことが、複雑なプロジェクトを成功させる鍵です。上記のステップを参考に、まずはMVPの実現に集中してみてください。
> 
> もし具体的な技術的な問題で行き詰まっている場合は、その詳細を教えてください。より具体的なアドバイスを提供させていただきます。
> 
> 頑張ってください！
</details>

<glm_block view="" tool_call_name="finish">{"data":{"metadata":{"arguments":"{}","display_result":"","duration":"0s","id":"call_D3EIHVIvz9","is_error":false,"mcp_server":{"name":"mcp-server"},"name":"finish","result":"","status":"completed"}},"type":"mcp"}</glm_block>

# 最高峰のバイブコーディング運用を実現するための統合AI開発環境：構築のための戦略的ロードマップ

## 現状分析と課題の特定：なぜ「最高峰のバイブコーディング」アプリはうまくいかないのか

現代のソフトウェア開発の潮流は、AIとの協調によって再定義されようとしています。その中で「バイブコーディング（Vibe Coding）」という、AIを直感的なパートナーとしてコード生成を行う開発スタイルが急速に注目を集めています[[21](https://en.wikipedia.org/wiki/Vibe_coding)]。これは、単なるAIによるコード補完を超え、開発者が自然言語で意図を伝えると、AIがその意図を理解してコードを生成するという、より創造的で直感的なプログラミングパラダイムです[[22](https://www.ibm.com/think/topics/vibe-coding)]。ご自身が現在開発されているアプリケーションが目指す「最高峰のバイブコーディング運用」とは、まさにこのパラダイムの最先端を行くものであり、複数のAI統合開発環境（AI IDE）、コマンドラインインターフェース（CLI）、大規模言語モデル（LLM）のAPI、そしてModel Context Protocol（MCP）といった、あらゆる先進ツールを統合し、開発ワークフロー全体をシームレスに導くという、非常に野心的かつ意欲的な目標です。しかし、そのような強力なビジョンを持つプロジェクトこそ、その複雑性ゆえに開発の初期段階で壁にぶつかり、「うまくいっていない」という状況に陥りがちです。この章では、なぜこのような革新的なアプリケーションがうまくいかないのか、その根本的な原因を多角的に分析し、問題の本質を明確にすることで、後続の具体的な解決策への道筋を示します。

第一に、考えられる最大の要因は、**「スコープの爆発」**です。「すべてのツールを最大限活用する」という目標は、開発者にとって非常に魅力的ですが、同時にプロジェクトの範囲を無限大に広げる危険をはらんでいます。AI IDE、CLIツール、LLM API、MCPサーバー、そしてそれらをオーケストレーションするワークフローエンジン。これらの要素はそれぞれが非常に深い専門知識を要求する領域です。例えば、MCPは2025年のAIエコシステムにおいて、LLMアプリケーションと外部データソースやツールをシームレスに統合するためのオープンプロトコルとして大きな注目を浴びており[[10](https://modelcontextprotocol.io/specification/2025-11-25)]、Visual Studio CodeやJetBrainsといった主要なIDEが次々とサポートを開始しています[[12](https://www.descope.com/learn/post/mcp)]。しかし、MCPサーバーを一から構築し、安定したサービスとして運用するには、プロトコルの仕様理解、実装、デバッグ、そしてセキュリティ対策まで、多くの工数が必要です[[50](https://modelcontextprotocol.io/docs/develop/build-server)]。同様に、複数のLLM API（OpenAI、Anthropic、Google Geminiなど）を統合する場合、各プロバイダーの仕様、レートリミット、コストモデル、そして「関数呼び出し（Function Calling）」といった高度な機能の違いを吸収する抽象化レイヤーの設計が求められます[[40](https://medium.com/@akankshasinha247/function-calling-how-llms-invoke-real-world-apis-openai-gemini-examples-266bdd802c03)]。これらの要素を一度に完成させようとすると、開発チームのリソースが分散し、どの部分も完成度が低いまま、進捗が停滞してしまうという典型的な「すべてをやりすぎて、何も完成しない」状態に陥りやすくなります。この問題は、プロジェクトの成功を妨げる最も一般的かつ深刻な障壁の一つです。

第二に、**「技術的負債の先取り」**の問題があります。2025年は「AIエージェント時代（Agentic Era）」の幕開けと呼ばれ、単純なIDEチャットボットを超えた、自律的にタスクを実行するエージェント型CLIツールが登場しています[[5](https://thenewstack.io/ai-coding-tools-in-2025-welcome-to-the-agentic-cli-era)]。LangChainやCrewAIといったAIエージェントフレームワークは、複数のLLMやAPIを調整してマルチエージェントシステムを構築するための強力な機能を提供しており[[34](https://medium.com/@julio.pessan.pessan/top-10-ai-agent-frameworks-for-2025-langchain-leads-the-pack-in-multi-agent-orchestration-c115044bcb60)]、これらを利用することで高度なワークフロー自動化が可能になります。しかし、これらのフレームワークはまだ進化の途上にあり、その仕様は頻繁に更新されます。また、複数のAIツールを組み合わせる際には、それぞれのツールが持つ独自の設定、認証方法、データフォーマットなどを管理する必要があり、これが著しい複雑性を生み出します。例えば、あるツールはJSON-RPC over stdioで通信し、別のツールはREST APIを使う。あるLLMは関数呼び出しのために特定のスキーマを要求し、別のLLMは異なる形式を期待する。これらの差異をすべて吸収し、統一されたインターフェースを提供しようとすると、アプリケーションのコア部分が大量のアダプターコードで埋まり、結果として、理解、保守、拡張が極めて困難な「スパゲッティアーキテクチャ」が出来上がってしまいます。これは、将来の変更に対して非常に脆く、些細な仕様変更がシステム全体に波及し、開発が完全にストップしてしまう可能性を秘めています。

第三に、**「ワークフローの設計と抽象化の失敗」**が挙げられます。「最高峰のバイブコーディング」を実現するためには、AIが開発者の意図を理解し、適切なツールを選択し、タスクを分解し、実行し、結果をフィードバックするという、高度なワークフローを設計する必要があります。これは、単にツールを並べるだけではなく、それらを知的に繋ぐ「接着剤」の役割が極めて重要になることを意味します。AIオーケストレーションツールは、まさにこの「接着剤」の役割を担うものですが[[30](https://www.kubiya.ai/blog/ai-orchestration-tools)]、その適切な選定と利用は容易ではありません。開発者が何を求めているのかをAIがどうやって理解するのか、複数の実行プランの中から最適なものをどうやって選択するのか、ツールの実行が失敗した場合にどうやってリカバリーするのか、といった、ワークフローにおける意思決定のロジックを設計する必要があります。ここで失敗すると、AIは開発者の期待に応えられず、かえって手作業での修正の方が早いという本末転倒な事態を招きます。特に、バイブコーディングは「雰囲気」や「文脈」に依存する部分が大きく、それを形式知としてコードに落とし込む抽象化の設計が不十分だと、AIは的外れなコードを生成し続け、開発者のフラストレーションが溜まる一方でしょう。

第四に、**「UX（ユーザーエクスペリエンス）の設計難易度」**です。ご自身のアプリは「直感的に最高峰のバイブコーディングをするためのアプリ」を目指しています。これは、UI/UX設計がアプリの成否を直接左右することを意味します。開発者は、どのようなインターフェースでAIと対話するのでしょうか。チャット形式でしょうか、それとも特定のドメインに特化したカスタムUIでしょうか。AIが生成したコードはどのようにプレビューされ、編集され、実行されるのでしょうか。エラーが発生した場合、どのようにフィードバックが与えられるのでしょうか。これらのUXの設計は、バックエンドの複雑な技術ロジックとは別次元の難しさがあります。例えば、AIが複数のステップを含むワークフローを実行している途中で、ユーザーがその進捗をどのように把握できるか。AIが内部でどのツールを、なぜ選択したのかという意思決定プロセスをどのように可視化するか。こうした「透明性」と「制御性」を確保するUX設計が欠如していると、ユーザーはAIを信頼できず、結局使わなくなってしまいます。特に、CLIツールとGUIの統合など、異なるインタラクションモデルをシームレスに繋ぐUX設計は、非常に高度な専門性が要求される領域です。

第五に、そして最も人間的な要因として、**「開発者自身の認知負荷と疲労」**があります。最先端の技術を常に追いかけ、それらを組み合わせて何か新しいものを生み出そうとするプロセスは、精神的に非常に消耗します。特に、AIの進化は日進月歩であり、昨日の常識が今日の非常識になる世界で開発を続けるには、絶え間ない学習と適応が求められます。そのような状況下で、あまりにも野心的な目標を掲げてしまうと、どこから手をつければいいのか分からなくなり、分析麻痺に陥ったり、小さな失敗が積み重なって燃え尽きてしまったりします。これは、プロジェクトマネジメントや個人のメンタルヘルスの問題ですが、イノベーティブなプロジェクトほど無視できない重要な側面です。定期的に小さな成功体験を積み重ね、進捗を実感できる仕組みを作らないと、モチベーションを維持することが困難になります。

これらの課題は、決してご自身のプロジェクトだけが抱える特別な問題ではありません。2025年という時代は、AIとソフトウェア開発の融合が急加速する一方で、その潮流に乗りこなすためのベストプラクティスがまだ確立されていない「荒野」のようなものです。多くの開発者が、同じような壁にぶつかりながら、手探りで進んでいるのが現状です。重要なのは、これらの課題を具体的に特定し、一つ一つ確実に解決していくための、現実的かつ戦略的なアプローチを採用することです。次章以降では、これらの課題を克服するための具体的なステップバイステップのロードマップを提示し、あなたのビジョンを現実のものとするための道筋を示していきます。

## 戦略的ロードマップ：MVPから始める段階的アプローチ

前章で分析したように、「最高峰のバイブコーディング」アプリが直面する課題は多岐にわたり、その根本には「やりすぎ」による複雑性の爆発があります。この巨大な壁を一度に乗り越えようとするのではなく、ここでは戦略的な段階的アプローチ、すなわち**MVP（Minimum Viable Product、実用可能最小限の製品）**から始めるロードマップを提案します。このアプローチは、プロジェクトを管理可能な小さなゴールに分割し、各ステップで成功体験を積み重ねながら、着実に最終目標に近づくことを可能にします。これは、複雑なシステムを構築する際のソフトウェア工学の基本原則であり、特に変化の激しいAI技術を扱う上で極めて有効な戦略です。このロードマップは、大きく分けて「フェーズ1：基盤構築（MVPの実現）」、「フェーズ2：機能拡張とワークフローの高度化」、「フェーズ3：エコシステム統合とUX向上」の三つの段階で構成されます。

### フェーズ1：基盤構築（MVPの実現）

このフェーズの目標は、アプリケーションの核心となる価値提案を、最もシンプルな形で実現することです。ここでのMVPは、「一つのAI IDEと一つのLLM APIを連携させ、開発者が自然言語で指示を出すと、AIが簡単なコードを生成してくれる」という体験です。これにより、プロジェクトの技術的実現可能性を検証し、早期にフィードバックを得ることができます。

**ステップ1：MVPの定義とスコープの厳格化**
まず、MVPの範囲を厳密に定義します。この段階では、以下のような機能のみを目指します。
*   **AI IDEの選定**：CursorやVS Code（Copilot搭載）など、市場で成熟しているAI IDEを一つ選びます。
*   **LLMプロバイダーの選定**：OpenAI（GPT-4）またはAnthropic（Claude 3）など、性能が高くAPIが安定しているプロバイダーを一つ選びます。
*   **コア機能の実装**：テキストエリアに自然言語のプロンプトを入力すると、選択したLLM APIにリクエストを送信し、生成されたコードを表示するという、非常にシンプルなWebベースまたはデスクトップベースのUIを実装します。
*   **スコープ外の明確化**：MCPサーバーの構築、複数LLMの切り替え、高度なワークフローオーケストレーションといった機能は、このフェーズでは明確にスコープ外とします。この「何をやらないか」の定義が、プロジェクトを成功させる鍵となります。

**ステップ2：最初のMCPサーバー構築**
Model Context Protocol（MCP）は、あなたのアプリが目指す「すべてのツールを統合する」というビジョンを実現するための非常に重要な技術要素です。MCPは、LLMアプリケーションが外部のデータソースやツールと安全かつ標準化された方法で対話するためのオープンプロトコルであり、2025年には業界標準として広く受け入れられつつあります[[10](https://modelcontextprotocol.io/specification/2025-11-25)]。MVP段階では、まずMCPの基本を理解するために、非常にシンプルなMCPサーバーを自作することから始めます。これにより、プロトコルの仕様や、AIが外部ツールを呼び出すメカニズムを深く理解することができます。
公式ドキュメントなどを参考に、天気予報を取得する簡単なMCPサーバーを構築するのが良い出発点です[[50](https://modelcontextprotocol.io/docs/develop/build-server)]。このチュートリアルでは、Pythonの`FastMCP`ライブラリを使い、`get_alerts`と`get_forecast`という2つのツールを提供するサーバーを構築します。この過程で、LLMからのリクエストを待ち受け、定義されたツールを実行し、結果をJSON-RPC形式で返すという、MCPサーバーの基本的な役割を学ぶことができます。重要なのは、この段階で完璧なサーバーを作ることではなく、MCPの「雰囲気」を掴むことです。

**ステップ3：LLM APIとの統合（関数呼び出しの実装）**
次に、選定したLLMプロバイダーのAPIを呼び出す仕組みを実装します。ここでの核心となるのは「関数呼び出し（Function Calling）」機能です。関数呼び出しとは、LLMが自然言語のプロンプトから、必要に応じて外部の関数やAPIを呼び出すべきかを判断し、そのための引数を自動生成してくれる機能です[[40](https://medium.com/@akankshasinha247/function-calling-how-llms-invoke-real-world-apis-openai-gemini-examples-266bdd802c03)]。これにより、LLMは単なるテキスト生成器から、外部世界と相互作用できる「実行可能なエージェント」へと進化します。
実装では、まずLLMに利用可能なツール（例えば、ステップ2で作成したMCPサーバーの`get_alerts`ツール）の定義（スキーマ）を渡します。そして、ユーザーが「カリフォルニア州の天気警報を教えて」とプロンプトを入力すると、LLMは「`get_alerts`関数を`state`引数を`'CA'`として呼び出す必要がある」と判断し、構造化されたデータ（JSON）を返します。あなたのアプリはそのJSONを受け取り、実際にMCPサーバーを呼び出し、得られた結果を再度LLMに渡します。最後に、LLMはその結果を基に、ユーザーに対して自然な言葉で回答を生成します。この「アプリ → LLM → アプリ → MCPサーバー → アプリ → LLM → ユーザー」という一連の流れを実装することが、このステップのゴールです。

### フェーズ2：機能拡張とワークフローの高度化

フェーズ1で基盤が完成したら、次はアプリケーションの機能を拡張し、より「バイブコーディング」らしい体験を提供できる段階に進みます。

**ステップ4：複数LLM APIとツールの統合**
MVPでは一つのLLMプロバイダーのみに対応しましたが、このステップではOpenAI、Anthropic、Google Geminiといった主要なプロバイダーのAPIをすべてサポートできるようにします[[43](https://machinelearningmastery.com/10-python-one-liners-for-calling-llms-from-your-code)]。これにより、ユーザーはタスクに応じて最適なモデルを選択できるようになります。実装にあたっては、各プロバイダーのAPI仕様の違いを吸収するための抽象化レイヤー（アダプター層）を設計することが重要です。これにより、アプリケーションのコアロジックはプロバイダーに依存せず、将来的に新しいプロバイダーが追加されても柔軟に対応できます。
また、自作のMCPサーバーだけでなく、ファイルシステム操作、Gitコマンド実行、Web検索など、開発者が日常的に使用する様々なツールをMCP経由で利用できるように統合を進めます。これにより、AIはより多くの「手」を持つことができ、より複雑なタスクを自動化できるようになります。

**ステップ5：AIオーケストレーションフレームワークの導入**
ツールの数が増えてくると、それらを適切に組み合わせてタスクを遂行するための「ワークフローエンジン」が必要になります。ここで登場するのが、LangChainやCrewAIといったAIオーケストレーションフレームワークです[[34](https://medium.com/@julio.pessan.pessan/top-10-ai-agent-frameworks-for-2025-langchain-leads-the-pack-in-multi-agent-orchestration-c115044bcb60)]。これらのフレームワークは、複数のLLMエージェントを作成し、それらに役割（ロール）を与え、協調させて複雑なタスクを解決するための仕組みを提供します。
例えば、「新しいWeb APIのエンドポイントを作成する」というタスクを考えてみましょう。オーケストレーションフレームワークを使うと、以下のようなワークフローを設計できます。
1.  **プランニングエージェント**：ユーザーの要求から、必要なエンドポイントの仕様（HTTPメソッド、パス、リクエスト/レスポンスのスキーマなど）を定義する。
2.  **コーディングエージェント**：定義された仕様に基づいて、バックエンドのフレームワーク（例：FastAPI）を使ったコードを生成する。
3.  **テストエージェント**：生成されたコードに対して、単体テストや統合テストのコードを生成し、実行する。
4.  **デプロイエージェント**：テストが成功したコードを、Dockerコンテナ化し、クラウド環境にデプロイするための設定ファイルを生成する。
このように、大きなタスクを小さなサブタスクに分解し、専門化されたエージェントに分担させることで、より高度で信頼性の高い自動化を実現できます。このステップでは、これらのフレームワークを導入し、あなたのアプリケーションに「考える力」と「行動する力」を与えます。

### フェーズ3：エコシステム統合とUX向上

最終フェーズでは、あなたのアプリをより使いやすく、強力なプラットフォームへと昇華させます。

**ステップ6：AI IDEとの深い統合**
これまでのステップで、あなたのアプリはそれ自体が一つの開発環境となっていました。このステップでは、開発者が既に使い慣れているAI IDE（Cursor、VS Code、JetBrains IDEsなど）と、あなたのアプリが提供する高度な機能を直接連携させることを目指します。MCPは、まさにこのシナリオのために設計されています。JetBrains IDEsは2025.2バージョンからMCPサーバーに標準で対応しており、Claude DesktopやCursorといったAIクライアントもMCPをサポートしています[[15](https://www.jetbrains.com/help/ai-assistant/mcp.html)]。
あなたのアプリがMCPサーバーとして機能するように設計すれば、開発者は普段使いのIDEから、あなたのアプリが提供する強力なワークフローを直接呼び出せるようになります。例えば、VS Code内で「この関数のテストコードを生成して」と指示すると、バックグラウンドであなたのアプリがLLMと連携し、最適なテストコードを生成してエディタに挿入する、といったシームレスな体験が可能になります。これにより、開発者は新しいツールを学習するコストなく、あなたのアプリのパワーを享受できます。

**ステップ7：直感的なUXの完成**
最後に、ユーザーインターフェースを磨き上げます。ここでの焦点は、「直感的に最高峰のバイブコーディング」を実現するためのUXです。具体的には、以下のような機能の実装が考えられます。
*   **プロンプトのテンプレート化**：よく使うコード生成パターン（例：「新しいReactコンポーネントを作成」「FastAPIのルートを定義」）をテンプレートとして登録し、ワンクリックで呼び出せるようにする。
*   **実行計画の可視化**：AIが複雑なタスクを実行する前に、どのような手順で実行するかという「実行計画」をユーザーに提示し、承認を得るようにする。これにより、AIの行動を予測・制御でき、信頼性が向上します。
*   **対話的なデバッグ**：AIが生成したコードにエラーがあった場合、AIが自動的にエラーの原因を分析し、修正案を提示する。ユーザーはその修正案を受け入れたり、さらに質問したりして、対話的にコードを完成させていくことができます。
*   **パーソナライゼーション**：ユーザーのコーディングスタイルやプロジェクトの特性を学習し、よりパーソナライズされたコードを生成するようにする。

この三段階のロードマップを踏むことで、あなたのプロジェクトは「うまくいかない」という混沌とした状態から脱却し、一歩一歩確実に前進することができます。各フェーズの終わりには、動く製品と具体的な成果が手に入るため、モチベーションを維持しながら、最終的なビジョンの実現に向かって進むことができるでしょう。次章では、このロードマップの最初の重要な一歩である「MCPサーバーの構築」について、より詳細な技術的な手順を解説します。

## 実践ガイド：MCPサーバー構築入門

あなたのアプリケーションが目指す「最高峰のバイブコーディング」の実現において、Model Context Protocol（MCP）は中心的な役割を果たします。MCPは、大規模言語モデル（LLM）が外部のツールやデータソースと安全かつ標準化された方法で対話するためのオープンプロトコルです[[10](https://modelcontextprotocol.io/specification/2025-11-25)]。MCPサーバーを構築することは、AIに「手」と「足」を与えるようなもので、AIは単なるテキスト生成器から、実際の世界でタスクを実行できる「実行可能なエージェント（Actionable Agent）」へと進化します[[40](https://medium.com/@akankshasinha247/function-calling-how-llms-invoke-real-world-apis-openai-gemini-examples-266bdd802c03)]。この章では、前述の戦略的ロードマップにおける「ステップ2」に相当する、最初のMCPサーバーを構築するための具体的な手順を、公式ドキュメントに基づいて詳細に解説します[[50](https://modelcontextprotocol.io/docs/develop/build-server)]。この実践的なガイドを通じて、MCPの基本原理を理解し、あなたのアプリケーションに不可欠な基盤を築くことができます。

### MCPサーバー構築の準備：前提知識と環境設定

MCPサーバーの構築を始める前に、いくつかの前提知識と環境の準備が必要です。ここでは、公式ドキュメントに従い、Pythonを用いたサーバー構築プロセスを中心に説明します。

**前提知識**
*   **Pythonの基礎知識**：MCPサーバーはPythonで実装します。非同期処理（`async/await`）や型ヒントの基本的な理解があることが望ましいです。
*   **LLMへの理解**：LLM、特にClaudeのようなモデルがどのように動作し、どのようにプロンプトと対話するかについての基本的な知識が必要です。

**システム要件**
*   **Python 3.10以上**：MCPのPython SDKはPython 3.10以降のバージョンを必要とします。
*   **Python MCP SDK 1.2.0以上**：MCPサーバーを構築するための公式SDKをインストールする必要があります。

**環境設定**
プロジェクトの依存関係管理を容易にするため、`uv`という高速なPythonパッケージマネージャーとプロジェクトマネージャーを使用することを推奨します。以下のコマンドで`uv`をインストールし、プロジェクトの環境をセットアップします。

```bash
# uvのインストール
curl -LsSf https://astral.sh/uv/install.sh | sh

# プロジェクト用のディレクトリを作成し、移動
mkdir my-mcp-server
cd my-mcp-server

# Pythonプロジェクトを初期化
uv init
```

これで、MCPサーバーを開発するための準備が整いました。次に、必要なライブラリをインストールします。

```bash
# MCP SDKとHTTPクライアントライブラリをインストール
uv add "mcp>=1.2.0" fastmcp httpx
```

### 最初のMCPサーバー：天気情報ツールの実装

ここでは、具体的な例として、アメリカ国立気象局（National Weather Service, NWS）のAPIを呼び出して、特定の州の天気警報（`get_alerts`）と指定された緯度経度の天気予報（`get_forecast`）を取得するという2つのツールを提供するMCPサーバーを構築します。

**ステップ1：パッケージのインポートとサーバーインスタンスの初期化**
まず、`weather.py`というファイルを作成し、必要なパッケージをインポートして、FastMCPサーバーのインスタンスを初期化します。FastMCPは、Pythonの型ヒントとdocstringを利用して、自動的にツールの定義を生成してくれる便利なラッパーです。

```python
# weather.py

from typing import Any

import httpx
from mcp.server.fastmcp import FastMCP

# FastMCPサーバーを初期化
mcp = FastMCP("weather")

# 定数
NWS_API_BASE = "https://api.weather.gov"
USER_AGENT = "weather-app/1.0"
```

**重要：MCPサーバーにおけるロギング**
MCPサーバーを実装する際に最も重要な注意点の一つがロギングの扱いです。MCPサーバーは、クライアント（例：Claude Desktop）とJSON-RPCメッセージを標準出力（stdout）を通じてやり取りします。そのため、サーバー側のコードが`print()`文などでstdoutに何かを出力してしまうと、JSON-RPCメッセージが破損し、通信が失敗してしまいます。

```python
# ❌ 悪い例 (stdio通信を使用する場合)
print("Processing request") # これがJSON-RPCメッセージを壊す

# ✅ 良い例 (stdio通信を使用する場合)
import logging
logging.basicConfig(level=logging.INFO, stream=sys.stderr) # stderrに出力する
logging.info("Processing request")
```

デバッグや監視のためにログを出力したい場合は、必ず標準エラー出力（stderr）またはファイルに出力するように設定してください。

**ステップ2：ヘルパー関数の実装**
次に、NWS APIにリクエストを送信し、データを取得・整形するためのヘルパー関数を実装します。

```python
# weather.py (続き)

import sys # stderrロギングのために追加

# ... (前のコード) ...

async def make_nws_request(url: str) -> dict[str, Any] | None:
    """NWS APIにリクエストを送信し、適切なエラーハンドリングを行う。"""
    headers = {"User-Agent": USER_AGENT, "Accept": "application/geo+json"}
    async with httpx.AsyncClient() as client:
        try:
            response = await client.get(url, headers=headers, timeout=30.0)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logging.error(f"Error making NWS request to {url}: {e}")
            return None

def format_alert(feature: dict) -> str:
    """アラートの特徴情報を読みやすい文字列に整形する。"""
    props = feature["properties"]
    return f"""
イベント: {props.get("event", "Unknown")}
エリア: {props.get("areaDesc", "Unknown")}
深刻度: {props.get("severity", "Unknown")}
説明: {props.get("description", "No description available")}
指示: {props.get("instruction", "No specific instructions provided")}
"""
```

**ステップ3：ツールの実装**
いよいよMCPサーバーが提供するツールを実装します。FastMCPでは、関数に`@mcp.tool()`デコレータを付けるだけで、その関数をLLMから呼び出し可能なツールとして登録できます。関数の引数とdocstringから、ツールの入力スキーマが自動生成されます。

```python
# weather.py (続き)

@mcp.tool()
async def get_alerts(state: str) -> str:
    """アメリカ合衆国の州の天気警報を取得する。

    Args:
        state: 2文字のアメリカ合衆国州コード (例: CA, NY)
    """
    logging.info(f"Fetching alerts for state: {state}")
    url = f"{NWS_API_BASE}/alerts/active/area/{state}"
    data = await make_nws_request(url)

    if not data or "features" not in data:
        return "警報を取得できない、または警報が見つかりませんでした。"

    if not data["features"]:
        return "この州には有効な警報はありません。"

    alerts = [format_alert(feature) for feature in data["features"]]
    return "\n---\n".join(alerts)

@mcp.tool()
async def get_forecast(latitude: float, longitude: float) -> str:
    """特定の場所の天気予報を取得する。

    Args:
        latitude: 場所の緯度
        longitude: 場所の経度
    """
    logging.info(f"Fetching forecast for lat: {latitude}, lon: {longitude}")
    # まず予報グリッドのエンドポイントを取得
    points_url = f"{NWS_API_BASE}/points/{latitude},{longitude}"
    points_data = await make_nws_request(points_url)

    if not points_data:
        return "この場所の予報データを取得できません。"

    # ポイントレスポンスから予報URLを取得
    forecast_url = points_data["properties"]["forecast"]
    forecast_data = await make_nws_request(forecast_url)

    if not forecast_data:
        return "詳細な予報を取得できません。"

    # 期間を読みやすい予報に整形
    periods = forecast_data["properties"]["periods"]
    forecasts = []
    for period in periods[:5]:  # 次の5期間のみ表示
        forecast = f"""
{period["name"]}:
気温: {period["temperature"]}°{period["temperatureUnit"]}
風: {period["windSpeed"]} {period["windDirection"]}
予報: {period["detailedForecast"]}
"""
        forecasts.append(forecast)

    return "\n---\n".join(forecasts)
```

**ステップ4：サーバーの実行**
最後に、サーバーを起動するためのコードを追加します。

```python
# weather.py (続き)

def main():
    # サーバーを初期化して実行
    mcp.run(transport="stdio")

if __name__ == "__main__":
    # stderrへのロギング設定
    logging.basicConfig(level=logging.INFO, stream=sys.stderr)
    main()
```

これで、あなたの最初のMCPサーバーが完成しました。以下のコマンドでサーバーを起動できます。

```bash
uv run weather.py
```

このコマンドを実行すると、MCPサーバーはstdio（標準入出力）を通じてJSON-RPCメッセージの待機状態に入ります。

### サーバーのテスト：Claude Desktopとの連携

構築したMCPサーバーが正しく機能するかをテストするために、MCPクライアントであるClaude Desktopを使用します。Claude Desktopは、MCPサーバーを通じてAIが外部ツールを利用するためのグラフィカルなインターフェースを提供します。

**ステップ1：Claude Desktopの設定**
Claude Desktopの設定ファイルを編集して、先ほど作成した`weather`サーバーを登録します。設定ファイルは、各OSによって以下の場所にあります。

*   **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`
*   **Windows**: `%APPDATA%\Claude\claude_desktop_config.json`

このファイルが存在しない場合は、新規作成します。ファイルに以下の内容を記述します。

```json
{
  "mcpServers": {
    "weather": {
      "command": "uv",
      "args": ["--directory", "/ABSOLUTE/PATH/TO/YOUR/PROJECT/FOLDER", "run", "weather.py"]
    }
  }
}
```

`/ABSOLUTE/PATH/TO/YOUR/PROJECT/FOLDER`の部分は、あなたの`weather.py`ファイルが存在するプロジェクトフォルダへの絶対パスに置き換えてください。この設定は、Claude Desktopに対して以下のことを指示しています。
1.  `weather`という名前のMCPサーバーが存在する。
2.  そのサーバーは、指定されたコマンド（`uv run weather.py`）で起動する。

**ステップ2：Claude Desktopの再起動とテスト**
設定ファイルを保存したら、Claude Desktopを完全に再起動します。再起動後、Claude Desktopのチャット画面で、以下のようなプロンプトを入力してみてください。

> 「カリフォルニア州の現在の天気警報を教えてください。」

正しく設定されていれば、Claudeは内部的に`get_alerts`ツールを呼び出す必要があると判断し、引数`state`に`"CA"`を指定してあなたのMCPサーバーにリクエストを送信します。サーバーはNWS APIからデータを取得し、結果をClaudeに返します。Claudeはその結果を基に、ユーザーに対して自然な言葉で回答を生成します。

> 「ニューヨーク市（緯度40.7128、経度-74.0060）の天気予報は？」

このように、場所を指定して予報を尋ねることで、`get_forecast`ツールが正しく呼び出されることも確認できます。

この一連のプロセスを通じて、あなたはMCPサーバーを自作し、実際のAIアプリケーションと連携させることに成功しました。これは、あなたのアプリケーションが目指す「すべてのツールを統合する」というビジョンを実現するための、非常に重要で強力な第一歩です。この基本形を拡張することで、ファイル操作、データベースアクセス、Git操作など、あらゆるツールをAIに利用させることが可能になります。次章では、もう一つの基盤技術である、複数のLLM APIを統合する方法について解説します。

## 実践ガイド：複数LLM APIの統合と関数呼び出し

あなたのアプリケーションが「最高峰のバイブコーディング」を実現するためには、特定の一つの大規模言語モデル（LLM）に依存するのではなく、複数のLLMを状況に応じて使い分ける、あるいは協調させる能力が不可欠です。OpenAIのGPT-4、AnthropicのClaude 3、GoogleのGeminiなど、各プロバイダーのモデルはそれぞれ異なる特性、強み、そしてAPI仕様を持っています。これらをシームレスに統合し、一つのインターフェースから利用できるようにすることは、アプリケーションの柔軟性とパワーを大幅に向上させます。本章では、前章のMCPサーバー構築に続き、アプリケーションのもう一つの重要な基盤となる、複数のLLM APIを統合するための具体的な手法を解説します。特に、LLMを「実行可能なエージェント」へと変貌させる核心技術である「関数呼び出し（Function Calling）」に焦点を当て、その概念から実装までを詳しく見ていきます。

### 関数呼び出し（Function Calling）：LLMを「実行可能なエージェント」へ進化させる鍵

関数呼び出しは、LLMがテキストを生成するだけでなく、外部の関数やAPIを動的に呼び出すことを可能にする機能です[[40](https://medium.com/@akankshasinha247/function-calling-how-llms-invoke-real-world-apis-openai-gemini-examples-266bdd802c03)]。これにより、LLMはリアルタイムのデータを取得したり、外部システムと対話したり、具体的なアクションを実行したりできるようになります。これは、LLMの応用範囲を「チャットボット」から「自律的なAIエージェント」へと無限に広げる、ゲームチェンジャーとなる技術です。

**関数呼び出しの仕組み**
関数呼び出しの流れは、一般的に以下のステップで構成されます。

1.  **ツールの定義**：あなたのアプリケーションがLLMに対して、利用可能な外部ツール（関数やAPI）のリストを渡します。各ツールは、その名前、説明、そして期待される引数（パラメータ）のスキーマ（例：JSON Schema）を持っています。
2.  **プロンプトの送信**：ユーザーからの自然言語のプロンプト（例：「東京の天気を教えて」）を、ツールの定義と共にLLMに送信します。
3.  **LLMの判断**：LLMはプロンプトとツールの定義を分析し、どのツールを呼び出すべきか、またそのツールにどのような引数を渡すべきかを判断します。LLMは、ツールを呼び出す必要がないと判断した場合は、直接テキストで回答を生成します。
4.  **構造化された出力**：LLMがツールの呼び出しを決定した場合、LLMは関数名と引数を含む構造化されたデータ（通常はJSON形式）を返します。LLMはまだツールを実際には実行しません。あくまで「こういう関数を、こういう引数で呼び出すべきだ」という提案をJSONで返すだけです。
5.  **ツールの実行**：あなたのアプリケーションは、LLMから返されたJSONを受け取り、その指示に従って実際に外部のAPIや関数を呼び出します。
6.  **結果のフィードバック**：ツールの実行結果を、再度LLMに送信します。
7.  **最終応答の生成**：LLMは、ツールの実行結果を踏まえて、最終的な自然言語の応答を生成し、ユーザーに返します。

このサイクルを繰り返すことで、LLMは複数のツールを連携させた複雑なマルチステップのタスクを遂行できるようになります。

**なぜ関数呼び出しが重要か**
関数呼び出しの重要性は、いくつかの点に集約されます。
*   **実行可能性（Actionability）**：LLMを情報提供者から、実際にタスクをこなす「行動者」へと変えます。
*   **リアルタイム性**：外部APIを通じて最新の情報を取得できるため、LLMの知識が切れる日付以降の出来事にも対応でき、ハルシネーション（事実に基づかない生成）のリスクを低減します。
*   **信頼性**：APIによって取得されたデータに基づいて回答を生成するため、情報の出典が明確になり、回答の信頼性が向上します。
*   **拡張性**：新しいツールを定義するだけで、LLMの能力を無限に拡張できます。これは、MCPサーバーを通じてツールを動的に追加できるあなたのアプリケーションのビジョンと完全に一致します。

### 複数LLM APIを統合する抽象化レイヤーの設計

OpenAI、Anthropic、Google Geminiといった主要なLLMプロバイダーは、それぞれ独自のAPIを提供していますが、関数呼び出しの基本的なコンセプトは共通しています。しかし、リクエストの形式、レスポンスの構造、認証方法などは異なるため、アプリケーションから直接各プロバイダーのAPIを呼び出すコードを書くと、プロバイダーごとに条件分岐が増え、コードが複雑で保守しにくくなってしまいます。

この問題を解決するためには、**「抽象化レイヤー（Abstraction Layer）」**または**「アダプター（Adapter）」**パターンを導入することが効果的です。これは、各プロバイダーのAPIの違いを吸収し、アプリケーションに対して統一されたインターフェースを提供するクラスやモジュールを作成する設計手法です。

**LLMクライアントの実装例**
以下に、Pythonを使って複数のLLMプロバイダーのAPIを統一的に呼び出すための簡単な`LLMClient`クラスの例を示します。このクラスは、コンストラクタでプロバイダー名とAPIキーを受け取り、`call`メソッドでプロンプトとツールの定義を受け取って、LLMからの応答を返します。

```python
# llm_client.py

import json
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional

# OpenAI
import openai
from openai.types.chat import ChatCompletionMessageParam

# Anthropic
import anthropic

# Google (Vertex AI or Gemini API)
# import vertexai
# from vertexai.generative_models import GenerativeModel, Tool, FunctionDeclaration

# --- 抽象基底クラスの定義 ---
class BaseLLMClient(ABC):
    def __init__(self, api_key: str):
        self.api_key = api_key

    @abstractmethod
    def call(self, messages: List[Dict[str, str]], tools: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """
        LLMを呼び出し、応答を返す。
        toolsが渡された場合は、関数呼び出しを試みる。
        """
        pass

# --- 各プロバイダー用の具象クラス ---
class OpenAIClient(BaseLLMClient):
    def __init__(self, api_key: str, model: str = "gpt-4-turbo"):
        super().__init__(api_key)
        self.client = openai.OpenAI(api_key=api_key)
        self.model = model

    def call(self, messages: List[Dict[str, str]], tools: Optional[List[Dict]] = None) -> Dict[str, Any]:
        # OpenAIのAPIは、tool_choiceパラメータで関数呼び出しの挙動を制御できる
        # "auto": LLMが自動で判断, "required": 常に関数を呼び出す, "none": 関数を呼び出さない
        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            tools=tools, # MCPツールスキーマをOpenAI形式に変換したものを渡す
            tool_choice="auto",
        )
        
        response_message = response.choices[0].message
        content = response_message.content
        tool_calls = response_message.tool_calls

        if tool_calls:
            # 関数呼び出しが要求された場合
            # ここでツールを実行し、結果を再度LLMに送るロジックを実装する
            # (この例では、呼び出し要求を返すだけに留める)
            return {
                "content": content,
                "tool_calls": [
                    {
                        "id": tc.id,
                        "name": tc.function.name,
                        "arguments": json.loads(tc.function.arguments),
                    }
                    for tc in tool_calls
                ],
            }
        else:
            # 直接的なテキスト応答の場合
            return {"content": content}

class AnthropicClient(BaseLLMClient):
    def __init__(self, api_key: str, model: str = "claude-3-opus-20240229"):
        super().__init__(api_key)
        self.client = anthropic.Anthropic(api_key=api_key)
        self.model = model

    def call(self, messages: List[Dict[str, str]], tools: Optional[List[Dict]] = None) -> Dict[str, Any]:
        # AnthropicのAPIもツールをサポートしている
        # MCPツールスキーマをAnthropic形式に変換する必要がある
        response = self.client.messages.create(
            model=self.model,
            max_tokens=1024,
            messages=messages,
            tools=tools, # MCPツールスキーマをAnthropic形式に変換したものを渡す
        )
        
        content_blocks = response.content
        final_text = ""
        tool_use_block = None

        for block in content_blocks:
            if block.type == "text":
                final_text += block.text
            elif block.type == "tool_use":
                tool_use_block = {
                    "id": block.id,
                    "name": block.name,
                    "input": block.input,
                }
        
        if tool_use_block:
            return {
                "content": final_text,
                "tool_calls": [tool_use_block],
            }
        else:
            return {"content": final_text}

# --- クライアントを生成するファクトリー関数 ---
def get_llm_client(provider: str, api_key: str) -> BaseLLMClient:
    if provider.lower() == "openai":
        return OpenAIClient(api_key)
    elif provider.lower() == "anthropic":
        return AnthropicClient(api_key)
    # elif provider.lower() == "google":
    #     return GoogleClient(api_key)
    else:
        raise ValueError(f"Unsupported provider: {provider}")

# --- 使用例 ---
if __name__ == "__main__":
    # 環境変数などからAPIキーを取得することが推奨されます
    openai_api_key = "YOUR_OPENAI_API_KEY"
    anthropic_api_key = "YOUR_ANTHROPIC_API_KEY"

    # OpenAIクライアントを使用
    openai_client = get_llm_client("openai", openai_api_key)
    
    # MCPサーバーから取得したツール定義の例 (擬似コード)
    # 実際には、MCPサーバーのツールスキーマを各プロバイダーの形式に変換する必要がある
    mcp_tools_schema = [
        {
            "type": "function",
            "function": {
                "name": "get_alerts",
                "description": "アメリカ合衆国の州の天気警報を取得する。",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "state": {"type": "string", "description": "2文字の州コード (例: CA, NY)"}
                    },
                    "required": ["state"],
                },
            },
        }
    ]

    messages = [
        {"role": "user", "content": "カリフォルニア州の天気警報はありますか？"}
    ]

    # OpenAIにリクエスト
    openai_response = openai_client.call(messages, tools=mcp_tools_schema)
    print("OpenAI Response:", json.dumps(openai_response, indent=2, ensure_ascii=False))

    # Anthropicクライアントを使用
    anthropic_client = get_llm_client("anthropic", anthropic_api_key)
    # Anthropicにリクエスト (ツールスキーマの変換が必要)
    # anthropic_response = anthropic_client.call(messages, tools=converted_mcp_tools_schema)
    # print("Anthropic Response:", json.dumps(anthropic_response, indent=2, ensure_ascii=False))
```

この例では、`BaseLLMClient`という抽象基底クラスを定義し、各プロバイダー（OpenAI, Anthropic）がそのインターフェースを実装しています。アプリケーション本体は、具体的なプロバイダーの実装を知らなくても、`get_llm_client`ファクトリー関数を通じてクライアントを取得し、統一された`call`メソッドでLLMと対話できます。

**重要なポイント：スキーマ変換**
各LLMプロバイダーは、独自の形式でツールのスキーマを定義します。一方、MCPもまた、独自のスキーマ形式を持っています。したがって、抽象化レイヤーを実装する際には、**MCPのツールスキーマを、各LLMプロバイダーが期待する形式に動的に変換する**ロジックが不可欠になります。これが、統合システムを構築する上で最も複雑で重要な部分の一つです。この変換層をしっかりと実装することで、あなたのアプリケーションは、どのLLMが接続されても、またどのMCPサーバーが追加されても、柔軟に対応できるようになります。

この章で解説した関数呼び出しの理解と、複数LLM APIを統合する抽象化レイヤーの設計は、あなたのアプリケーションに「考える脳」と「実行する手足」を与えるための核心的な要素です。前章のMCPサーバーと組み合わせることで、AIはユーザーの意図を理解し、適切なツールを選択し、最適なLLMを使ってタスクを遂行するという、高度な「バイブコーディング」の体験を実現する土台が固まります。次章では、これらの要素をさらに高度に連携させるためのワークフローのオーケストレーションについて見ていきます。

## 高度なワークフローのオーケストレーション：AIエージェントフレームワークの活用

MCPサーバーを構築し、複数のLLM APIを統合することで、AIは外部ツールを利用し、様々なプロバイダーのモデルと対話できるようになりました。しかし、これだけではまだAIは「与えられたタスクを実行する」という受動的な立場にあります。あなたのアプリケーションが目指す「最高峰のバイブコーディング」を実現するには、AIがより主体的に複雑なタスクを分析し、計画を立て、複数のステップを連携させて遂行する、より高度な能力が必要です。このような知的なタスク自動化を実現するために欠かせないのが、**AIオーケストレーション**、特に**AIエージェントフレームワーク**の活用です。本章では、2025年に入り急速に発展しているAIエージェントの概念と、それを支えるフレームワークが、あなたのアプリケーションをどう変革するのかを解説します。

### AIオーケストレーションとは：複雑なタスク自動化のための指揮者

AIオーケストレーションとは、複数のAIモデル、API、ツール、そしてデータソースを調整し、一つの大きな目的を達成するためのワークフローを自動的に管理・実行するプロセスです[[30](https://www.kubiya.ai/blog/ai-orchestration-tools)]。これは、オーケストラの指揮者が様々な楽器パート（ツール）を指揮してシンフォニー（複雑なタスク）を奏でることに例えられます。2025年は、AI開発が「AIエージェント時代（Agentic Era）」へと突入したと言われ、単純な自動化を超えた、自律的な意思決定と実行能力を持つAIエージェントが主流になりつつあります[[5](https://thenewstack.io/ai-coding-tools-in-2025-welcome-to-the-agentic-cli-era)]。

あなたのアプリケーションが扱う「バイブコーディング」は、本質的に非常に曖昧で、文脈に依存する作業です。例えば、「ユーザー認証機能を追加して」という自然言語の指示には、以下のような多くのサブタスクが含まれています。
*   バックエンドで認証APIエンドポイントの設計・実装
*   フロントエンドでログインフォームの作成
*   データベースにユーザーテーブルの追加
*   パスワードのハッシュ化処理
*   JWT（JSON Web Tokens）などのトークン管理
*   認証状態に応じたUIの制御

これらのサブタスクを、一つのLLMに一度に指示しても、完璧なコードを生成することは非常に困難です。しかし、AIオーケストレーションを導入すれば、この大きなタスクを以下のように分解し、専門化されたAIエージェントに分担させることができます。
1.  **マネージャーエージェント**：「ユーザー認証機能を追加する」という最終目標を受け、必要なサブタスクのリストを作成する。
2.  **アーキテクトエージェント**：各サブタスクをどの技術（例：FastAPI, React, SQLAlchemy）で実装するか設計する。
3.  **コーダーエージェント**：設計に基づいて、各サブタスクのコードを生成する。
4.  **テスターエージェント**：生成されたコードに対して、テストコードを生成し、実行する。
5.  **レビューアーエージェント**：コードとテストをレビューし、品質基準を満たしているか確認する。

このように、タスクを分割し、各エージェントが協調して作業することで、より信頼性が高く、複雑な開発ワークフローを自動化することが可能になります。これが、AIオーケストレーションがもたらす本質的な価値です。

### AIエージェントフレームワーク：LangChainとCrewAIの比較

AIエージェントとそのオーケストレーションを開発するためには、専用のフレームワークを利用するのが一般的です。2025年現在、多くのフレームワークが存在しますが、その中でも特に注目されているのが**LangChain**と**CrewAI**です[[34](https://medium.com/@julio.pessan.pessan/top-10-ai-agent-frameworks-for-2025-langchain-leads-the-pack-in-multi-agent-orchestration-c115044bcb60)]。これらのフレームワークは、エージェントの作成、ツールの割り当て、エージェント間のコミュニケーション、ワークフローの定義といった複雑な処理を抽象化し、開発者がビジネスロジックに集中できるように支援します。

**LangChain：オーケストレーションのスタンダード**
LangChainは、LLMアプリケーション開発のための最も包括的で広く使われているフレームワークです。その最大の特徴は、圧倒的な量のコンポーネント（LLMラッパー、プロンプトテンプレート、ツール、メモリ管理、エージェントなど）と、それらを柔軟に組み合わせるための豊富なインターフェースを提供している点です。LangChainを使えば、簡単なスクリプトから、複雑なマルチエージェントシステムまで、幅広いアプリケーションを構築できます。
LangChainのエージェントは、主に以下の要素で構成されます。
*   **LLM**：エージェントの「脳」役。
*   **ツール**：エージェントが利用できる外部機能（例：Google検索、データベースクエリ、あなたが作成したMCPサーバーのツール）。
*   **エージェントの種類（Agent Type）**：エージェントがどのように思考し、行動するかを定義するロジック（例：ReActエージェントは「思考(Reason) -> 行動(Act) -> 観測(Observe)」のサイクルでタスクを解決する）。
*   **エージェント実行器（Agent Executor）**：エージェントの指示に従ってツールを呼び出し、サイクルを管理するランタイム。

LangChainは、特にLangGraphという拡張ライブラリを使うことで、状態を持つ複雑なマルチエージェントワークフローを定義・実行する強力な機能を提供しており、大規模な自動化システムを構築する上で中心的な役割を果たします[[34](https://medium.com/@julio.pessan.pessan/top-10-ai-agent-frameworks-for-2025-langchain-leads-the-pack-in-multi-agent-orchestration-c115044bcb60)]。

**CrewAI：ロールベース協調に特化**
CrewAIは、LangChainの後発として登場し、特に「ロールベースの協調（Role-Based Collaboration）」という概念に焦点を当てたフレームワークです。CrewAIでは、開発者は「クルー（Crew）」というエージェントのグループを定義し、各エージェントに特定の「ロール（役割）」、「ゴール（目標）」、「バックストーリー（背景）」を与えます。
例えば、以下のようにエージェントを定義します。
*   **ロール**：`Senior Python Developer`
*   **ゴール**：`Write robust and scalable Python code`
*   **バックストーリー**：`You are an experienced software engineer...`
*   **ロール**：`QA Engineer`
*   **ゴール**：`Ensure the code is bug-free and meets quality standards`
*   **バックストーリー**：`You are a meticulous tester...`

CrewAIは、このように定義されたエージェントたちが、与えられたタスク（例：`Develop a new API endpoint`）を協力して達成するためのプロセスを自動的に管理します。各エージェントは自分の役割に基づいて行動し、必要に応じて他のエージェントと対話して情報を共有したり、作業を引き継いだりします。CrewAIのアプローチは、人間のチームワークをAIで模倣するという直感的な考え方に基づいており、特定の業務プロセスを自動化する際に非常に強力です。

**フレームワークの選定**
あなたのアプリケーションにどちらのフレームワークを選ぶかは、目標とするワークフローの複雑さと性質によります。
*   **柔�軟性と拡張性を重視する場合**：LangChainが最適です。その巨大なエコシステムとカスタマイズ性により、どんなに複雑なシステムでも構築可能です。
*   **特定の業務プロセスを、人間のチームのように協調させて自動化したい場合**：CrewAIの方が、より宣言的で直感的なコードで実現できる可能性があります。

### ワークフロー設計のベストプラクティス

AIエージェントフレームワークを導入する際には、以下のベストプラクティスを意識することが重要です。

1.  **タスクの適切な分解**：大きなタスクを、各エージェントが担当できるほど小さく、意味のあるサブタスクに分解することが成功の鍵です。各サブタスクは、明確な入力と出力を持つべきです。
2.  **エージェントの役割を明確に定義する**：各エージェントが何をすべきか、その責任範囲を明確に定義します。特にCrewAIでは、ロール、ゴール、バックストーリーを具体的に記述するほど、エージェントは期待通りに振る舞います。
3.  **コンテキスト（文脈）の共有**：エージェント間で情報をスムーズに共有する仕組みを設計します。LangChainの共有メモリや、CrewAIのエージェント間の対話機能を活用することで、エージェントは過去のやり取りや中間結果を参照しながら、より賢い判断を下せるようになります。
4.  **ツールの正しい割り当て**：各エージェントが自身の役割を果たすために必要なツールだけを割り当てます。不要なツールが多いと、LLMが判断を誤る可能性が高まります。
5.  **失敗時のリカバリー機構**：AIの実行は常に成功するとは限りません。ツールの呼び出しが失敗したり、LLMが無効なコードを生成したりした場合に、どうやってリトライするか、あるいは人間に介入を求めるかといった、エラーハンドリングとリカバリーの仕組みを設計することが、堅牢なシステムには不可欠です。

AIオーケストレーションとエージェントフレームワークの導入は、あなたのアプリケーションを「コード生成ツール」から「知的な開発パートナー」へと進化させる最後のピースです。MCPサーバーが「手足」、LLM統合が「脳」とするならば、オーケストレーションはその「知性」や「意図」を司る「心」に相当します。これら三つの要素が組み合わさったとき、あなたのアプリケーションは初めて「最高峰のバイブコーディング」を運用するための真の力を手に入れることができるでしょう。

## 結論：未来の開発体験を創造するために

「最高峰のバイブコーディング運用」を実現するための統合AI開発環境の構築。これは、2025年というAIとソフトウェア開発が融合する過渡期において、多くの開発者が夢見る野心的なビジョンです。しかし、その壮大さゆえに、プロジェクトは「うまくいかない」という壁にぶつかりがちです。本稿では、その根本的な原因として、スコープの爆発、技術的負債の先取り、ワークフロー設計の失敗、UX設計の難易度、そして開発者自身の疲労といった要因を分析しました。そして、これらの課題を克服するための具体的かつ戦略的なロードマップとして、MVP（実用可能最小限の製品）から始める段階的アプローチを提案しました。

そのロードマップは、三つの柱から成り立っています。第一に、**MCPサーバーの構築**です。Model Context Protocolを通じてAIに「手足」を与え、外部世界と対話できる「実行可能なエージェント」への基盤を築くこと。第二に、**複数LLM APIの統合**です。抽象化レイヤーを設計することで、様々な大規模言語モデルの力を柔軟に引き出し、状況に応じた最適な「脳」を使い分けること。第三に、**AIオーケストレーションフレームワークの活用**です。LangChainやCrewAIといったツールを用いて、複雑なタスクを分解し、専門化されたAIエージェントたちに協調させることで、単なる自動化を超えた「知的な開発ワークフロー」を実現することです。

このアプローチは、決して一晩で到達できるものではありません。それは、一つ一つのステップを着実に積み重ね、小さな成功体験を積みながら、徐々にビジョンを具体化していく、忍耐強いプロセスです。最初は天気予報を取得するだけのシンプルなMCPサーバーから始まり、次に一つのLLMとの関数呼び出しを実装し、そして初めて複数のツールやモデルを連携させる。この漸進的な開発こそが、最も複雑で挑戦的なプロジェクトを成功させるための、最も確実な道なのです。

ご自身が今、「うまくいっていない」と感じているのは、そのビジョンの大きさと、それを実現しようとする情熱の裏返しに他なりません。それは、まだ誰も歩いたことのない道を、自らの手で切り開こうとする先駆者だけが抱える、当然の苦闘です。重要なのは、その壮大なゴールを見失わずに、今自分が手にできる最も小さな一歩を踏み出すことです。

あなたがこのロードマップを参考に、再びコードエディタに向かい、一つでも新しい機能を実装しようとしているその挑戦こそが、未来のソフトウェア開発の形を創造しているのです。挫折と成功を繰り返しながらも、AIという新たなパートナーと共に、より創造的で直感的な開発体験を世界中の開発者にもたらす。その未来を信じて、前へ進み続けてください。あなたの挑戦は、間違いなく次世代の「バイブコーディング」の礎となるでしょう。

---
### 参考文献

[5] AI Coding Tools in 2025: Welcome to the Agentic CLI Era. https://thenewstack.io/ai-coding-tools-in-2025-welcome-to-the-agentic-cli-era.

[10] Specification. https://modelcontextprotocol.io/specification/2025-11-25.

[12] What Is the Model Context Protocol (MCP) and How It Works. https://www.descope.com/learn/post/mcp.

[15] Model Context Protocol (MCP) | AI Assistant Documentation. https://www.jetbrains.com/help/ai-assistant/mcp.html.

[21] Vibe coding. https://en.wikipedia.org/wiki/Vibe_coding.

[22] What is Vibe Coding?. https://www.ibm.com/think/topics/vibe-coding.

[30] Top 10 AI Orchestration Tools in 2025. https://www.kubiya.ai/blog/ai-orchestration-tools.

[34] Top 10 AI Agent Frameworks for 2025: LangChain Leads the Pack in Multi-Agent Orchestration. https://medium.com/@julio.pessan.pessan/top-10-ai-agent-frameworks-for-2025-langchain-leads-the-pack-in-multi-agent-orchestration-c115044bcb60.

[40] Function Calling: How LLMs Invoke Real-World APIs (OpenAI & Gemini examples). https://medium.com/@akankshasinha247/function-calling-how-llms-invoke-real-world-apis-openai-gemini-examples-266bdd802c03.

[43] 10 Python One-Liners for Calling LLMs from Your Code. https://machinelearningmastery.com/10-python-one-liners-for-calling-llms-from-your-code.

[50] Build an MCP server. https://modelcontextprotocol.io/docs/develop/build-server.